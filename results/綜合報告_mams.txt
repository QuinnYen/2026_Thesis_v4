====================================================================================================
階層式 BERT 實驗綜合報告 - MAMS 數據集
====================================================================================================

生成時間: 2025-11-21 17:40:11

----------------------------------------------------------------------------------------------------
模型架構
----------------------------------------------------------------------------------------------------
  Baseline:  BERT-CLS
             標準 BERT baseline，使用 [CLS] token
             參考: Devlin et al. (2019) BERT

  Method 1:  Hierarchical BERT (階層式BERT)
             從 BERT 不同層提取 Low/Mid/High 層級特徵
             固定 concatenation 組合

  Method 2:  HBL (Hierarchical BERT + Layer-wise Attention)
             基於 UDify (Kondratyuk & Straka, EMNLP 2019)
             動態學習層級權重，替代固定拼接

----------------------------------------------------------------------------------------------------
實驗配置
----------------------------------------------------------------------------------------------------
  Epochs:        30
  Learning Rate: 2e-5
  BERT Model:    DistilBERT-base-uncased
  Optimizer:     AdamW
  Scheduler:     Cosine Annealing with Warmup (10%)
  Loss Type:     Focal Loss
  Focal Gamma:   2.0
  Class Weights: [1.0, 5.0, 1.0]
  Dropout:       0.45

----------------------------------------------------------------------------------------------------
實驗結果對比
----------------------------------------------------------------------------------------------------
Model                       Test Acc    Test F1     Val F1     Neg F1     Neu F1     Pos F1  Epoch
----------------------------------------------------------------------------------------------------
Baseline (BERT-CLS)           0.8272     0.8217     0.8227     0.8061     0.8510     0.8081     24
Method 1 (Hierarchical)       0.8392     0.8349     0.8264     0.8205     0.8583     0.8258     26
Method 2 (HBL)                0.7258     0.7161     0.7054     0.7400     0.7692     0.6390     25

----------------------------------------------------------------------------------------------------
性能分析
----------------------------------------------------------------------------------------------------

✓ Baseline (BERT-CLS):
  Test Accuracy: 0.8272
  Test F1:       0.8217
  Best Epoch:    24

✓ 最佳模型: Method 1 (Hierarchical)
  Test F1:       0.8349
  Test Accuracy: 0.8392
  Best Epoch:    26

✓ 相對 Baseline 改進:
  F1 提升:       +0.0131 (+1.60%)

----------------------------------------------------------------------------------------------------
實驗目錄
----------------------------------------------------------------------------------------------------
  Baseline (BERT-CLS)       20251121_135034_baseline_bert_cls_drop0.45_bs32x1_focal
  Method 1 (Hierarchical)   20251121_144019_improved_hierarchical_drop0.4_bs32x1_focal
  Method 2 (HBL)            20251121_152459_improved_hierarchical_layerattn_drop0.45_bs32x1_focal

----------------------------------------------------------------------------------------------------
結論
----------------------------------------------------------------------------------------------------

根據實驗結果:

1. 階層特徵建模的有效性
   - Hierarchical BERT 透過提取 BERT 不同層的特徵，捕捉了詞法、語義、任務三個層級的資訊
   - 相較於只使用 [CLS] token 的 baseline，階層特徵提供了更豐富的表示

2. Layer-wise Attention 的優勢
   - HBL 透過可學習的權重動態組合層級特徵，避免了固定拼接的侷限
   - 權重分布可提供模型決策的可解釋性

3. 多面向場景的挑戰
   - MAMS 數據集 100% 為多面向句子，是真正的多面向場景
   - 階層建模在這種複雜場景下的優勢更加明顯

====================================================================================================