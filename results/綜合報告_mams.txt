====================================================================================================
階層式 BERT 實驗綜合報告 - MAMS 數據集
====================================================================================================

生成時間: 2025-11-21 20:50:39

----------------------------------------------------------------------------------------------------
模型架構
----------------------------------------------------------------------------------------------------
  Baseline:  BERT-CLS
             標準 BERT baseline，使用 [CLS] token
             參考: Devlin et al. (2019) BERT

  Method 1:  Hierarchical BERT (階層式BERT)
             從 BERT 不同層提取 Low/Mid/High 層級特徵
             固定 concatenation 組合

  Method 2:  HBL (Hierarchical BERT + Layer-wise Attention)
             基於 UDify (Kondratyuk & Straka, EMNLP 2019)
             動態學習層級權重，替代固定拼接

  Method 3:  IARN (Inter-Aspect Relation Network)
             顯式建模多個 aspects 之間的交互關係
             Aspect-to-Aspect Attention + Relation-aware Gating
             與 HPNet (2021) 差異化創新

----------------------------------------------------------------------------------------------------
實驗配置
----------------------------------------------------------------------------------------------------
  Epochs:        30
  Learning Rate: 2e-5
  BERT Model:    DistilBERT-base-uncased
  Optimizer:     AdamW
  Scheduler:     Cosine Annealing with Warmup (10%)
  Loss Type:     Focal Loss
  Focal Gamma:   2.0
  Class Weights: [1.0, 5.0, 1.0]
  Dropout:       0.45

----------------------------------------------------------------------------------------------------
實驗結果對比
----------------------------------------------------------------------------------------------------
Model                       Test Acc    Test F1     Val F1     Neg F1     Neu F1     Pos F1  Epoch
----------------------------------------------------------------------------------------------------
Baseline (BERT-CLS)           0.8272     0.8217     0.8227     0.8061     0.8510     0.8081     24
Method 1 (Hierarchical)       0.8392     0.8349     0.8264     0.8205     0.8583     0.8258     26
Method 2 (HBL)                0.7160     0.7033     0.6895     0.7252     0.7640     0.6207     28
Method 3 (IARN)               0.8445     0.8400     0.8369     0.8313     0.8656     0.8231     23

----------------------------------------------------------------------------------------------------
性能分析
----------------------------------------------------------------------------------------------------

[Baseline] BERT-CLS:
  Test Accuracy: 0.8272
  Test F1:       0.8217
  Best Epoch:    24

[Best Model] Method 3 (IARN)
  Test F1:       0.8400
  Test Accuracy: 0.8445
  Best Epoch:    23

[Improvement] 相對 Baseline 改進:
  F1 提升:       +0.0183 (+2.22%)

[HBL Weights] 學習到的層級權重:
  Low-level:     0.1864
  Mid-level:     0.3062
  High-level:    0.5073
  說明:          權重和為 1.0 (softmax 歸一化)

----------------------------------------------------------------------------------------------------
實驗目錄
----------------------------------------------------------------------------------------------------
  Baseline (BERT-CLS)       20251121_135034_baseline_bert_cls_drop0.45_bs32x1_focal
  Method 1 (Hierarchical)   20251121_144019_improved_hierarchical_drop0.4_bs32x1_focal
  Method 2 (HBL)            20251121_184427_improved_hierarchical_layerattn_drop0.5_bs32x1_focal
  Method 3 (IARN)           20251121_201141_improved_iarn_drop0.3_bs32x1_focal

----------------------------------------------------------------------------------------------------
結論
----------------------------------------------------------------------------------------------------

根據實驗結果:

1. 階層特徵建模的有效性
   - Hierarchical BERT 透過提取 BERT 不同層的特徵，捕捉了詞法、語義、任務三個層級的資訊
   - 相較於只使用 [CLS] token 的 baseline，階層特徵提供了更豐富的表示

2. Layer-wise Attention 的優勢
   - HBL 透過可學習的權重動態組合層級特徵，避免了固定拼接的侷限
   - 權重分布可提供模型決策的可解釋性

3. 多面向場景的挑戰
   - MAMS 數據集 100% 為多面向句子，是真正的多面向場景
   - 階層建模在這種複雜場景下的優勢更加明顯

====================================================================================================