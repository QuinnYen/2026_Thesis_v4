====================================================================================================
階層式 BERT 實驗綜合報告 - RESTAURANTS 數據集
====================================================================================================

生成時間: 2025-11-21 22:16:36

----------------------------------------------------------------------------------------------------
模型架構
----------------------------------------------------------------------------------------------------
  Baseline:  BERT-CLS
             標準 BERT baseline，使用 [CLS] token
             參考: Devlin et al. (2019) BERT

  Method 1:  Hierarchical BERT (階層式BERT)
             從 BERT 不同層提取 Low/Mid/High 層級特徵
             固定 concatenation 組合

  Method 2:  HBL (Hierarchical BERT + Layer-wise Attention)
             基於 UDify (Kondratyuk & Straka, EMNLP 2019)
             動態學習層級權重，替代固定拼接

  Method 3:  IARN (Inter-Aspect Relation Network)
             顯式建模多個 aspects 之間的交互關係
             Aspect-to-Aspect Attention + Relation-aware Gating
             與 HPNet (2021) 差異化創新

----------------------------------------------------------------------------------------------------
實驗配置
----------------------------------------------------------------------------------------------------
  Epochs:        30
  Learning Rate: 2e-5
  BERT Model:    DistilBERT-base-uncased
  Optimizer:     AdamW
  Scheduler:     Cosine Annealing with Warmup (10%)
  Loss Type:     Focal Loss
  Focal Gamma:   2.5
  Class Weights: [1.0, 8.0, 1.0]
  Dropout:       0.3-0.4

----------------------------------------------------------------------------------------------------
實驗結果對比
----------------------------------------------------------------------------------------------------
Model                       Test Acc    Test F1     Val F1     Neg F1     Neu F1     Pos F1  Epoch
----------------------------------------------------------------------------------------------------
Baseline (BERT-CLS)           0.8125     0.7220     0.7104     0.7344     0.5301     0.9014     12
Method 1 (Hierarchical)       0.8069     0.7228     0.7043     0.7214     0.5479     0.8991     16
Method 2 (HBL)                   N/A        N/A        N/A        N/A        N/A        N/A    N/A
Method 3 (IARN)               0.7992     0.7090     0.7170     0.7302     0.5088     0.8882     25

----------------------------------------------------------------------------------------------------
性能分析
----------------------------------------------------------------------------------------------------

[Baseline] BERT-CLS:
  Test Accuracy: 0.8125
  Test F1:       0.7220
  Best Epoch:    12

[Best Model] Method 1 (Hierarchical)
  Test F1:       0.7228
  Test Accuracy: 0.8069
  Best Epoch:    16

[Improvement] 相對 Baseline 改進:
  F1 提升:       +0.0009 (+0.12%)

----------------------------------------------------------------------------------------------------
實驗目錄
----------------------------------------------------------------------------------------------------
  Baseline (BERT-CLS)       20251121_213136_baseline_bert_cls_drop0.3_bs32x1_focal
  Method 1 (Hierarchical)   20251121_214227_improved_hierarchical_drop0.4_bs32x1_focal
  Method 3 (IARN)           20251121_215520_improved_iarn_drop0.3_bs32x1_focal

----------------------------------------------------------------------------------------------------
結論
----------------------------------------------------------------------------------------------------

根據實驗結果:

1. 階層特徵建模的有效性
   - Hierarchical BERT 透過提取 BERT 不同層的特徵，捕捉了詞法、語義、任務三個層級的資訊
   - 相較於只使用 [CLS] token 的 baseline，階層特徵提供了更豐富的表示

2. Layer-wise Attention 的優勢
   - HBL 透過可學習的權重動態組合層級特徵，避免了固定拼接的侷限
   - 權重分布可提供模型決策的可解釋性

3. 多面向場景的挑戰
   - RESTAURANTS 數據集約 20% 為多面向句子
   - 模型需要同時處理單面向和多面向的場景

====================================================================================================