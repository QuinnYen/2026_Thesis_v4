================================================================================
Multi-Aspect ABSA 實驗報告 - RESTAURANTS Dataset
================================================================================

生成時間: 2025-11-25 01:06:34

--------------------------------------------------------------------------------
模型架構
--------------------------------------------------------------------------------
  Baseline:  BERT-CLS
             標準 BERT baseline，使用 [CLS] token
             參考: Devlin et al. (2019) BERT

  Method 1:  Hierarchical BERT (階層式BERT)
             從 BERT 不同層提取 Low/Mid/High 層級特徵
             適用於單面向為主的數據集 (如 Restaurants, Laptops)
             配置: unified_hierarchical.yaml

  Method 2:  IARN (Inter-Aspect Relation Network)
             顯式建模多個 aspects 之間的交互關係
             Aspect-to-Aspect Attention + Relation-aware Gating
             適用於多面向數據集 (如 MAMS)

  Method 3:  HSA (Hierarchical Syntax Attention)
             階層式語法注意力網絡
             在語法結構上進行階層式傳播 (Token → Phrase → Clause)
             結合「階層式」概念與語法結構信息

  (所有模型均採用 LLRD 優化訓練)

--------------------------------------------------------------------------------
實驗配置
--------------------------------------------------------------------------------
  Epochs:        30
  Patience:      10
  Learning Rate: 2e-5
  BERT Model:    BERT-base-uncased
  Optimizer:     AdamW
  Scheduler:     Cosine Annealing with Warmup (10%)
  Loss Type:     Focal Loss
  Focal Gamma:   2.0
  Class Weights: auto (動態計算)
  Dropout:       0.3-0.4 (各模型略有不同)

--------------------------------------------------------------------------------
實驗結果對比 (Main Results)
--------------------------------------------------------------------------------
Model                             Acc (%)   Macro-F1 (%)   Best/Total Epoch
--------------------------------------------------------------------------------
Baseline (BERT-CLS)                 80.69          72.22                 14
Method 1 (Hierarchical BERT)        80.20          72.00                 16
Method 2 (IARN)                     81.61          72.92                 17
Method 3 (HSA)                      80.90          72.17                 12
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
Per-class F1 Analysis (for error analysis)
--------------------------------------------------------------------------------
Model                                Neg F1       Neu F1       Pos F1
--------------------------------------------------------------------------------
Baseline (BERT-CLS)                   75.00        52.03        89.63
Method 1 (Hierarchical BERT)          73.06        53.71        89.24
Method 2 (IARN)                       74.70        54.18        89.89
Method 3 (HSA)                        72.76        53.53        90.23

--------------------------------------------------------------------------------
性能分析
--------------------------------------------------------------------------------

[Baseline] BERT-CLS:
  Accuracy:  80.69%
  Macro-F1:  72.22%
  Best Epoch:    14 / None

[Best Model] Method 2 (IARN)
  Macro-F1:  72.92%
  Accuracy:  81.61%
  Best Epoch:    17 / None

[Improvement] 相對 Baseline 改進:
  Macro-F1 提升: +0.71% (+0.98% relative)

--------------------------------------------------------------------------------
實驗目錄
--------------------------------------------------------------------------------
  Baseline (BERT-CLS)       20251125_002936_baseline_bert_cls_drop0.3_bs32x1_focal
  Method 1 (Hierarchical BERT) 20251125_003800_improved_hierarchical_llrd_drop0.4_bs32x1_focal
  Method 2 (IARN)           20251125_004717_improved_iarn_drop0.3_bs32x1_focal
  Method 3 (HSA)            20251125_005718_improved_hsa_drop0.3_bs32x1_focal

--------------------------------------------------------------------------------
結論
--------------------------------------------------------------------------------

根據實驗結果:

1. 階層特徵建模的有效性
   - Hierarchical BERT 透過提取 BERT 不同層的特徵，捕捉了詞法、語義、任務三個層級的資訊
   - 相較於只使用 [CLS] token 的 baseline，階層特徵提供了更豐富的表示

2. 場景自適應策略
   - RESTAURANTS 數據集以單面向句子為主
   - 最佳模型: Method 2 (IARN)
   - HSA 結合階層式概念與語法結構，在 Neutral 類別上表現突出

3. 模型選擇指南
   - 多面向比例 > 50%: 使用 IARN (Aspect-to-Aspect Attention)
   - 多面向比例 <= 50%: 使用 HSA 或 Hierarchical BERT (階層特徵)

================================================================================