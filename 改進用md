# VP-ACLè«–æ–‡:é¡åˆ¥ä¸å¹³è¡¡è™•ç†èˆ‡F1è¨ˆç®—åˆ†æ

---

## ğŸ“Š é¡åˆ¥ä¸å¹³è¡¡å•é¡Œ

### å•é¡Œæè¿°

ABSAæ•¸æ“šé›†æ™®éå­˜åœ¨**é¡åˆ¥ä¸å¹³è¡¡**å•é¡Œ:
- **Positiveå’ŒNegative**æ¨£æœ¬è¼ƒå¤š
- **Neutral**æ¨£æœ¬ç›¸å°è¼ƒå°‘
- å°è‡´æ¨¡å‹å®¹æ˜“åå‘å¤šæ•¸é¡,å¿½ç•¥å°‘æ•¸é¡

**å…¸å‹æ•¸æ“šåˆ†å¸ƒ**:
```
SemEval-2014 Restaurants:
  Positive: ~60%
  Negative: ~25%
  Neutral: ~15%  â† å°‘æ•¸é¡

MAMS:
  ç›¸å°å¹³è¡¡,ä½†ä»æœ‰å·®ç•°
```

---

## ğŸ› ï¸ VP-ACLçš„è™•ç†ç­–ç•¥

### ç­–ç•¥ä¸€:èª¿æ•´è¨“ç·´Epochæ•¸é‡ â­â­â­â­

**æ ¸å¿ƒæ€æƒ³**: **é‡å°ä¸å¹³è¡¡æ•¸æ“šé›†ä½¿ç”¨è¼ƒå°‘çš„è¨“ç·´è¼ªæ•¸**

**è«–æ–‡åŸæ–‡**:
> "For datasets with imbalanced class distributions, we observed early convergence during training. Therefore, **a smaller number of training epochs was used to prevent the model from overfitting to majority-class samples** in the early stages, which could impair its ability to recognize minority classes."

**å…·é«”å¯¦æ–½** (Table 3):

| æ•¸æ“šé›† | Epochæ•¸ | é¡åˆ¥å¹³è¡¡ç¨‹åº¦ | èªªæ˜ |
|--------|---------|-------------|------|
| **Laptop** | **15** | ä¸å¹³è¡¡ | **æœ€å°‘**,é˜²æ­¢éæ“¬åˆ |
| Rest14 | 30 | ä¸­åº¦ä¸å¹³è¡¡ | æ¨™æº–è¨­ç½® |
| Rest15 | 30 | ä¸­åº¦ä¸å¹³è¡¡ | æ¨™æº–è¨­ç½® |
| Rest16 | 30 | ä¸å¹³è¡¡ | æ¨™æº–è¨­ç½® |
| **MAMS** | **60** | **ç›¸å°å¹³è¡¡** | **æœ€å¤š**,å……åˆ†è¨“ç·´ |

**ç‚ºä»€éº¼æœ‰æ•ˆ?**

```python
# Early stoppingåŸç†
è¨“ç·´åˆæœŸ (epoch 1-5):
  â†’ æ¨¡å‹å¿«é€Ÿå­¸ç¿’å¤šæ•¸é¡(Positive/Negative)
  â†’ Accuracyå¿«é€Ÿä¸Šå‡
  â†’ ä½†å°å°‘æ•¸é¡(Neutral)å­¸ç¿’ä¸è¶³

è¨“ç·´ä¸­æœŸ (epoch 6-15):
  â†’ æ¨¡å‹é–‹å§‹å­¸ç¿’å°‘æ•¸é¡ç‰¹å¾µ
  â†’ Accuracyä¸Šå‡è®Šç·©
  â†’ F1åˆ†æ•¸æŒçºŒæå‡ â† æœ€ä½³æ™‚æœŸ â­

è¨“ç·´å¾ŒæœŸ (epoch 16+):
  â†’ æ¨¡å‹éåº¦æ“¬åˆå¤šæ•¸é¡
  â†’ Accuracyå¯èƒ½ä»ä¸Šå‡
  â†’ ä½†å°‘æ•¸é¡æ€§èƒ½ä¸‹é™ â† éæ“¬åˆ!

ç­–ç•¥: åœ¨è¨“ç·´ä¸­æœŸ(epoch 15å·¦å³)åœæ­¢è¨“ç·´
```

---

### ç­–ç•¥äºŒ:æ¨™æº–è¨“ç·´æŠ€å·§

é›–ç„¶è«–æ–‡æ²’æœ‰æ˜ç¢ºèªªæ˜,ä½†å¾é…ç½®å¯ä»¥æ¨æ–·ä½¿ç”¨äº†:

#### 1. Dropoutæ­£å‰‡åŒ–

```yaml
# Table 3: ä¸åŒæ•¸æ“šé›†çš„Dropoutè¨­ç½®
Rest14:  0.4  # é«˜dropout = æ›´å¼·æ­£å‰‡åŒ–
Laptop:  0.1  # ä½dropout (å› ç‚ºæ•¸æ“šé‡å°)
MAMS:    0.3  # ä¸­ç­‰dropout
Rest15:  0.3
Rest16:  0.2
```

**åŸç†**: é˜²æ­¢æ¨¡å‹éåº¦æ“¬åˆå¤šæ•¸é¡

#### 2. Learning Rateèª¿æ•´

```yaml
æ‰€æœ‰æ•¸æ“šé›†çµ±ä¸€:
  learning_rate: 5e-5  # è¼ƒå°çš„å­¸ç¿’ç‡
  warmup_rate: 0.1-0.3  # ä½¿ç”¨warmup
```

**åŸç†**: 
- å°learning rate: é¿å…åœ¨å¤šæ•¸é¡ä¸Šæ”¶æ–‚éå¿«
- Warmup: è®“æ¨¡å‹æœ‰æ™‚é–“å­¸ç¿’å°‘æ•¸é¡

#### 3. Batch Size

```yaml
æ‰€æœ‰æ•¸æ“šé›†çµ±ä¸€:
  batch_size: 16  # ç›¸å°è¼ƒå°
```

**åŸç†**: å°batch sizeæä¾›æ›´å¤šæ¨£åŒ–çš„æ¢¯åº¦æ›´æ–°

---

### ç­–ç•¥ä¸‰:å°æ¯”å­¸ç¿’çš„éš±å¼å¹³è¡¡

**Dropoutç­–ç•¥ç”Ÿæˆæ­£æ¨£æœ¬**:
```python
def generate_positive_sample(x, dropout_rate):
    # éš¨æ©Ÿmaskç‰¹å¾µ
    mask = dropout(x, p=dropout_rate)
    return mask
```

**ç‚ºä»€éº¼å°ä¸å¹³è¡¡æœ‰å¹«åŠ©?**

1. **å¢åŠ å°‘æ•¸é¡æ¨£æœ¬å¤šæ¨£æ€§**: 
   - Neutralæ¨£æœ¬å°‘,ä½†é€šéDropoutç”Ÿæˆå¤šå€‹è®Šé«”
   - ç›¸ç•¶æ–¼æ•¸æ“šå¢å¼·

2. **é˜²æ­¢è¨˜æ†¶å¤šæ•¸é¡**:
   - Dropoutç ´å£äº†å®Œæ•´ç‰¹å¾µ
   - æ¨¡å‹å¿…é ˆå­¸ç¿’æ›´é­¯æ£’çš„è¡¨ç¤º

---

### ç­–ç•¥å››:å°æŠ—æ¨£æœ¬æ ¹æ“šaspectæ•¸é‡èª¿æ•´

```python
# è‡ªé©æ‡‰æ“¾å‹•ç¯„åœ
if num_aspects >= 3:
    perturbation_range = Î´ Ã— 2.0
elif num_aspects == 2:
    perturbation_range = Î´ Ã— 1.5
else:
    perturbation_range = Î´ Ã— 1.0
```

**å°ä¸å¹³è¡¡çš„é–“æ¥å¹«åŠ©**:
- å¤šaspectå¥å­é€šå¸¸åŒ…å«æ›´å¤šæƒ…æ„Ÿå¤šæ¨£æ€§
- è‡ªé©æ‡‰æ“¾å‹•ç¢ºä¿ä¸åŒé¡åˆ¥éƒ½èƒ½å¾—åˆ°å……åˆ†è¨“ç·´

---

## ğŸ“ˆ F1åˆ†æ•¸è¨ˆç®—æ–¹æ³•

### è¨ˆç®—å…¬å¼

VP-ACLä½¿ç”¨**Macro F1**ä½œç‚ºä¸»è¦è©•ä¼°æŒ‡æ¨™:

$$
F1_{macro} = \frac{F1_{neg} + F1_{neu} + F1_{pos}}{3}
$$

å…¶ä¸­æ¯å€‹é¡åˆ¥çš„F1ç‚º:

$$
F1_c = 2 \times \frac{Precision_c \times Recall_c}{Precision_c + Recall_c}
$$

$$
Precision_c = \frac{TP_c}{TP_c + FP_c}
$$

$$
Recall_c = \frac{TP_c}{TP_c + FN_c}
$$

---

### ç‚ºä»€éº¼é¸æ“‡Macro F1?

**Macro F1 vs. Micro F1 vs. Weighted F1**:

```python
# å‡è¨­æ•¸æ“šåˆ†å¸ƒ
Positive: 600æ¨£æœ¬
Negative: 300æ¨£æœ¬  
Neutral:  100æ¨£æœ¬  â† å°‘æ•¸é¡
ç¸½è¨ˆ:     1000æ¨£æœ¬

# æ¨¡å‹é æ¸¬çµæœ
Positive: F1 = 0.90 (é«˜)
Negative: F1 = 0.85 (é«˜)
Neutral:  F1 = 0.50 (ä½) â† æ€§èƒ½å·®

# ä¸åŒF1è¨ˆç®—
Macro F1 = (0.90 + 0.85 + 0.50) / 3 = 0.75
  â†’ å¹³ç­‰å°å¾…æ¯å€‹é¡åˆ¥ âœ…

Weighted F1 = (0.90Ã—600 + 0.85Ã—300 + 0.50Ã—100) / 1000 = 0.83
  â†’ åå‘å¤šæ•¸é¡ âŒ

Micro F1 = è¨ˆç®—ç¸½é«”TP/FP/FN = ~0.85
  â†’ å®Œå…¨å¿½ç•¥å°‘æ•¸é¡æ€§èƒ½ âŒ
```

**çµè«–**: Macro F1æ›´èƒ½åæ˜ æ¨¡å‹åœ¨**æ‰€æœ‰é¡åˆ¥**(åŒ…æ‹¬å°‘æ•¸é¡)ä¸Šçš„çœŸå¯¦æ€§èƒ½

---

### å¯¦é©—è¨­ç½®ç´°ç¯€

**è«–æ–‡åŸæ–‡**:
> "This study uses F1-score and accuracy as metrics to evaluate the model's performance."
> 
> "The results of our model are **averaged over three independent runs** on each dataset to reduce the impact of randomness."

**å…·é«”æµç¨‹**:

```python
def evaluate_vp_acl(dataset):
    results = []
    
    # é‹è¡Œ3æ¬¡ç¨ç«‹å¯¦é©—
    for seed in [42, 123, 456]:
        set_seed(seed)
        
        # è¨“ç·´æ¨¡å‹
        model = train_model(dataset, seed)
        
        # æ¸¬è©¦é›†è©•ä¼°
        y_true, y_pred = predict(model, test_data)
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„F1
        f1_neg = f1_score(y_true, y_pred, labels=[0], average='binary')
        f1_neu = f1_score(y_true, y_pred, labels=[1], average='binary')
        f1_pos = f1_score(y_true, y_pred, labels=[2], average='binary')
        
        # Macro F1
        f1_macro = (f1_neg + f1_neu + f1_pos) / 3
        
        # æˆ–ä½¿ç”¨sklearnç›´æ¥è¨ˆç®—
        f1_macro = f1_score(y_true, y_pred, average='macro')
        
        results.append(f1_macro)
    
    # å ±å‘Šå¹³å‡å€¼
    final_f1 = np.mean(results)
    return final_f1

# è«–æ–‡ä¸­å ±å‘Šçš„F1åˆ†æ•¸
Rest14: 82.62%  # 3æ¬¡é‹è¡Œçš„å¹³å‡
Laptop: 79.18%
MAMS:   84.83%
Rest15: 76.28%
Rest16: 79.10%
```

---

### è©•ä¼°æŒ‡æ¨™é¸æ“‡

**Table 4ä¸­å ±å‘Šçš„æŒ‡æ¨™**:

| æŒ‡æ¨™ | èªªæ˜ | ç”¨é€” |
|------|------|------|
| **Accuracy** | æ•´é«”æº–ç¢ºç‡ | æ¨¡å‹ç¸½é«”æ€§èƒ½ |
| **F1 (Macro)** | ä¸‰é¡å¹³å‡F1 | **ä¸»è¦æŒ‡æ¨™**,åæ˜ ä¸å¹³è¡¡æ€§èƒ½ |

**ç‚ºä»€éº¼ä¸å ±å‘Šæ¯å€‹é¡åˆ¥çš„F1?**
- è«–æ–‡Table 4åªå ±å‘Šç¸½é«”æŒ‡æ¨™
- ä½†åœ¨æ¶ˆèå¯¦é©—(Table 5)ä¸­ä½¿ç”¨F1ä½œç‚ºè©•ä¼°
- å¯èƒ½åœ¨è£œå……ææ–™ä¸­æœ‰per-classçµæœ

---

## ğŸ” èˆ‡ä½ çš„IARNç ”ç©¶çš„å°æ¯”

### VP-ACLçš„æ–¹æ³•

| ç­–ç•¥ | å¯¦æ–½æ–¹å¼ | æ•ˆæœ |
|------|----------|------|
| Epochèª¿æ•´ | Laptop=15, MAMS=60 | é˜²æ­¢éæ“¬åˆå¤šæ•¸é¡ |
| Dropout | 0.1-0.4 (æ•¸æ“šé›†ç‰¹å®š) | æ­£å‰‡åŒ– |
| å°æ¯”å­¸ç¿’ | Dropout+å°æŠ—æ¨£æœ¬ | å¢åŠ å°‘æ•¸é¡å¤šæ¨£æ€§ |
| è©•ä¼°æŒ‡æ¨™ | Macro F1 | å¹³ç­‰è©•ä¼°æ‰€æœ‰é¡åˆ¥ |

### ä½ çš„IARNç•¶å‰æ–¹æ³•

**å¾å°ˆæ¡ˆé…ç½®æª”çœ‹åˆ°**:

```yaml
# configs/hierarchical_bert_mams.yaml
training:
  epochs: 30
  loss_type: "focal"
  focal_gamma: 2.5
  class_weights: [1.0, 8.0, 1.0]  # Neutralæ¬Šé‡Ã—8
  
# Focal Losså¯¦ç¾
class FocalLoss:
    def __init__(self, alpha=[1.0, 8.0, 1.0], gamma=2.5):
        # alpha: é¡åˆ¥æ¬Šé‡
        # gamma: é›£æ¨£æœ¬é—œæ³¨åº¦
```

**ä½ çš„å„ªå‹¢**:
âœ… **Focal Loss** - VP-ACLæ²’ç”¨,ä½ æœ‰
âœ… **Class Weights** - æ˜ç¢ºçµ¦Neutralé¡æ›´é«˜æ¬Šé‡
âœ… **æ›´ç³»çµ±åŒ–** - VP-ACLåªèª¿epoch,ä½ ç”¨äº†å¤šç¨®æŠ€è¡“

---

## ğŸ’¡ å»ºè­°:å¦‚ä½•é€²ä¸€æ­¥æ”¹é€²

### å»ºè­°ä¸€:å€Ÿé‘’VP-ACLçš„Epochç­–ç•¥

**ç•¶å‰å•é¡Œ**: 
- ä½ çš„é…ç½®æ‰€æœ‰æ•¸æ“šé›†éƒ½ç”¨30 epochs
- æ²’æœ‰æ ¹æ“šæ•¸æ“šé›†ä¸å¹³è¡¡ç¨‹åº¦èª¿æ•´

**æ”¹é€²æ–¹æ¡ˆ**:
```yaml
# MAMS (ç›¸å°å¹³è¡¡)
epochs: 40-50  # å¯ä»¥è¨“ç·´æ›´ä¹…

# Restaurants (ä¸å¹³è¡¡)
epochs: 20-25  # ææ—©åœæ­¢

# é…åˆearly stopping
patience: 8-12  # æ ¹æ“šæ•¸æ“šé›†èª¿æ•´
```

---

### å»ºè­°äºŒ:å„ªåŒ–Class Weights

**ç•¶å‰è¨­ç½®**:
```yaml
class_weights: [1.0, 8.0, 1.0]  # NeutralÃ—8
```

**å•é¡Œ**: 
- 8.0å¯èƒ½éé«˜,å°è‡´æ¨¡å‹éåº¦é—œæ³¨Neutral
- å¯èƒ½çŠ§ç‰²Positive/Negativeçš„æ€§èƒ½

**VP-ACLå•Ÿç¤º**:
- VP-ACL **ä¸ä½¿ç”¨class weights**
- è€Œæ˜¯é€šéepochèª¿æ•´+å°æ¯”å­¸ç¿’å¹³è¡¡


**æ”¹é€²æ–¹æ¡ˆB: å‹•æ…‹èª¿æ•´æ¬Šé‡**
```python
# æ ¹æ“šé©—è­‰é›†F1å‹•æ…‹èª¿æ•´
class AdaptiveWeightedLoss:
    def update_weights(self, class_f1_scores):
        # F1ä½çš„é¡åˆ¥ â†’ æ›´é«˜æ¬Šé‡
        inverse_f1 = 1.0 / (f1_scores + 0.1)
        new_weights = inverse_f1 / inverse_f1.sum() * 3
        
# å¯¦ç¾åœ¨ utils/focal_loss.py ä¸­å·²æœ‰!
```

---

### å»ºè­°ä¸‰:çµ„åˆç­–ç•¥

**æœ€ä½³å¯¦è¸**:

```yaml
# é‡å°MAMS (ç›¸å°å¹³è¡¡)
training:
  epochs: 40
  patience: 12
  loss_type: "focal"
  focal_gamma: 2.0  # é™ä½(åŸ2.5)
  class_weights: [1.0, 3.0, 1.0]  # é™ä½Neutralæ¬Šé‡
  dropout: 0.3

# é‡å°Restaurants (ä¸å¹³è¡¡)
training:
  epochs: 25  # â† å€Ÿé‘’VP-ACL,ææ—©åœæ­¢
  patience: 8
  loss_type: "focal"
  focal_gamma: 2.5
  class_weights: [1.0, 5.0, 1.0]
  dropout: 0.4  # å¢åŠ æ­£å‰‡åŒ–
```
