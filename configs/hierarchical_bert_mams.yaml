# Method 1: Hierarchical BERT on MAMS-ATSA
# 階層式BERT：利用BERT不同層的階層特性
# 在真正的多面向場景下驗證階層建模的優勢

experiment_name: "hierarchical_bert_mams"

# 模型配置
model:
  improved: "hierarchical"  # 方法 1: 階層式BERT (固定拼接)
  bert_model: "bert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  dropout: 0.4

# 數據配置
data:
  use_augmented: false  # MAMS不需要數據增強
  min_aspects: 2
  max_aspects: 8
  include_single_aspect: false  # MAMS 100%多面向
  virtual_aspect_mode: "none"
  max_text_len: 128
  max_aspect_len: 10

# 訓練配置
training:
  batch_size: 32
  accumulation_steps: 1
  epochs: 40  # MAMS 相對平衡，可充分訓練 (VP-ACL 策略)
  lr: 2.0e-5
  weight_decay: 0.05
  grad_clip: 1.0
  patience: 12
  seed: 42

  # 調度器
  use_scheduler: true
  warmup_ratio: 0.1

  # 損失函數
  loss_type: "focal"
  focal_gamma: 2.0  # 降低 (原 2.5)
  class_weights: [1.0, 3.0, 1.0]  # Neutral 3x (MAMS 相對平衡，降低權重)

  # Virtual aspect
  virtual_weight: 0.5
