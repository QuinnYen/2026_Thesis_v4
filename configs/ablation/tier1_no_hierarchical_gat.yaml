# =============================================================================
# Tier 1 核心架構消融: w/o Hierarchical GAT
# =============================================================================
# 移除三層級 GAT，改用 Hierarchical BERT (HBL) 替代
# 預期效果差異: 3-8%（驗證圖結構的必要性）
#
# 保留：Knowledge Enhancement, Inter-Aspect Module, Loss Engineering
# 替代方案：使用 HBL (Layer-wise Attention) 做階層特徵融合

experiment_name: "ablation_no_hierarchical_gat"

model:
  # 使用 HBL 替代 HKGAN
  improved: "hbl"
  bert_model:
    laptops: "data/dapt/laptop_dapt/final"
    lap16: "data/dapt/laptop_dapt/final"
    restaurants: "data/dapt/restaurant_dapt/final"
    rest16: "data/dapt/restaurant_dapt/final"
    default: "bert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  dropout: 0.3

  # HBL 參數
  num_attention_heads: 4

data:
  use_augmented: false
  use_self_training: false
  min_aspects: 2
  max_aspects: 8
  include_single_aspect: true
  virtual_aspect_mode: "overall"
  max_text_len: 128
  max_aspect_len: 10

training:
  batch_size: 16
  accumulation_steps: 2
  epochs: 30
  lr: 3.0e-5
  weight_decay: 0.01
  grad_clip: 1.0
  patience: 10
  seed: 42

  use_scheduler: true
  warmup_ratio: 0.1

  # 保留 Loss Engineering
  loss_type: "focal"
  focal_gamma: 2.0
  class_weights: [0.8, 1.8, 0.8]

  virtual_weight: 0.5

  # 保留 LLRD
  use_llrd: true
  llrd_decay: 0.95

  # 保留對比學習
  contrastive_weight: 0.1
  contrastive_temperature: 0.07

  # 保留 Logit 調整
  neutral_boost:
    laptops: 0.8
    lap16: 0.8
    restaurants: 0.6
    rest16: 1.0
    mams: 0.0
    default: 0.0

  neg_suppress:
    laptops: 0.6
    lap16: 0.6
    restaurants: 0.0
    rest16: 0.2
    mams: 0.0
    default: 0.0

  pos_suppress:
    laptops: 0.0
    lap16: 0.0
    restaurants: 0.8
    rest16: 0.5
    mams: 0.0
    default: 0.0
