# =============================================================================
# Tier 1 核心架構消融: w/o Inter-Aspect Module
# =============================================================================
# 移除 Inter-Aspect Attention + Sentiment-Aware Isolation
# 預期效果差異: 2-5%（驗證多面向處理的貢獻）
#
# 保留：Knowledge Enhancement, Hierarchical GAT, Loss Engineering
#
# 注意：這需要在模型中新增 use_inter_aspect 參數來控制

experiment_name: "ablation_no_inter_aspect"

model:
  improved: "hkgan"
  bert_model:
    laptops: "data/dapt/laptop_dapt/final"
    lap16: "data/dapt/laptop_dapt/final"
    restaurants: "data/dapt/restaurant_dapt/final"
    rest16: "data/dapt/restaurant_dapt/final"
    default: "bert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  dropout: 0.3

  # GAT 參數（保留）
  gat_heads: 4
  gat_layers: 2

  # 知識增強（保留）
  use_senticnet: true
  knowledge_weight: 0.1
  use_confidence_gate: true
  use_dynamic_gate: true
  domain: null

  # === 移除 Inter-Aspect 模組 ===
  use_inter_aspect: false
  iarm_heads: 4

data:
  use_augmented: false
  use_self_training: false
  min_aspects: 2
  max_aspects: 8
  include_single_aspect: true
  virtual_aspect_mode: "overall"
  max_text_len: 128
  max_aspect_len: 10

training:
  batch_size: 16
  accumulation_steps: 2
  epochs: 30
  lr: 3.0e-5
  weight_decay: 0.01
  grad_clip: 1.0
  patience: 10
  seed: 42

  use_scheduler: true
  warmup_ratio: 0.1

  # 保留 Loss Engineering
  loss_type: "focal"
  focal_gamma: 2.0
  class_weights: [0.8, 1.8, 0.8]

  virtual_weight: 0.5

  # 保留 LLRD
  use_llrd: true
  llrd_decay: 0.95

  # 保留對比學習
  contrastive_weight: 0.1
  contrastive_temperature: 0.07

  # 保留 Logit 調整
  neutral_boost:
    laptops: 0.8
    lap16: 0.8
    restaurants: 0.6
    rest16: 1.0
    mams: 0.0
    default: 0.0

  neg_suppress:
    laptops: 0.6
    lap16: 0.6
    restaurants: 0.0
    rest16: 0.2
    mams: 0.0
    default: 0.0

  pos_suppress:
    laptops: 0.0
    lap16: 0.0
    restaurants: 0.8
    rest16: 0.5
    mams: 0.0
    default: 0.0
