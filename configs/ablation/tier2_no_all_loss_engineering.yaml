# =============================================================================
# Tier 2 組件組合消融: w/o All Loss Engineering
# =============================================================================
# 移除所有損失函數工程：Focal Loss + Contrastive + Logit Adjustment
# 改用標準 CrossEntropy Loss
# 預期效果差異: 2-8%（驗證損失函數設計的整體貢獻）
#
# 保留：HKGAN 架構, Knowledge Enhancement

experiment_name: "ablation_no_all_loss_eng"

model:
  improved: "hkgan"
  bert_model:
    laptops: "data/dapt/laptop_dapt/final"
    lap16: "data/dapt/laptop_dapt/final"
    restaurants: "data/dapt/restaurant_dapt/final"
    rest16: "data/dapt/restaurant_dapt/final"
    default: "bert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  dropout: 0.3

  # GAT 參數（保留）
  gat_heads: 4
  gat_layers: 2

  # 知識增強（保留）
  use_senticnet: true
  knowledge_weight: 0.1
  use_confidence_gate: true
  use_dynamic_gate: true
  domain: null

  # Inter-Aspect（保留）
  iarm_heads: 4

data:
  use_augmented: false
  use_self_training: false
  min_aspects: 2
  max_aspects: 8
  include_single_aspect: true
  virtual_aspect_mode: "overall"
  max_text_len: 128
  max_aspect_len: 10

training:
  batch_size: 16
  accumulation_steps: 2
  epochs: 30
  lr: 3.0e-5
  weight_decay: 0.01
  grad_clip: 1.0
  patience: 10
  seed: 42

  use_scheduler: true
  warmup_ratio: 0.1

  # === 移除所有 Loss Engineering ===
  # 使用標準 CrossEntropy，不使用 Focal Loss
  loss_type: "ce"
  class_weights: [1.0, 1.0, 1.0]

  virtual_weight: 0.5

  # 保留 LLRD（這是優化器相關，不是 loss engineering）
  use_llrd: true
  llrd_decay: 0.95

  # 移除對比學習
  contrastive_weight: 0.0
  contrastive_temperature: 0.07

  # 移除 Logit 調整
  neutral_boost:
    default: 0.0
  neg_suppress:
    default: 0.0
  pos_suppress:
    default: 0.0
