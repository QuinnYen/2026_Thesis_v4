# Method 3: Inter-Aspect Relation Network (IARN) on MAMS-ATSA
# 核心創新：顯式建模多個 aspects 之間的交互關係
#
# 與 HPNet 的差異化：
# - HPNet: 獨立處理每個 aspect (task-specific layer selection)
# - IARN: 顯式建模 aspect 之間的依賴關係 (inter-aspect dependency)
#
# 創新點：
# 1. Aspect-to-Aspect Attention: 讓每個 aspect 關注其他 aspects
# 2. Relation-aware Gating: 動態調整自身特徵 vs 上下文特徵
# 3. Hierarchical Features: 統一的語義階層劃分 (Low/Mid/High)
#
# 適用場景：
# - MAMS: 100% 多 aspect 句子（平均 3.2 aspects/sentence）
# - 對比關係: "food is great but service is bad"
# - 因果關係: "service is slow, making experience frustrating"
#
# 預期提升：+3-5% F1 (相比 Method 1)

experiment_name: "iarn_mams"

# 模型配置
model:
  improved: "iarn"  # Inter-Aspect Relation Network
  bert_model: "distilbert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  dropout: 0.3  # 較低 dropout，因為 attention 本身有正則化效果
  num_attention_heads: 4  # Aspect-to-Aspect Attention 的 head 數量

# 數據配置
data:
  use_augmented: false
  min_aspects: 2  # IARN 專為多 aspect 設計
  max_aspects: 8
  include_single_aspect: false  # MAMS 100% 多方面
  virtual_aspect_mode: "none"
  max_text_len: 128
  max_aspect_len: 10

# 訓練配置
training:
  batch_size: 32
  accumulation_steps: 1
  epochs: 30
  lr: 2.0e-5  # 與 Method 1 保持一致
  weight_decay: 0.05  # 中等正則化
  grad_clip: 1.0
  patience: 12  # 相同的 patience
  seed: 42

  # 調度器
  use_scheduler: true
  warmup_ratio: 0.1

  # 損失函數 - 與 Method 1 保持一致以便公平對比
  loss_type: "focal"
  focal_gamma: 2.5
  class_weights: [1.0, 8.0, 1.0]  # Neutral 類別仍需加權

  # Virtual aspect
  virtual_weight: 0.5

# IARN 特有說明
#
# 架構亮點：
# 1. Hierarchical Feature Extraction
#    - Low-level (Layer 1-2): 詞法特徵
#    - Mid-level (Layer 3-4): 語義特徵
#    - High-level (Layer 5-6): 任務特徵
#
# 2. Aspect-to-Aspect Attention (4 heads)
#    - Query: 當前 aspect 的特徵
#    - Key/Value: 所有 aspects 的特徵
#    - Output: 融合其他 aspects 的上下文特徵
#
# 3. Relation-aware Gating
#    - Input: [self_features; context_features]
#    - Gate: sigmoid(MLP([self; context]))
#    - Output: gate * self + (1-gate) * context
#    - 動態平衡自身特徵與上下文特徵
#
# 與其他方法的對比：
# - Method 1 (Hierarchical): 固定 concatenation，無 aspect 交互
# - HBL (Abandoned): Layer-wise attention，過擬合
# - IARN: Aspect-to-aspect attention，專為多 aspect 優化
#
# 可視化分析：
# - aspect_attention_weights: [batch, max_aspects, max_aspects]
#   顯示哪些 aspects 之間存在強關聯
# - gate_values: [batch, max_aspects]
#   顯示每個 aspect 依賴上下文的程度
