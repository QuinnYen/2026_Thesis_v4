# Hierarchical BERT + Layer-wise Attention (UDify-inspired)
# 基於 Kondratyuk & Straka (EMNLP 2019) 的 Layer-wise Attention 機制
#
# 核心創新:
#   1. 動態學習 Low/Mid/High 三個層級的權重 (替代固定 concatenation)
#   2. 讓模型自動發現哪個層級對 ABSA 任務最重要
#   3. 提供可解釋性 (可視化層級權重分布)
#
# 預期改進:
#   - 比固定 concatenation 版本提升 0.5-1% Acc
#   - MAMS: 82.04% → 82.5-83%
#   - Restaurants: 79.49% → 80-81%
#   - Laptops: 73.59% → 74-75%

experiment_name: "hierarchical_bert_layerattn"

# 模型配置
model:
  improved: "hierarchical_layerattn"  # Improved 模型：Layer-wise Attention
  bert_model: "distilbert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  dropout: 0.5  # 使用優化版的 dropout

# 數據配置
data:
  use_augmented: false  # 關閉數據增強（優化版）
  min_aspects: 2
  max_aspects: 8
  include_single_aspect: true
  virtual_aspect_mode: "none"  # 關閉虛擬方面（優化版）
  max_text_len: 128
  max_aspect_len: 10

# 訓練配置
training:
  batch_size: 32
  accumulation_steps: 1
  epochs: 30
  lr: 2.0e-5
  weight_decay: 0.1  # 增加正則化
  grad_clip: 1.0
  patience: 5  # 更早停止
  seed: 42

  # 調度器
  use_scheduler: true
  warmup_ratio: 0.1

  # 損失函數
  loss_type: "focal"
  focal_gamma: 1.5  # 降低 gamma
  label_smoothing: 0.1  # 添加 label smoothing
  class_weights: [1.0, 3.0, 1.0]  # 降低 Neutral 權重

  # Virtual aspect
  virtual_weight: 0.5
