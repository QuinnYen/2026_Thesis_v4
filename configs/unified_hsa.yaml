# Unified Hierarchical Syntax Attention (HSA) Configuration
# 統一配置：所有數據集使用相同的超參數
#
# Method 3: HSA (Hierarchical Syntax Attention)
# 階層式語法注意力網絡
# 在語法結構上進行階層式傳播（詞級→短語級→子句級）
#
# 與 Hierarchical BERT 的差異：
#   - Hierarchical BERT: 從 BERT 不同層提取特徵
#   - HSA: 在語法樹上進行階層式傳播
#
# 創新點：結合「階層式」主題與語法結構信息

experiment_name: "hsa"

# 模型配置 (所有數據集統一)
model:
  improved: "hsa"
  bert_model: "bert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  dropout: 0.3
  # HSA 特定參數
  num_syntax_layers: 2  # 語法傳播層數

# 數據配置
data:
  use_augmented: false  # 關閉數據增強，保持原始分布
  min_aspects: 2
  max_aspects: 8
  include_single_aspect: true
  virtual_aspect_mode: "overall"
  max_text_len: 128
  max_aspect_len: 10

# 訓練配置 (所有數據集統一)
training:
  batch_size: 32
  accumulation_steps: 1
  epochs: 30
  lr: 2.0e-5
  weight_decay: 0.01
  grad_clip: 1.0
  patience: 10
  seed: 42

  # 調度器
  use_scheduler: true
  warmup_ratio: 0.1

  # 損失函數 - 統一配置
  loss_type: "focal"
  focal_gamma: 2.0
  class_weights: "auto"  # 動態計算，根據數據集自動調整

  # Virtual aspect
  virtual_weight: 0.5
