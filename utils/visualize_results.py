"""
Multi-Aspect HMAC-Net è¨“ç·´çµæœå¯è¦–åŒ–å·¥å…·

åŠŸèƒ½:
1. è¨“ç·´æ›²ç·š (Loss, F1, Accuracy)
2. æ··æ·†çŸ©é™£
3. æ¯é¡åˆ¥ F1 åˆ†æ
4. Aspect æ•¸é‡åˆ†å¸ƒåˆ†æ
"""

import json
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
from sklearn.metrics import confusion_matrix
import torch


# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")


def load_results(results_path):
    """åŠ è¼‰çµæœ JSON"""
    with open(results_path, 'r') as f:
        return json.load(f)


def plot_training_curves(history, save_dir):
    """ç¹ªè£½è¨“ç·´æ›²ç·š"""
    if not history or 'train_loss' not in history:
        print("No training history found!")
        return

    epochs = list(range(1, len(history['train_loss']) + 1))

    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # 1. Lossæ›²ç·š
    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)
    axes[0, 0].set_xlabel('Epoch', fontsize=12)
    axes[0, 0].set_ylabel('Loss', fontsize=12)
    axes[0, 0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')
    axes[0, 0].legend(fontsize=11)
    axes[0, 0].grid(True, alpha=0.3)

    # 2. Validation F1
    if 'val_f1' in history:
        axes[0, 1].plot(epochs, history['val_f1'], 'g-', label='Val F1 (Macro)', linewidth=2, marker='o')
        best_epoch = np.argmax(history['val_f1']) + 1
        best_f1 = max(history['val_f1'])
        axes[0, 1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch: {best_epoch}')
        axes[0, 1].axhline(y=best_f1, color='r', linestyle='--', alpha=0.7, label=f'Best F1: {best_f1:.4f}')
        axes[0, 1].set_xlabel('Epoch', fontsize=12)
        axes[0, 1].set_ylabel('F1 Score', fontsize=12)
        axes[0, 1].set_title('Validation F1 (Macro) Over Time', fontsize=14, fontweight='bold')
        axes[0, 1].legend(fontsize=11)
        axes[0, 1].grid(True, alpha=0.3)

    # 3. Validation Accuracy
    if 'val_acc' in history:
        axes[1, 0].plot(epochs, history['val_acc'], 'm-', label='Val Accuracy', linewidth=2, marker='s')
        axes[1, 0].set_xlabel('Epoch', fontsize=12)
        axes[1, 0].set_ylabel('Accuracy', fontsize=12)
        axes[1, 0].set_title('Validation Accuracy Over Time', fontsize=14, fontweight='bold')
        axes[1, 0].legend(fontsize=11)
        axes[1, 0].grid(True, alpha=0.3)

    # 4. Per-Class F1
    if 'val_f1_per_class' in history:
        f1_per_class = np.array(history['val_f1_per_class'])  # [epochs, 3]
        axes[1, 1].plot(epochs, f1_per_class[:, 0], 'r-', label='Negative F1', linewidth=2, marker='o')
        axes[1, 1].plot(epochs, f1_per_class[:, 1], 'y-', label='Neutral F1', linewidth=2, marker='s')
        axes[1, 1].plot(epochs, f1_per_class[:, 2], 'g-', label='Positive F1', linewidth=2, marker='^')
        axes[1, 1].set_xlabel('Epoch', fontsize=12)
        axes[1, 1].set_ylabel('F1 Score', fontsize=12)
        axes[1, 1].set_title('Per-Class F1 Over Time', fontsize=14, fontweight='bold')
        axes[1, 1].legend(fontsize=11)
        axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    save_path = save_dir / 'training_curves.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Training curves saved to: {save_path}")
    plt.close()


def plot_confusion_matrix(y_true, y_pred, save_dir, dataset_name='Test'):
    """ç¹ªè£½æ··æ·†çŸ©é™£"""
    cm = confusion_matrix(y_true, y_pred)

    # è¨ˆç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # 1. è¨ˆæ•¸æ··æ·†çŸ©é™£
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],
                xticklabels=['Negative', 'Neutral', 'Positive'],
                yticklabels=['Negative', 'Neutral', 'Positive'],
                cbar_kws={'label': 'Count'})
    axes[0].set_xlabel('Predicted Label', fontsize=12)
    axes[0].set_ylabel('True Label', fontsize=12)
    axes[0].set_title(f'{dataset_name} Set - Confusion Matrix (Counts)', fontsize=14, fontweight='bold')

    # 2. ç™¾åˆ†æ¯”æ··æ·†çŸ©é™£
    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='YlOrRd', ax=axes[1],
                xticklabels=['Negative', 'Neutral', 'Positive'],
                yticklabels=['Negative', 'Neutral', 'Positive'],
                cbar_kws={'label': 'Percentage (%)'})
    axes[1].set_xlabel('Predicted Label', fontsize=12)
    axes[1].set_ylabel('True Label', fontsize=12)
    axes[1].set_title(f'{dataset_name} Set - Confusion Matrix (Percentage)', fontsize=14, fontweight='bold')

    plt.tight_layout()
    save_path = save_dir / f'confusion_matrix_{dataset_name.lower()}.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Confusion matrix saved to: {save_path}")
    plt.close()


def plot_class_performance(results, save_dir):
    """ç¹ªè£½æ¯å€‹é¡åˆ¥çš„æ€§èƒ½åˆ†æ"""
    f1_scores = results['test_metrics']['f1_per_class']
    classes = ['Negative', 'Neutral', 'Positive']

    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # 1. F1 åˆ†æ•¸æŸ±ç‹€åœ–
    colors = ['#e74c3c', '#f39c12', '#27ae60']
    bars = axes[0].bar(classes, f1_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
    axes[0].axhline(y=np.mean(f1_scores), color='blue', linestyle='--', linewidth=2,
                    label=f'Macro F1: {np.mean(f1_scores):.4f}')
    axes[0].set_ylabel('F1 Score', fontsize=12)
    axes[0].set_title('Per-Class F1 Score (Test Set)', fontsize=14, fontweight='bold')
    axes[0].set_ylim([0, 1.0])
    axes[0].legend(fontsize=11)
    axes[0].grid(axis='y', alpha=0.3)

    # åœ¨æŸ±ç‹€åœ–ä¸Šæ¨™è¨»æ•¸å€¼
    for bar, score in zip(bars, f1_scores):
        height = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{score:.4f}',
                    ha='center', va='bottom', fontsize=11, fontweight='bold')

    # 2. æ€§èƒ½æŒ‡æ¨™å°æ¯”
    precision = results['test_metrics']['precision']
    recall = results['test_metrics']['recall']
    f1_macro = results['test_metrics']['f1_macro']
    accuracy = results['test_metrics']['accuracy']

    metrics = ['Precision', 'Recall', 'F1 (Macro)', 'Accuracy']
    values = [precision, recall, f1_macro, accuracy]
    colors_metrics = ['#3498db', '#9b59b6', '#e67e22', '#1abc9c']

    bars2 = axes[1].bar(metrics, values, color=colors_metrics, alpha=0.8, edgecolor='black', linewidth=1.5)
    axes[1].set_ylabel('Score', fontsize=12)
    axes[1].set_title('Overall Test Metrics', fontsize=14, fontweight='bold')
    axes[1].set_ylim([0, 1.0])
    axes[1].grid(axis='y', alpha=0.3)

    # åœ¨æŸ±ç‹€åœ–ä¸Šæ¨™è¨»æ•¸å€¼
    for bar, value in zip(bars2, values):
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{value:.4f}',
                    ha='center', va='bottom', fontsize=11, fontweight='bold')

    plt.tight_layout()
    save_path = save_dir / 'class_performance.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Class performance plot saved to: {save_path}")
    plt.close()


def plot_aspect_distribution(data_stats, save_dir):
    """ç¹ªè£½ aspect æ•¸é‡åˆ†å¸ƒ"""
    if 'aspect_distribution' not in data_stats:
        print("No aspect distribution data found!")
        return

    aspect_counts = data_stats['aspect_distribution']

    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # 1. æŸ±ç‹€åœ–
    num_aspects = list(aspect_counts.keys())
    counts = list(aspect_counts.values())

    bars = axes[0].bar(num_aspects, counts, color='steelblue', alpha=0.8, edgecolor='black')
    axes[0].set_xlabel('Number of Aspects', fontsize=12)
    axes[0].set_ylabel('Sample Count', fontsize=12)
    axes[0].set_title('Distribution of Aspect Counts', fontsize=14, fontweight='bold')
    axes[0].grid(axis='y', alpha=0.3)

    # æ¨™è¨»æ•¸å€¼
    for bar, count in zip(bars, counts):
        height = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width()/2., height + 5,
                    f'{int(count)}',
                    ha='center', va='bottom', fontsize=10)

    # 2. ç´¯ç©ç™¾åˆ†æ¯”
    total = sum(counts)
    cumulative_pct = np.cumsum(counts) / total * 100

    axes[1].plot(num_aspects, cumulative_pct, 'o-', color='darkorange', linewidth=2, markersize=8)
    axes[1].fill_between(num_aspects, 0, cumulative_pct, alpha=0.3, color='orange')
    axes[1].set_xlabel('Number of Aspects', fontsize=12)
    axes[1].set_ylabel('Cumulative Percentage (%)', fontsize=12)
    axes[1].set_title('Cumulative Distribution of Aspects', fontsize=14, fontweight='bold')
    axes[1].grid(True, alpha=0.3)
    axes[1].set_ylim([0, 105])

    # æ¨™è¨»é—œéµç™¾åˆ†æ¯”
    for i, (aspect, pct) in enumerate(zip(num_aspects, cumulative_pct)):
        if i % 2 == 0:  # åªæ¨™è¨»å¶æ•¸ç´¢å¼•é¿å…æ“æ“ 
            axes[1].text(aspect, pct + 2, f'{pct:.1f}%',
                        ha='center', fontsize=9)

    plt.tight_layout()
    save_path = save_dir / 'aspect_distribution.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"Aspect distribution plot saved to: {save_path}")
    plt.close()


def generate_performance_report(results, save_dir):
    """ç”Ÿæˆæ€§èƒ½å ±å‘Š Markdown"""
    report = f"""# Multi-Aspect HMAC-Net è¨“ç·´çµæœå ±å‘Š

## æ¸¬è©¦é›†æ€§èƒ½ç¸½è¦½

| æŒ‡æ¨™ | åˆ†æ•¸ |
|------|------|
| **Accuracy** | **{results['test_metrics']['accuracy']:.4f}** ({results['test_metrics']['accuracy']*100:.2f}%) |
| **F1 (Macro)** | **{results['test_metrics']['f1_macro']:.4f}** |
| **Precision** | {results['test_metrics']['precision']:.4f} |
| **Recall** | {results['test_metrics']['recall']:.4f} |
| **Best Val F1** | {results['best_val_f1']:.4f} |

## æ¯é¡åˆ¥æ€§èƒ½

| é¡åˆ¥ | F1 Score | æ€§èƒ½è©•åƒ¹ |
|------|----------|----------|
| **Negative** | {results['test_metrics']['f1_per_class'][0]:.4f} | {'âœ… è‰¯å¥½' if results['test_metrics']['f1_per_class'][0] > 0.7 else 'âš ï¸ éœ€æ”¹é€²'} |
| **Neutral** | {results['test_metrics']['f1_per_class'][1]:.4f} | {'âœ… è‰¯å¥½' if results['test_metrics']['f1_per_class'][1] > 0.7 else 'âš ï¸ éœ€æ”¹é€²'} |
| **Positive** | {results['test_metrics']['f1_per_class'][2]:.4f} | {'âœ… å„ªç§€' if results['test_metrics']['f1_per_class'][2] > 0.8 else 'âœ… è‰¯å¥½'} |

## é—œéµç™¼ç¾

### ğŸ¯ å„ªå‹¢
- **Positive é¡åˆ¥è­˜åˆ¥å„ªç§€** (F1={results['test_metrics']['f1_per_class'][2]:.4f})
- **æ•´é«”æº–ç¢ºç‡é”æ¨™** ({results['test_metrics']['accuracy']*100:.2f}%)
- **Negative é¡åˆ¥æ€§èƒ½è‰¯å¥½** (F1={results['test_metrics']['f1_per_class'][0]:.4f})

### âš ï¸ æ”¹é€²ç©ºé–“
- **Neutral é¡åˆ¥æ€§èƒ½è¼ƒå¼±** (F1={results['test_metrics']['f1_per_class'][1]:.4f})
  - åŸå› åˆ†æ: Neutral é¡åˆ¥æ¨£æœ¬è¼ƒå°‘ä¸”ç‰¹å¾µä¸æ˜é¡¯
  - æ”¹é€²æ–¹å‘: Focal Loss, Class Weighting, Data Augmentation

## æ¨¡å‹é…ç½®

```json
{json.dumps(results['args'], indent=2, ensure_ascii=False)}
```

## å¯è¦–åŒ–çµæœ

- ğŸ“Š è¨“ç·´æ›²ç·š: `training_curves.png`
- ğŸ¯ æ··æ·†çŸ©é™£: `confusion_matrix_test.png`
- ğŸ“ˆ é¡åˆ¥æ€§èƒ½: `class_performance.png`
- ğŸ“Š Aspect åˆ†å¸ƒ: `aspect_distribution.png`

---
**ç”Ÿæˆæ™‚é–“**: {Path(save_dir / 'performance_report.md').parent.name}
**æ¨¡å‹**: Multi-Aspect HMAC-Net (DistilBERT + AAHA + PMAC + IARM)
"""

    report_path = save_dir / 'performance_report.md'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"\nPerformance report saved to: {report_path}")


def main():
    """ä¸»å‡½æ•¸"""
    import argparse

    parser = argparse.ArgumentParser(description='Visualize Multi-Aspect HMAC-Net Results')
    parser.add_argument('--results', type=str, default='results/reports/multiaspect_results.json',
                        help='Path to results JSON file')
    parser.add_argument('--history', type=str, default='results/reports/training_history.json',
                        help='Path to training history JSON file (optional)')
    parser.add_argument('--predictions', type=str, default='results/reports/test_predictions.pt',
                        help='Path to test predictions (optional, for confusion matrix)')
    parser.add_argument('--output_dir', type=str, default='results/visualizations',
                        help='Output directory for plots')

    args = parser.parse_args()

    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("="*80)
    print("Multi-Aspect HMAC-Net Results Visualization")
    print("="*80)

    # åŠ è¼‰çµæœ
    results_path = Path(args.results)
    if not results_path.exists():
        print(f"Error: Results file not found: {results_path}")
        return

    results = load_results(results_path)
    print(f"\nLoaded results from: {results_path}")
    print(f"Test Accuracy: {results['test_metrics']['accuracy']:.4f}")
    print(f"Test F1 (Macro): {results['test_metrics']['f1_macro']:.4f}")

    # 1. ç¹ªè£½é¡åˆ¥æ€§èƒ½
    print("\n[1/5] Generating class performance plot...")
    plot_class_performance(results, output_dir)

    # 2. ç”Ÿæˆæ€§èƒ½å ±å‘Š
    print("\n[2/5] Generating performance report...")
    generate_performance_report(results, output_dir)

    # 3. ç¹ªè£½è¨“ç·´æ›²ç·šï¼ˆå¦‚æœæœ‰è¨“ç·´æ­·å²ï¼‰
    history_path = Path(args.history)
    if history_path.exists():
        print("\n[3/5] Generating training curves...")
        with open(history_path, 'r') as f:
            history = json.load(f)
        plot_training_curves(history, output_dir)
    else:
        print(f"\n[3/5] Skipping training curves (history file not found: {history_path})")

    # 4. ç¹ªè£½æ··æ·†çŸ©é™£ï¼ˆå¦‚æœæœ‰é æ¸¬çµæœï¼‰
    pred_path = Path(args.predictions)
    if pred_path.exists():
        print("\n[4/5] Generating confusion matrix...")
        predictions = torch.load(pred_path)
        y_true = predictions['labels']
        y_pred = predictions['predictions']
        plot_confusion_matrix(y_true, y_pred, output_dir, dataset_name='Test')
    else:
        print(f"\n[4/5] Skipping confusion matrix (predictions file not found: {pred_path})")

    # 5. Aspect åˆ†å¸ƒï¼ˆå¦‚æœæœ‰æ•¸æ“šçµ±è¨ˆï¼‰
    if 'data_stats' in results:
        print("\n[5/5] Generating aspect distribution plot...")
        plot_aspect_distribution(results['data_stats'], output_dir)
    else:
        print("\n[5/5] Skipping aspect distribution (no data stats in results)")

    print("\n" + "="*80)
    print(f"All visualizations saved to: {output_dir}")
    print("="*80)


if __name__ == '__main__':
    main()
