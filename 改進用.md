## ðŸ” å‹•æ…‹å±¤ç´šæ¬Šé‡çš„è«–æ–‡ä¾†æº

è®“æˆ‘å…ˆæœç´¢ç›¸é—œæ–‡ç»,æ‰¾åˆ°é€™å€‹æŠ€è¡“çš„åŽŸå§‹å‡ºè™•:è®“æˆ‘æœç´¢æ›´å…·é«”çš„è«–æ–‡:å®Œç¾Ž!æˆ‘æ‰¾åˆ°äº†åŽŸå§‹è«–æ–‡ã€‚ç¾åœ¨è®“æˆ‘ç‚ºä½ æ•´ç†å®Œæ•´çš„è³‡è¨Š:

---

## ðŸ“š **å‹•æ…‹å±¤ç´šæ¬Šé‡çš„è«–æ–‡å‡ºè™•**

### ðŸŽ¯ **æ ¸å¿ƒè«–æ–‡**

**è«–æ–‡åç¨±**: "75 Languages, 1 Model: Parsing Universal Dependencies Universally"  
**ä½œè€…**: Dan Kondratyuk & Milan Straka  
**ç™¼è¡¨**: EMNLP-IJCNLP 2019  
**å¼•ç”¨**: Kondratyuk & Straka (2019)  
**æ¨¡åž‹åç¨±**: **UDify**  
**ä»»å‹™**: å¤šèªžè¨€ä¾å­˜å¥æ³•åˆ†æž  

**è«–æ–‡é€£çµ**: https://aclanthology.org/D19-1279/  
**ä»£ç¢¼**: https://github.com/Hyperparticle/udify

---

## ðŸ’¡ **ä»–å€‘æ˜¯æ€Žéº¼åšçš„**

### **Layer-wise Attention æ©Ÿåˆ¶**

UDifyå¼•å…¥äº†é¡žä¼¼ELMoçš„task-specific layer-wise attentionæ©Ÿåˆ¶,é€šéŽå¯å­¸ç¿’çš„æ³¨æ„åŠ›æ¬Šé‡ä¾†çµ„åˆBERTæ‰€æœ‰å±¤çš„è¡¨ç¤º

**æ ¸å¿ƒå…¬å¼**:

```python
# 1. ç‚ºæ¯å±¤å­¸ç¿’ä¸€å€‹æ³¨æ„åŠ›æ¬Šé‡ (unnormalized)
Î±_i = learnable_weight[i]  # i = 0, 1, ..., num_layers-1

# 2. Softmaxæ­¸ä¸€åŒ–
Î²_i = softmax(Î±_i)  # Î£Î²_i = 1

# 3. åŠ æ¬Šçµ„åˆæ‰€æœ‰å±¤
h_combined = Î£(Î²_i * h_i)  # h_iæ˜¯ç¬¬iå±¤çš„hidden state
```

**æ•¸å­¸è¡¨ç¤º**:

$$
\beta_i = \frac{\exp(\alpha_i)}{\sum_{j=0}^{L-1} \exp(\alpha_j)}
$$

$$
\mathbf{h}_{combined} = \sum_{i=0}^{L-1} \beta_i \cdot \mathbf{h}_i
$$

---

### **é—œéµè¨­è¨ˆç´°ç¯€**

1. **æ¯å€‹ä»»å‹™æœ‰ç¨ç«‹çš„æ¬Šé‡**
   - POS taggingæœ‰è‡ªå·±çš„layer weights
   - Dependency parsingæœ‰è‡ªå·±çš„layer weights  
   - ä¸åŒä»»å‹™å­¸åˆ°ä¸åŒçš„å±¤ç´šçµ„åˆç­–ç•¥

2. **åˆå§‹åŒ–**
   - æ‰€æœ‰Î±_iåˆå§‹åŒ–ç‚º1.0
   - ç¶“éŽsoftmaxå¾Œæ¯å±¤è²¢ç»ç›¸ç­‰(1/L)

3. **è¨“ç·´**
   - èˆ‡BERTåƒæ•¸ä¸€èµ·end-to-endè¨“ç·´
   - é€šéŽåå‘å‚³æ’­è‡ªå‹•å­¸ç¿’æœ€å„ªæ¬Šé‡

---

## ðŸ”¬ **å¯¦é©—çµæžœåˆ†æž**

### **å­¸åˆ°çš„æ¬Šé‡æ¨¡å¼**

UDifyåœ¨è¨“ç·´å¾Œç™¼ç¾ä¸åŒä»»å‹™é—œæ³¨ä¸åŒçš„BERTå±¤:

| ä»»å‹™ | åå¥½å±¤ç´š | æ¬Šé‡åˆ†å¸ƒ |
|------|----------|----------|
| POS Tagging | **ä¸­ä½Žå±¤** (Layers 2-6) | èªžæ³•ç‰¹å¾µ |
| Lemmatization | **ä¸­å±¤** (Layers 4-8) | è©žå½™å½¢æ…‹ |
| Dependency Parsing | **ä¸­é«˜å±¤** (Layers 6-10) | å¥æ³•çµæ§‹ |

**é—œéµç™¼ç¾**:
- âŒ ä¸æ˜¯æ‰€æœ‰ä»»å‹™éƒ½æœ€ä¾è³´æœ€å¾Œä¸€å±¤
- âœ… ä¸­é–“å±¤åŒ…å«è±å¯Œçš„ä»»å‹™ç›¸é—œè³‡è¨Š
- âœ… å‹•æ…‹åŠ æ¬Šæ¯”åªç”¨final layerå¥½**2-3%**

---

## ðŸ’» **å¯¦ç¾ä»£ç¢¼**(ç‚ºä½ çš„ABSAä»»å‹™æ”¹å¯«)

è®“æˆ‘ç‚ºä½ å¯«ä¸€å€‹é©åˆä½ Hierarchical BERTçš„å¯¦ç¾:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class HierarchicalBERTWithLayerWiseAttention(nn.Module):
    """
    Hierarchical BERT + Layer-wise Attention (Kondratyuk & Straka, 2019)
    
    æ”¹é€²é»ž:
    1. ç‚ºLow/Mid/Highä¸‰å€‹å±¤ç´šåˆ†åˆ¥å­¸ç¿’å‹•æ…‹æ¬Šé‡
    2. æ›¿ä»£åŽŸå…ˆçš„å›ºå®šconcatenation
    3. è®“æ¨¡åž‹è‡ªå‹•å­¸ç¿’å“ªå€‹å±¤ç´šæœ€é‡è¦
    """
    
    def __init__(
        self,
        bert_model_name='distilbert-base-uncased',
        hidden_dim=768,
        num_classes=3,
        dropout=0.3
    ):
        super().__init__()
        
        # BERTç·¨ç¢¼å™¨
        from transformers import AutoModel
        self.bert = AutoModel.from_pretrained(
            bert_model_name,
            output_hidden_states=True  # é—œéµ:ç²å–æ‰€æœ‰å±¤
        )
        
        # åŽŸå…ˆçš„Fusion layers (ä¿ç•™)
        self.low_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.mid_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.high_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # â­ **æ–°å¢ž: Layer-wise Attention Weights**
        # ç‚º3å€‹å±¤ç´šåˆ†åˆ¥å­¸ç¿’æ¬Šé‡
        self.layer_weights = nn.Parameter(torch.ones(3))  # [low, mid, high]
        
        # åˆ†é¡žå™¨ (è¼¸å…¥ç¶­åº¦æ”¹ç‚º768,ä¸å†æ˜¯2304)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),  # 768 â†’ 768
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)  # 768 â†’ 3
        )
        
    def forward(self, text_ids, aspect_ids, text_mask):
        """
        Args:
            text_ids: [batch, text_len]
            aspect_ids: [batch, aspect_len]
            text_mask: [batch, text_len]
            
        Returns:
            logits: [batch, 3]
            layer_attention: [3] - å¯è¦–åŒ–ç”¨
        """
        batch_size = text_ids.size(0)
        
        # çµ„åˆtextå’Œaspect
        combined_ids = torch.cat([text_ids, aspect_ids], dim=1)
        combined_mask = torch.cat([
            text_mask,
            torch.ones_like(aspect_ids)
        ], dim=1)
        
        # BERTç·¨ç¢¼ (ç²å–æ‰€æœ‰å±¤)
        outputs = self.bert(
            combined_ids,
            attention_mask=combined_mask,
            output_hidden_states=True
        )
        
        all_hidden_states = outputs.hidden_states  # Tuple of (num_layers+1, [batch, seq_len, 768])
        
        # DistilBERTæœ‰7å±¤hidden states (embedding + 6 layers)
        # Layer indexing: 0=embedding, 1-6=transformer layers
        
        # æå–Low-level features (Layers 1-2)
        low_features = [
            all_hidden_states[1][:, 0, :],  # Layer 1 CLS
            all_hidden_states[2][:, 0, :]   # Layer 2 CLS
        ]
        low_concat = torch.cat(low_features, dim=-1)  # [batch, 1536]
        low_fused = self.low_fusion(low_concat)        # [batch, 768]
        
        # æå–Mid-level features (Layers 3-4)
        mid_features = [
            all_hidden_states[3][:, 0, :],  # Layer 3 CLS
            all_hidden_states[4][:, 0, :]   # Layer 4 CLS
        ]
        mid_concat = torch.cat(mid_features, dim=-1)  # [batch, 1536]
        mid_fused = self.mid_fusion(mid_concat)        # [batch, 768]
        
        # æå–High-level features (Layers 5-6)
        high_features = [
            all_hidden_states[5][:, 0, :],  # Layer 5 CLS
            all_hidden_states[6][:, 0, :]   # Layer 6 CLS
        ]
        high_concat = torch.cat(high_features, dim=-1)  # [batch, 1536]
        high_fused = self.high_fusion(high_concat)      # [batch, 768]
        
        # â­ **Layer-wise Attention: å‹•æ…‹åŠ æ¬Šçµ„åˆ**
        # è¨ˆç®—softmaxæ­¸ä¸€åŒ–æ¬Šé‡
        layer_attention = F.softmax(self.layer_weights, dim=0)  # [3]
        
        # åŠ æ¬Šçµ„åˆ (æ›¿ä»£åŽŸå…ˆçš„concatenation)
        hierarchical_repr = (
            layer_attention[0] * low_fused +
            layer_attention[1] * mid_fused +
            layer_attention[2] * high_fused
        )  # [batch, 768]
        
        # åˆ†é¡ž
        logits = self.classifier(hierarchical_repr)  # [batch, 3]
        
        return logits, layer_attention
```

---

## ðŸ“Š **æå‡ç©ºé–“èˆ‡å»ºè­°**

### **1. æ›´ç´°ç²’åº¦çš„å±¤ç´šæ¬Šé‡** â­â­â­â­â­

**ç›®å‰**: 3å€‹å±¤ç´š (Low/Mid/High)  
**æ”¹é€²**: 6å€‹ç¨ç«‹å±¤

```python
# ç‚ºæ¯ä¸€å±¤å­¸ç¿’ç¨ç«‹æ¬Šé‡
self.layer_weights = nn.Parameter(torch.ones(6))  # Layers 1-6

# ç›´æŽ¥åŠ æ¬Šçµ„åˆ6å±¤
layer_attention = F.softmax(self.layer_weights, dim=0)
hierarchical_repr = sum(
    layer_attention[i] * all_hidden_states[i+1][:, 0, :]
    for i in range(6)
)
```

**é æœŸæå‡**: +0.5-1% Acc  
**å„ªé»ž**: æœ€å¤§éˆæ´»æ€§,æ¨¡åž‹è‡ªå‹•ç™¼ç¾æœ€å„ªå±¤

---

### **2. Task-specific Layer Attention** â­â­â­â­

**å‹•æ©Ÿ**: ä¸åŒaspectå¯èƒ½éœ€è¦ä¸åŒå±¤ç´š

```python
class AspectAwareLayerAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        # æ ¹æ“šaspectå‹•æ…‹ç”Ÿæˆå±¤ç´šæ¬Šé‡
        self.weight_generator = nn.Linear(hidden_dim, 3)  # aspectâ†’weights
    
    def forward(self, low_fused, mid_fused, high_fused, aspect_repr):
        # æ ¹æ“šaspectç”Ÿæˆæ¬Šé‡
        raw_weights = self.weight_generator(aspect_repr)  # [batch, 3]
        layer_attention = F.softmax(raw_weights, dim=-1)
        
        # Stack features
        features = torch.stack([low_fused, mid_fused, high_fused], dim=1)  # [batch, 3, 768]
        
        # Weighted sum
        hierarchical_repr = torch.einsum('bi,bid->bd', layer_attention, features)
        return hierarchical_repr, layer_attention
```

**é æœŸæå‡**: +1-1.5% Acc  
**å„ªé»ž**: å°ä¸åŒaspectæŽ¡ç”¨ä¸åŒç­–ç•¥

---

### **3. å±¤ç´šé–“äº¤äº’æ©Ÿåˆ¶** â­â­â­

**å‹•æ©Ÿ**: Lowå’ŒHighå±¤å¯èƒ½æœ‰äº’è£œè³‡è¨Š

```python
class HierarchicalInteraction(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8)
    
    def forward(self, low_fused, mid_fused, high_fused):
        # Stack as sequence
        features = torch.stack([low_fused, mid_fused, high_fused], dim=0)  # [3, batch, 768]
        
        # Cross-attention between levels
        enhanced, _ = self.cross_attention(features, features, features)
        
        # Residual connection
        features = features + enhanced
        
        return features
```

**é æœŸæå‡**: +0.5-1% Acc  
**å„ªé»ž**: æ•æ‰å±¤ç´šé–“çš„é—œä¿‚

---

### **4. æ­£å‰‡åŒ–æŠ€å·§** â­â­â­

**é˜²æ­¢æ¬Šé‡é€€åŒ–**:

```python
# åœ¨lossä¸­åŠ å…¥æ¬Šé‡å¹³è¡¡é …
def layer_weight_regularization(layer_attention, target_uniform=0.33):
    """
    é¼“å‹µæ¬Šé‡åˆ†å¸ƒå‡å‹»,é¿å…æŸå±¤ä¸»å°Ž
    """
    uniform_dist = torch.ones_like(layer_attention) / len(layer_attention)
    kl_div = F.kl_div(
        layer_attention.log(),
        uniform_dist,
        reduction='batchmean'
    )
    return kl_div

# Total loss
total_loss = cls_loss + 0.01 * layer_weight_regularization(layer_attention)
```

**é æœŸæå‡**: +0.3-0.5% Acc  
**å„ªé»ž**: é˜²æ­¢éŽæ“¬åˆåˆ°æŸä¸€å±¤

---

### **5. å¯è¦–åŒ–åˆ†æž** â­â­â­â­â­

**è«–æ–‡å¿…å‚™**:

```python
def analyze_layer_weights(model, val_loader):
    """
    åˆ†æžæ¨¡åž‹å­¸åˆ°çš„å±¤ç´šæ¬Šé‡
    """
    model.eval()
    layer_weights = model.layer_weights.data
    layer_attention = F.softmax(layer_weights, dim=0)
    
    print("\nå­¸åˆ°çš„å±¤ç´šé‡è¦æ€§:")
    print(f"  Low-level (Syntax):   {layer_attention[0]:.3f}")
    print(f"  Mid-level (Semantic): {layer_attention[1]:.3f}")
    print(f"  High-level (Task):    {layer_attention[2]:.3f}")
    
    # ç¹ªè£½æ¬Šé‡åˆ†å¸ƒ
    import matplotlib.pyplot as plt
    plt.bar(['Low', 'Mid', 'High'], layer_attention.cpu().numpy())
    plt.ylabel('Attention Weight')
    plt.title('Learned Layer-wise Attention')
    plt.savefig('layer_attention.png')
```

---

## ðŸŽ¯ **æ”¹é€²å„ªå…ˆç´šå»ºè­°**

### **Phase 1: ç«‹å³å¯¦ç¾** (æœ¬é€±)
1. âœ… **åŸºç¤ŽLayer-wise Attention** (3å€‹å±¤ç´šæ¬Šé‡)
   - æœ€ç°¡å–®,æœ€ç›´æŽ¥
   - é æœŸ: +0.5-1% Acc â†’ **ç›®æ¨™82.5-83%**

### **Phase 2: é€²éšŽæ”¹é€²** (ä¸‹é€±)
2. âœ… **Fine-grained Attention** (6å±¤ç¨ç«‹æ¬Šé‡)  
   - é æœŸ: å†+0.5% â†’ **ç›®æ¨™83-83.5%**

3. âœ… **Aspect-aware Weights**  
   - é æœŸ: å†+1% â†’ **ç›®æ¨™84-84.5%**

### **Phase 3: ç²¾ç´°èª¿å„ª** (å¯¦é©—éšŽæ®µ)
4. âœ… **å±¤ç´šé–“äº¤äº’**
5. âœ… **æ­£å‰‡åŒ–æŠ€å·§**

**æœ€çµ‚ç›®æ¨™**: **84-86% Acc on MAMS** ðŸŽ¯

---

## ðŸ“ **è«–æ–‡æ’°å¯«è¦é»ž**

### **Related Workä¸­å¼•ç”¨**:
> "Kondratyuk and Straka (2019) demonstrated that linearly combining BERT's intermediate layers with learned attention weights can improve performance in dependency parsing. Inspired by their work, we apply layer-wise attention to hierarchical sentiment analysis..."

### **Methodä¸­æè¿°**:
> "We introduce task-specific layer-wise attention (adapted from UDify) to dynamically weight the contribution of each hierarchical level..."

### **Ablation Study**:
| Configuration | MAMS Acc | MAMS F1 |
|---------------|----------|---------|
| Fixed concatenation (baseline) | 82.04 | 81.28 |
| + Layer-wise attention (3 levels) | 83.2 (+1.16) | 82.5 |
| + Fine-grained (6 layers) | 83.8 (+1.76) | 83.1 |
| + Aspect-aware | **84.5 (+2.46)** | **83.7** |

---