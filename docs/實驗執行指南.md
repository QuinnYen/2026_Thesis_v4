# å¯¦é©—åŸ·è¡ŒæŒ‡å—

## ğŸ“Š æ”¯æŒçš„æ•¸æ“šé›†

ç³»çµ±ç¾åœ¨æ”¯æŒä¸‰å€‹æ•¸æ“šé›†ï¼š

| æ•¸æ“šé›† | æ¨£æœ¬æ•¸ | å¤šé¢å‘æ¯”ä¾‹ | é©ç”¨å ´æ™¯ | æ¨è–¦ç”¨é€” |
|--------|--------|-----------|---------|---------|
| **SemEval Restaurants** | ~1,500 | ~20% | ç°¡å–®å¤šé¢å‘ | è£œå……é©—è­‰ |
| **SemEval Laptops** | ~900 | ~18% | ç°¡å–®å¤šé¢å‘ | è£œå……é©—è­‰ |
| **MAMS**  | **4,297** | **100%** | çœŸå¯¦å¤šé¢å‘ | **ä¸»è¦å¯¦é©—** |

---

## ğŸ¯ æ¨¡å‹æ¶æ§‹èªªæ˜

### Baseline æ¨¡å‹

**BERT-CLS (æ¨™æº– BERT baseline)**
- ABSA é ˜åŸŸæœ€æ¨™æº–çš„ baseline
- ä½¿ç”¨ BERT çš„ [CLS] token ä½œç‚ºå¥å­è¡¨ç¤º
- åƒè€ƒæ–‡ç»: Devlin et al. (2019) BERT
- é…ç½®æ–‡ä»¶ï¼š
  - `configs/baseline_bert_cls_mams.yaml` (MAMS)
  - `configs/baseline_bert_cls.yaml` (Restaurants)

### æˆ‘å€‘çš„æ–¹æ³•

**Method 1: Hierarchical BERT (éšå±¤å¼BERT)**
- å¾ BERT çš„ Low/Mid/High å±¤æå–å¤šå±¤ç´šç‰¹å¾µ
- ä½¿ç”¨å›ºå®š concatenation çµ„åˆä¸‰å€‹å±¤ç´š
- è­‰æ˜éšå±¤ç‰¹å¾µæå–çš„æœ‰æ•ˆæ€§
- é…ç½®æ–‡ä»¶ï¼š
  - `configs/hierarchical_bert_mams.yaml` (MAMS)
  - `configs/hierarchical_bert.yaml` (Restaurants)

**Method 2: HBL (Hierarchical BERT + Layer-wise Attention)** âŒ **å·²æ”¾æ£„**
- åœ¨ Hierarchical BERT åŸºç¤ä¸ŠåŠ å…¥ Layer-wise Attention
- åŸºæ–¼ UDify (Kondratyuk & Straka, EMNLP 2019) çš„å‹•æ…‹æ¬Šé‡æ©Ÿåˆ¶
- **å¯¦é©—çµæœ**: åš´é‡éæ“¬åˆï¼ŒF1 ä¸‹é™ 13.16%
- **å¤±æ•—åŸå› **: æ¨¡å‹å®¹é‡ï¼ˆMultiheadAttentionï¼‰è¶…éæ•¸æ“šé›†è¦æ¨¡
- **è«–æ–‡åƒ¹å€¼**: ä½œç‚ºæ¶ˆèå¯¦é©—çš„è² é¢çµæœ
- é…ç½®æ–‡ä»¶ï¼š
  - `configs/hierarchical_bert_layerattn_mams.yaml` (MAMS)

**Method 3: IARN (Inter-Aspect Relation Network)**
- é¡¯å¼å»ºæ¨¡å¤šå€‹ aspects ä¹‹é–“çš„äº¤äº’é—œä¿‚
- **æ ¸å¿ƒå‰µæ–°**:
  1. Aspect-to-Aspect Attentionï¼ˆè®“ aspects äº’ç›¸é—œæ³¨ï¼‰
  2. Relation-aware Gatingï¼ˆå‹•æ…‹èª¿æ•´è‡ªèº« vs ä¸Šä¸‹æ–‡ç‰¹å¾µï¼‰
  3. Hierarchical Feature Extractionï¼ˆçµ±ä¸€çš„èªç¾©éšå±¤ï¼‰
- **èˆ‡ HPNet å·®ç•°åŒ–**: å°ˆæ³¨æ–¼ inter-aspect dependency modeling
- **å¯¦é©—çµæœ**: MAMS F1 = 0.8400
- **é™åˆ¶**: åœ¨å–® aspect ç‚ºä¸»çš„æ•¸æ“šé›†ï¼ˆå¦‚ Restaurants ~20%ï¼‰ä¸Šè¡¨ç¾ä¸ä½³
- é…ç½®æ–‡ä»¶ï¼š
  - `configs/iarn_mams.yaml` (MAMS)

**Method 4: VP-IARN (Vector Projection enhanced IARN)** â­ **ä¸»è¦è²¢ç»**
- çµåˆå‘é‡æŠ•å½±èˆ‡ Aspect-to-Aspect Attentionï¼Œçµ±ä¸€è™•ç†å–®/å¤š aspect å ´æ™¯
- **æ ¸å¿ƒå‰µæ–°**ï¼ˆåŸºæ–¼ VP-ACL 2025 æ€æƒ³ï¼‰:
  1. Vector Projectionï¼ˆå¾èšåˆå‘é‡ä¸­æå–ç›®æ¨™ aspect èªç¾©ï¼‰
  2. Aspect-to-Aspect Attentionï¼ˆå¤š aspect äº¤äº’å»ºæ¨¡ï¼‰
  3. Adaptive Fusionï¼ˆå¯å­¸ç¿’çš„èåˆæ¬Šé‡ï¼Œæ ¹æ“š aspect æ•¸é‡å‹•æ…‹èª¿æ•´ï¼‰
- **èˆ‡ VP-ACL å·®ç•°**: VP-ACL ç´”æŠ•å½±ï¼ŒVP-IARN = æŠ•å½± + Attention + è‡ªé©æ‡‰èåˆ
- **èˆ‡ IARN å·®ç•°**: IARN å–® aspect å¤±æ•ˆï¼ŒVP-IARN çµ±ä¸€è™•ç†æ‰€æœ‰å ´æ™¯
- **é æœŸçµæœ**:
  - MAMS F1 â‰ˆ 0.8450ï¼ˆ+0.5% vs IARNï¼‰
  - Restaurants F1 â‰ˆ 0.7280ï¼ˆè¶…è¶Š Baseline 0.7220ï¼‰
- é…ç½®æ–‡ä»¶ï¼š
  - `configs/vp_iarn_restaurants.yaml` (Restaurants) â­
  - `configs/vp_iarn_mams.yaml` (MAMS) â­

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æ¨è–¦åŸ·è¡Œé †åº

#### MAMS æ•¸æ“šé›†ï¼ˆ100% å¤šé¢å‘ï¼‰

```bash
# 1. Baseline: BERT-CLS
python experiments/train_from_config.py --config configs/baseline_bert_cls_mams.yaml --dataset mams

# 2. Method 1: Hierarchical BERT
python experiments/train_from_config.py --config configs/hierarchical_bert_mams.yaml --dataset mams

# 3. Method 3: IARN
python experiments/train_from_config.py --config configs/iarn_mams.yaml --dataset mams

# 4. Method 4: VP-IARN â­ (æ¨è–¦)
python experiments/train_from_config.py --config configs/vp_iarn_mams.yaml --dataset mams

# (å¯é¸) Method 2: HBL - åƒ…ç”¨æ–¼è«–æ–‡ä¸­çš„æ¶ˆèå¯¦é©—/è² é¢çµæœ
python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn_mams.yaml --dataset mams
```

#### SemEval Restaurants æ•¸æ“šé›†ï¼ˆ~20% å¤šé¢å‘ï¼‰â­ **VP-IARN é©—è­‰å ´æ™¯**

```bash
# 1. Baseline: BERT-CLS
python experiments/train_from_config.py --config configs/baseline_bert_cls.yaml --dataset restaurants

# 2. Method 1: Hierarchical BERT
python experiments/train_from_config.py --config configs/hierarchical_bert.yaml --dataset restaurants

# 3. Method 4: VP-IARN (æ¨è–¦) â­ çµ±ä¸€è™•ç†å–®/å¤šé¢å‘
python experiments/train_multiaspect.py --dataset restaurants --improved vp_iarn --epochs 30 --batch_size 32 --lr 2e-5 --dropout 0.3 --weight_decay 0.05 --patience 10 --loss_type focal --focal_gamma 2.0 --include_single_aspect
```

#### ç”Ÿæˆç¶œåˆå ±å‘Š

```bash
# MAMS æ•¸æ“šé›†å ±å‘Š
python experiments/generate_comprehensive_report.py --dataset mams
cat results/ç¶œåˆå ±å‘Š_mams.txt

# Restaurants æ•¸æ“šé›†å ±å‘Š
python experiments/generate_comprehensive_report.py --dataset restaurants
cat results/ç¶œåˆå ±å‘Š_restaurants.txt
```

---

## ğŸ“‹ å¯¦é©—çµæœä½ç½®

è¨“ç·´å®Œæˆå¾Œï¼Œçµæœæœƒä¿å­˜åœ¨ä»¥ä¸‹ä½ç½®ï¼š

```
results/
â”œâ”€â”€ baseline/                    # Baseline æ¨¡å‹çµæœ
â”‚   â””â”€â”€ mams/
â”‚       â””â”€â”€ YYYYMMDD_HHMMSS_baseline_bert_cls_*/
â”‚           â”œâ”€â”€ checkpoints/         # æ¨¡å‹æª¢æŸ¥é»
â”‚           â”œâ”€â”€ visualizations/      # è¨“ç·´æ›²ç·š
â”‚           â””â”€â”€ reports/             # å¯¦é©—å ±å‘Š
â”‚
â””â”€â”€ improved/                    # Improved æ¨¡å‹çµæœ
    â””â”€â”€ mams/
        â”œâ”€â”€ YYYYMMDD_HHMMSS_improved_hierarchical_*/      # Method 1
        â”œâ”€â”€ YYYYMMDD_HHMMSS_improved_hierarchical_layerattn_*/  # Method 2 (HBL)
        â””â”€â”€ YYYYMMDD_HHMMSS_improved_iarn_*/              # Method 3 (IARN) â­
            â”œâ”€â”€ checkpoints/
            â”‚   â””â”€â”€ best_model.pt
            â”œâ”€â”€ visualizations/
            â”‚   â””â”€â”€ comprehensive_training_metrics.png
            â””â”€â”€ reports/
                â”œâ”€â”€ experiment_config.json
                â”œâ”€â”€ experiment_results.json
                â””â”€â”€ experiment_summary.txt
```

**æ¯å€‹å¯¦é©—ç›®éŒ„åŒ…å«**ï¼š
- `checkpoints/` - æœ€ä½³æ¨¡å‹æ¬Šé‡
- `visualizations/` - è¨“ç·´æ›²ç·šåœ–å’Œæ€§èƒ½åˆ†æåœ–
- `reports/` - JSON æ ¼å¼çš„å¯¦é©—é…ç½®å’Œçµæœ

---

## âš™ï¸ MAMS æ•¸æ“šé›†çµ±ä¸€é…ç½®åƒæ•¸

æ‰€æœ‰é…ç½®ä½¿ç”¨ç›¸åŒçš„åŸºç¤è¨“ç·´åƒæ•¸ï¼ˆç¢ºä¿å…¬å¹³æ¯”è¼ƒï¼‰ï¼š

| åƒæ•¸ | Baseline & Method 1 | Method 3 (IARN) | èªªæ˜ |
|------|---------------------|-----------------|------|
| `epochs` | 30 | 30 | è¨“ç·´è¼ªæ•¸ |
| `batch_size` | 32 | 32 | æ‰¹æ¬¡å¤§å° |
| `learning_rate` | 2e-5 | 2e-5 | å­¸ç¿’ç‡ |
| `dropout` | 0.4-0.45 | **0.3** | IARN ä½¿ç”¨è¼ƒä½ dropout |
| `weight_decay` | 0.05 | 0.05 | L2 æ­£å‰‡åŒ– |
| `patience` | 12 | 12 | Early stopping |
| `loss_type` | focal | focal | Focal Loss |
| `focal_gamma` | 2.5 | 2.5 | Focal Loss gamma |
| `class_weights` | [1.0, 8.0, 1.0] | [1.0, 8.0, 1.0] | Neg/Neu/Pos æ¬Šé‡ |
| `num_attention_heads` | - | **4** | IARN å°ˆç”¨ |

**é—œéµå·®ç•°**ï¼š
- IARN ä½¿ç”¨è¼ƒä½çš„ dropout (0.3)ï¼Œå› ç‚º Aspect-to-Aspect Attention æœ¬èº«æä¾›æ­£å‰‡åŒ–æ•ˆæœ
- IARN æ·»åŠ äº† `num_attention_heads` åƒæ•¸ï¼ˆMultiheadAttention çš„ head æ•¸é‡ï¼‰

---

## ğŸ“Š ç•¶å‰å¯¦é©—çµæœ (MAMS)

| æ¨¡å‹ | Test F1 | Test Acc | ç›¸å° Baseline | ç‹€æ…‹ |
|------|---------|----------|---------------|------|
| **Baseline (BERT-CLS)** | 0.8217 | 0.8272 | - | âœ… |
| **Method 1 (Hierarchical)** | 0.8349 | 0.8392 | +1.6% | âœ… |
| **Method 2 (HBL)** | 0.7033 | 0.7160 | -14.4% | âŒ å·²æ”¾æ£„ |
| **Method 3 (IARN)** | **0.8400** | **0.8445** | **+2.22%** | â­ **æœ€ä½³** |

### é—œéµç™¼ç¾

1. **Method 1 è­‰æ˜éšå±¤ç‰¹å¾µæœ‰æ•ˆ**: +1.6% F1
2. **Method 2 (HBL) å¤±æ•—**: éæ“¬åˆå°è‡´æ€§èƒ½ä¸‹é™
3. **Method 3 (IARN) æœ€ä½³**: Inter-aspect modeling å¸¶ä¾†é¡å¤–æå‡

---

## ğŸ”¬ IARN ç‰¹æœ‰åŠŸèƒ½

### 1. æ¸¬è©¦ IARN æ¨¡å‹

```bash
# é‹è¡Œå–®å…ƒæ¸¬è©¦
python experiments/test_iarn.py
```

**æ¸¬è©¦å…§å®¹**:
- âœ… æ¨¡å‹å‰µå»ºï¼ˆ94M åƒæ•¸ï¼‰
- âœ… Forward pass
- âœ… Aspect-to-Aspect Attention weights
- âœ… Gate values åˆ†å¸ƒ
- âœ… Masking æ©Ÿåˆ¶

### 2. åˆ†æ IARN è¼¸å‡º

IARN æ¨¡å‹è¿”å›é¡å¤–çš„åˆ†æä¿¡æ¯ï¼š

```python
logits, extras = model(text_ids, text_mask, aspect_ids, aspect_mask_input, aspect_mask)

# extras åŒ…å«:
# - 'aspect_attention_weights': [batch, max_aspects, max_aspects]
#   é¡¯ç¤º aspects ä¹‹é–“çš„é—œæ³¨åº¦
# - 'gate_values': [batch, max_aspects]
#   é¡¯ç¤ºæ¯å€‹ aspect ä¾è³´ä¸Šä¸‹æ–‡çš„ç¨‹åº¦ (0-1)
# - 'avg_gate': float
#   å¹³å‡ gate å€¼
```

### 3. å¯è¦–åŒ– Attention Weightsï¼ˆæœªä¾†å·¥ä½œï¼‰

```python
# å¯ä»¥å‰µå»ºè…³æœ¬å¯è¦–åŒ– aspect-to-aspect attention
# ä¾‹å¦‚: é¡¯ç¤º "food" aspect å¦‚ä½•é—œæ³¨ "service" aspect
```

---

## ğŸ› ï¸ HBL è¨ºæ–·å·¥å…·ï¼ˆç”¨æ–¼è«–æ–‡æ¶ˆèå¯¦é©—ï¼‰

å¦‚æœéœ€è¦åˆ†æ HBL çš„å¤±æ•—åŸå› ï¼ˆè«–æ–‡å¯«ä½œæ™‚ï¼‰ï¼š

```bash
# è¨ºæ–· HBL æ¨¡å‹çš„ layer attention æ¬Šé‡
python experiments/diagnose_hbl.py \
    --checkpoint results/improved/mams/20251121_184427_improved_hierarchical_layerattn_*/checkpoints/best_model.pt
```

**è¼¸å‡ºåˆ†æ**:
- Raw weights: [0.186, 0.306, 0.507]
- Softmax weights: [18.6%, 30.6%, 50.7%]
- è¨ºæ–·: æ¬Šé‡å­¸ç¿’æˆåŠŸï¼Œä½†æ¨¡å‹éæ“¬åˆ

---

## ğŸ’¡ å¯¦é©—å»ºè­°

### æ¨è–¦å¯¦é©—é †åº

#### 1. å®Œæ•´çš„ MAMS å¯¦é©—ï¼ˆå»ºç«‹ä¸»è¦çµæœï¼‰

```bash
# æŒ‰é †åºåŸ·è¡Œ
python experiments/train_from_config.py --config configs/baseline_bert_cls_mams.yaml --dataset mams
python experiments/train_from_config.py --config configs/hierarchical_bert_mams.yaml --dataset mams
python experiments/train_from_config.py --config configs/iarn_mams.yaml --dataset mams

# (å¯é¸) HBL ä½œç‚ºè² é¢çµæœ
python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn_mams.yaml --dataset mams

# ç”Ÿæˆå ±å‘Š
python experiments/generate_comprehensive_report.py --dataset mams
```

#### 2. (å¯é¸) å…¶ä»–æ•¸æ“šé›†é©—è­‰

```bash
# å¦‚æœæ™‚é–“å…è¨±ï¼Œå¯ä»¥åœ¨ Restaurants/Laptops ä¸Šé©—è­‰ IARN
# ä½† MAMS æ˜¯ä¸»è¦å¯¦é©—æ•¸æ“šé›†
```

---

## ğŸ” é æœŸçµæœèˆ‡åˆ†æ

### MAMS æ•¸æ“šé›†ï¼ˆ100% å¤šé¢å‘ï¼‰

**Baseline â†’ Method 1**:
- æå‡: +1.6% F1
- è­‰æ˜: éšå±¤ç‰¹å¾µæå–åœ¨å¤šé¢å‘å ´æ™¯ä¸‹æœ‰æ•ˆ

**Method 1 â†’ Method 3 (IARN)**:
- æå‡: +0.61% F1ï¼ˆé¡å¤–æå‡ï¼‰
- è­‰æ˜: Inter-aspect modeling æ•æ‰äº† aspects ä¹‹é–“çš„é—œä¿‚

**Method 2 (HBL) å¤±æ•—åˆ†æ**:
- ä¸‹é™: -13.16% F1
- åŸå› : æ¨¡å‹å®¹é‡éå¤§ï¼ˆ3å€‹ MultiheadAttentionï¼‰è¶…éæ•¸æ“šé›†è¦æ¨¡
- åƒ¹å€¼: ä½œç‚ºæ¶ˆèå¯¦é©—çš„è² é¢çµæœï¼Œè­‰æ˜ç°¡å–®æ–¹æ³•åœ¨å°æ•¸æ“šé›†ä¸Šæ›´æœ‰æ•ˆ

### è«–æ–‡ä¸­çš„å‘ˆç¾

```
å¯¦é©—çµæœé¡¯ç¤º:
1. Hierarchical BERT (Method 1) ç›¸è¼ƒ Baseline æå‡ 1.6%ï¼Œè­‰æ˜éšå±¤ç‰¹å¾µæœ‰æ•ˆ
2. IARN (Method 3) é€²ä¸€æ­¥æå‡è‡³ F1=0.8400ï¼Œè­‰æ˜ inter-aspect modeling çš„åƒ¹å€¼
3. HBL (Method 2) åš´é‡éæ“¬åˆï¼Œèªªæ˜è¤‡é›œçš„ attention æ©Ÿåˆ¶ä¸é©åˆå°è¦æ¨¡æ•¸æ“šé›†

èˆ‡ HPNet (2021) çš„å·®ç•°:
- HPNet: ç¨ç«‹è™•ç†æ¯å€‹ aspect (task-specific layer selection)
- IARN: é¡¯å¼å»ºæ¨¡ aspects ä¹‹é–“çš„ä¾è³´é—œä¿‚ (inter-aspect dependency)
- é©ç”¨å ´æ™¯: MAMS (100% å¤š aspect) vs HPNet (æ··åˆå ´æ™¯)
```

---

## ğŸ› ï¸ å¸¸è¦‹å•é¡Œ

### Q1: ç‚ºä»€éº¼æ”¾æ£„ HBLï¼Ÿ

**ç­”**: HBL ä½¿ç”¨ 3 å€‹ MultiheadAttention å±¤é€²è¡Œ aspect-aware fusionï¼Œåƒæ•¸é‡éå¤§ã€‚åœ¨ MAMS æ•¸æ“šé›†ï¼ˆ4,297 è¨“ç·´æ¨£æœ¬ï¼‰ä¸Šåš´é‡éæ“¬åˆï¼ˆVal Loss: 0.596 â†’ 2.016ï¼‰ï¼ŒTest F1 ä¸‹é™è‡³ 0.7033ã€‚

é›–ç„¶ layer weights æˆåŠŸå­¸ç¿’ï¼ˆHigh-level æ¬Šé‡ 50.7%ï¼‰ï¼Œä½†æ¨¡å‹è¨˜æ†¶è¨“ç·´é›†è€Œéå­¸ç¿’æ³›åŒ–ç‰¹å¾µã€‚é€™è­‰æ˜äº† Occam's Razor åŸå‰‡ï¼šç°¡å–®æ–¹æ³•ï¼ˆMethod 1 çš„ Linear projectionï¼‰å„ªæ–¼è¤‡é›œ attention æ©Ÿåˆ¶ã€‚

### Q2: IARN ç‚ºä»€éº¼èƒ½æˆåŠŸï¼Ÿ

**ç­”**: IARN çš„è¨­è¨ˆæ›´åŠ é«˜æ•ˆï¼š
1. ä½¿ç”¨è¼•é‡ç´šçš„ Linear projectionsï¼ˆè€Œé MultiheadAttentionï¼‰æå–éšå±¤ç‰¹å¾µ
2. Aspect-to-Aspect Attention åªåœ¨æœ€å¾Œéšæ®µæ‡‰ç”¨ï¼ˆè€Œéæ¯å±¤éƒ½ç”¨ï¼‰
3. Relation-aware Gating åƒæ•¸å°‘ä½†æœ‰æ•ˆ
4. ç¸½åƒæ•¸é‡é©ä¸­ï¼Œé¿å…éæ“¬åˆ

### Q3: å¦‚ä½•èª¿æ•´ IARN è¶…åƒæ•¸ï¼Ÿ

ä¿®æ”¹ `configs/iarn_mams.yaml`ï¼š

```yaml
model:
  dropout: 0.3               # èª¿æ•´ dropoutï¼ˆ0.2-0.4ï¼‰
  num_attention_heads: 4     # èª¿æ•´ attention headsï¼ˆ2, 4, 8ï¼‰

training:
  batch_size: 32            # æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´
  lr: 2.0e-5                # å­¸ç¿’ç‡
  weight_decay: 0.05        # L2 æ­£å‰‡åŒ–
```

### Q4: è¨˜æ†¶é«”ä¸è¶³æ€éº¼è¾¦ï¼Ÿ

1. é™ä½ `batch_size`ï¼ˆ32 â†’ 16ï¼‰
2. é™ä½ `num_attention_heads`ï¼ˆ4 â†’ 2ï¼‰
3. ä½¿ç”¨ DistilBERTï¼ˆå·²æ˜¯é è¨­ï¼‰

---

## ğŸ“š é…ç½®æ–‡ä»¶èªªæ˜

### ç•¶å‰å¯ç”¨é…ç½®

| æ–‡ä»¶ | æ¨¡å‹ | æ•¸æ“šé›† | èªªæ˜ |
|------|------|--------|------|
| `baseline_bert_cls_mams.yaml` | Baseline | MAMS | BERT-CLS baseline |
| `hierarchical_bert_mams.yaml` | Method 1 | MAMS | éšå±¤å¼ BERTï¼ˆå›ºå®šæ‹¼æ¥ï¼‰|
| `hierarchical_bert_layerattn_mams.yaml` | Method 2 | MAMS | HBLï¼ˆå·²æ”¾æ£„ï¼Œåƒ…ç”¨æ–¼æ¶ˆèï¼‰|
| `iarn_mams.yaml` | Method 3 | MAMS | **IARNï¼ˆæ¨è–¦ï¼‰** â­ |

---

## ğŸ“ è«–æ–‡æ’°å¯«å»ºè­°

### å¯¦é©—ç« ç¯€çµæ§‹

```
4. Experiments
4.1 Datasets and Settings
    - MAMS (100% multi-aspect, ä¸»è¦æ•¸æ“šé›†)
    - è¨“ç·´é…ç½®è¡¨æ ¼

4.2 Baseline and Methods
    - Baseline: BERT-CLS
    - Method 1: Hierarchical BERT
    - Method 2: HBL (Failed approach)
    - Method 3: IARN (Main contribution)

4.3 Main Results
    - MAMS æ•¸æ“šé›†çµæœè¡¨æ ¼
    - IARN é”åˆ°æœ€ä½³æ€§èƒ½ (F1=0.8400)

4.4 Ablation Studies
    - Method 1 vs Baseline: éšå±¤ç‰¹å¾µçš„ä½œç”¨ (+1.6%)
    - Method 3 vs Method 1: Inter-aspect modeling çš„ä½œç”¨ (+0.61%)
    - Method 2 (HBL) å¤±æ•—åˆ†æ: æ¨¡å‹å®¹é‡ vs æ•¸æ“šè¦æ¨¡

4.5 Analysis
    - Attention weights å¯è¦–åŒ–ï¼ˆIARNï¼‰
    - Gate values åˆ†å¸ƒåˆ†æ
    - Case studies: aspects ä¹‹é–“çš„é—œä¿‚

4.6 Comparison with HPNet (2021)
    - ä»»å‹™å®šç¾©å·®ç•°
    - æ–¹æ³•è«–å·®ç•°
    - Inter-aspect modeling vs Independent processing
```

### é—œéµè³£é»

âœ… **æ˜ç¢ºçš„ç ”ç©¶è²¢ç»**ï¼š
1. **Method 1**: è­‰æ˜éšå±¤ç‰¹å¾µæå–åœ¨ multi-aspect ABSA ä¸­æœ‰æ•ˆ
2. **Method 3 (IARN)**: é¦–æ¬¡é¡¯å¼å»ºæ¨¡ aspects ä¹‹é–“çš„äº¤äº’é—œä¿‚
3. **èˆ‡ HPNet å·®ç•°åŒ–**: å°ˆæ³¨æ–¼ inter-aspect dependencyï¼Œè€Œé task-specific layers

âœ… **å®Œæ•´çš„å¯¦é©—è¨­è¨ˆ**ï¼š
- 3 å€‹æ–¹æ³• + 1 å€‹ baseline
- è² é¢çµæœï¼ˆHBLï¼‰ä½œç‚ºæ¶ˆèå¯¦é©—
- MAMS (100% multi-aspect) æ•¸æ“šé›†

âœ… **å¯è§£é‡‹æ€§**ï¼š
- Attention weights é¡¯ç¤º aspects ä¹‹é–“çš„é—œä¿‚
- Gate values é¡¯ç¤ºæ¨¡å‹å¦‚ä½•å¹³è¡¡è‡ªèº« vs ä¸Šä¸‹æ–‡ç‰¹å¾µ

---

## ğŸ“ å¿«é€Ÿå‘½ä»¤å‚™å¿˜

### MAMS æ•¸æ“šé›†ï¼ˆ100% å¤šé¢å‘ï¼‰

```bash
# è¨“ç·´ Baseline
python experiments/train_from_config.py --config configs/baseline_bert_cls_mams.yaml --dataset mams

# è¨“ç·´ Hierarchical BERT
python experiments/train_from_config.py --config configs/hierarchical_bert_mams.yaml --dataset mams

# è¨“ç·´ IARN
python experiments/train_from_config.py --config configs/iarn_mams.yaml --dataset mams

# è¨“ç·´ VP-IARN â­ (æ¨è–¦)
python experiments/train_from_config.py --config configs/vp_iarn_mams.yaml --dataset mams

# ç”Ÿæˆå ±å‘Š
python experiments/generate_comprehensive_report.py --dataset mams
```

### Restaurants æ•¸æ“šé›†ï¼ˆ~20% å¤šé¢å‘ï¼‰â­ VP-IARN é©—è­‰

```bash
# è¨“ç·´ Baseline
python experiments/train_from_config.py --config configs/baseline_bert_cls.yaml --dataset restaurants

# è¨“ç·´ VP-IARN â­ (å–®è¡Œå‘½ä»¤)
python experiments/train_multiaspect.py --dataset restaurants --improved vp_iarn --epochs 30 --batch_size 32 --lr 2e-5 --dropout 0.3 --weight_decay 0.05 --patience 10 --loss_type focal --focal_gamma 2.0 --include_single_aspect

# ç”Ÿæˆå ±å‘Š
python experiments/generate_comprehensive_report.py --dataset restaurants
```

### æ¸¬è©¦èˆ‡è¨ºæ–·

```bash
# æ¸¬è©¦ IARN æ¨¡å‹
python experiments/test_iarn.py

# è¨ºæ–· HBLï¼ˆç”¨æ–¼è«–æ–‡æ¶ˆèå¯¦é©—ï¼‰
python experiments/diagnose_hbl.py --checkpoint path/to/hbl/best_model.pt
```

---

## ğŸ”¬ VP-IARN ç‰¹æœ‰åŠŸèƒ½

### æ¶æ§‹èªªæ˜

```
VP-IARN Forward æµç¨‹:
1. Hierarchical Feature Extraction (Low/Mid/High BERT layers)
2. Multi-Aspect Aggregation (èšåˆæ‰€æœ‰ aspects)
3. Vector Projection: proj(u,v) = (uÂ·v / ||v||Â²) * v
4. Aspect-to-Aspect Attention (aspects äº¤äº’å»ºæ¨¡)
5. Relation-aware Gating (è‡ªèº« vs ä¸Šä¸‹æ–‡)
6. Adaptive Fusion (å¯å­¸ç¿’çš„ Î± æ§åˆ¶æŠ•å½± vs æ³¨æ„åŠ›æ¬Šé‡)
7. Classification
```

### èˆ‡ VP-ACL (2025) çš„å·®ç•°

| æ–¹é¢ | VP-ACL | VP-IARN |
|------|--------|---------|
| å‘é‡æŠ•å½± | âœ… | âœ… |
| Aspect-to-Aspect Attention | âŒ | âœ… |
| è‡ªé©æ‡‰èåˆ | âŒ | âœ… (learnable Î±) |
| é©ç”¨å ´æ™¯ | å–® aspect | å–®/å¤š aspect çµ±ä¸€ |

### åˆ†æ VP-IARN è¼¸å‡º

```python
logits, extras = model(text_ids, text_mask, aspect_ids, aspect_mask_input, aspect_mask)

# extras åŒ…å«:
# - 'aspect_attention_weights': [batch, max_aspects, max_aspects]
# - 'gate_values': [batch, max_aspects]
# - 'adaptive_alpha': float (å­¸ç¿’åˆ°çš„èåˆä¿‚æ•¸)
# - 'dynamic_alpha_mean': float (å‹•æ…‹æ¬Šé‡å¹³å‡å€¼)
# - 'n_multi_aspect_samples': int
# - 'n_single_aspect_samples': int
```

---

**æœ€å¾Œæ›´æ–°**: 2025-11-22
**ç•¶å‰ç‹€æ…‹**: VP-IARN å¯¦ç¾å®Œæˆï¼Œå¾…è¨“ç·´é©—è­‰
