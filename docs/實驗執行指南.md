# å¯¦é©—åŸ·è¡ŒæŒ‡å—

## ğŸ“Š æ”¯æŒçš„æ•¸æ“šé›†

ç³»çµ±ç¾åœ¨æ”¯æŒä¸‰å€‹æ•¸æ“šé›†ï¼š

| æ•¸æ“šé›† | æ¨£æœ¬æ•¸ | å¤šé¢å‘æ¯”ä¾‹ | é©ç”¨å ´æ™¯ | æ¨è–¦ç”¨é€” |
|--------|--------|-----------|---------|---------|
| **SemEval Restaurants** | ~1,500 | ~20% | ç°¡å–®å¤šé¢å‘ | è£œå……é©—è­‰ |
| **SemEval Laptops** | ~900 | ~18% | ç°¡å–®å¤šé¢å‘ | è£œå……é©—è­‰ |
| **MAMS**  | **4,297** | **100%** | çœŸå¯¦å¤šé¢å‘ | **ä¸»è¦å¯¦é©—** |

---

## ğŸ¯ æ¨¡å‹æ¶æ§‹èªªæ˜

### Baseline æ¨¡å‹

**BERT-CLS (æ¨™æº– BERT baseline)**
- ABSA é ˜åŸŸæœ€æ¨™æº–çš„ baseline
- ä½¿ç”¨ BERT çš„ [CLS] token ä½œç‚ºå¥å­è¡¨ç¤º
- åƒè€ƒæ–‡ç»: Devlin et al. (2019) BERT
- é…ç½®æ–‡ä»¶ï¼š
  - `configs/baseline_bert_cls.yaml` (SemEval)
  - `configs/baseline_bert_cls_mams.yaml` (MAMS)

### æˆ‘å€‘çš„æ–¹æ³•

**æ–¹æ³• 1: Hierarchical BERT (éšå±¤å¼BERT)**
- å¾ BERT çš„ Low/Mid/High å±¤æå–å¤šå±¤ç´šç‰¹å¾µ
- ä½¿ç”¨å›ºå®šconcatenationçµ„åˆä¸‰å€‹å±¤ç´š
- è­‰æ˜éšå±¤ç‰¹å¾µæå–çš„æœ‰æ•ˆæ€§
- é…ç½®æ–‡ä»¶ï¼š
  - `configs/hierarchical_bert.yaml` (SemEval)
  - `configs/hierarchical_bert_mams.yaml` (MAMS)

**æ–¹æ³• 2: Hierarchical BERT + Layer-wise Attention (HBLï¼Œä¸»è¦è²¢ç»)**
- åœ¨ Hierarchical BERT åŸºç¤ä¸ŠåŠ å…¥ Layer-wise Attention
- åŸºæ–¼ UDify (Kondratyuk & Straka, EMNLP 2019) çš„å‹•æ…‹æ¬Šé‡æ©Ÿåˆ¶
- **å¯å­¸ç¿’çš„å±¤ç´šæ¬Šé‡**ï¼šæ¨¡å‹è‡ªå‹•å­¸ç¿’æœ€ä½³çš„ Low/Mid/High å±¤çµ„åˆ
- é…ç½®æ–‡ä»¶ï¼š
  - `configs/hierarchical_bert_layerattn.yaml` (SemEval, DistilBERT)
  - `configs/hierarchical_bert_layerattn_mams.yaml` (MAMS, DistilBERT)
  - `configs/hierarchical_bert_layerattn_mams_bertbase.yaml` (MAMS, BERT-base)
  - `configs/hierarchical_bert_layerattn_mams_roberta.yaml` (MAMS, RoBERTa)

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æ–¹æ³• 1: å–®ä¸€é…ç½®åŸ·è¡Œ

**æ‰€æœ‰å‘½ä»¤éƒ½æ˜¯å–®è¡Œæ ¼å¼ï¼Œå¯ç›´æ¥è¤‡è£½åŸ·è¡Œ**

#### MAMS æ•¸æ“šé›†ï¼ˆä¸»è¦å¯¦é©—ï¼‰

```bash
# Baseline: BERT-CLS
python experiments/train_from_config.py --config configs/baseline_bert_cls_mams.yaml --dataset mams

# æ–¹æ³• 1: Hierarchical BERT
python experiments/train_from_config.py --config configs/hierarchical_bert_mams.yaml --dataset mams

# æ–¹æ³• 2: HBL (DistilBERT)
python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn_mams.yaml --dataset mams

# æ–¹æ³• 2: HBL (BERT-base)
python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn_mams_bertbase.yaml --dataset mams

# æ–¹æ³• 2: HBL (RoBERTa) - æœ€ä½³æ€§èƒ½
python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn_mams_roberta.yaml --dataset mams
```

#### SemEval æ•¸æ“šé›†ï¼ˆè£œå……é©—è­‰ï¼‰

```bash
# Baseline: BERT-CLS
python experiments/train_from_config.py --config configs/baseline_bert_cls.yaml --dataset restaurants
python experiments/train_from_config.py --config configs/baseline_bert_cls.yaml --dataset laptops

# æ–¹æ³• 1: Hierarchical BERT
python experiments/train_from_config.py --config configs/hierarchical_bert.yaml --dataset restaurants
python experiments/train_from_config.py --config configs/hierarchical_bert.yaml --dataset laptops

# æ–¹æ³• 2: HBL (DistilBERT)
python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn.yaml --dataset restaurants
python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn.yaml --dataset laptops
```

---

## ğŸ“‹ å¯¦é©—çµæœä½ç½®

è¨“ç·´å®Œæˆå¾Œï¼Œçµæœæœƒä¿å­˜åœ¨ä»¥ä¸‹ä½ç½®ï¼š

```
results/
â”œâ”€â”€ baseline/                    # Baseline æ¨¡å‹çµæœ
â”‚   â””â”€â”€ YYYYMMDD_HHMMSS_baseline_*/
â”‚       â”œâ”€â”€ checkpoints/         # æ¨¡å‹æª¢æŸ¥é»
â”‚       â”œâ”€â”€ visualizations/      # è¨“ç·´æ›²ç·š
â”‚       â””â”€â”€ reports/             # å¯¦é©—å ±å‘Š
â”‚
â””â”€â”€ improved/                    # Improved æ¨¡å‹çµæœ
    â””â”€â”€ YYYYMMDD_HHMMSS_improved_*/
        â”œâ”€â”€ checkpoints/
        â”œâ”€â”€ visualizations/
        â””â”€â”€ reports/
```

**æ¯å€‹å¯¦é©—ç›®éŒ„åŒ…å«**ï¼š
- `checkpoints/` - æœ€ä½³æ¨¡å‹æ¬Šé‡ (`best_model.pth`)
- `visualizations/` - è¨“ç·´æ›²ç·šåœ–å’Œæ€§èƒ½åˆ†æåœ–
- `reports/` - JSON æ ¼å¼çš„å¯¦é©—é…ç½®å’Œçµæœ

---

## âš™ï¸ çµ±ä¸€é…ç½®åƒæ•¸

æ‰€æœ‰é…ç½®ä½¿ç”¨ç›¸åŒçš„è¨“ç·´åƒæ•¸ï¼ˆç¢ºä¿å…¬å¹³æ¯”è¼ƒï¼‰ï¼š

### MAMS æ•¸æ“šé›†åƒæ•¸

| åƒæ•¸ | å€¼ | èªªæ˜ |
|------|-----|------|
| `epochs` | 30 | è¨“ç·´è¼ªæ•¸ |
| `batch_size` | 32 | æ‰¹æ¬¡å¤§å° |
| `learning_rate` | 2e-5 | å­¸ç¿’ç‡ |
| `dropout` | 0.45 | Dropout æ¯”ä¾‹ |
| `weight_decay` | 0.05 | L2 æ­£å‰‡åŒ– |
| `patience` | 12 | Early stopping è€å¿ƒå€¼ |
| `loss_type` | focal | Focal Loss (gamma=2.0) |
| `label_smoothing` | 0.05 | æ¨™ç±¤å¹³æ»‘ |
| `class_weights` | [1.0, 5.0, 1.0] | é¡åˆ¥æ¬Šé‡ (Neg/Neu/Pos) |
| `virtual_weight` | 0.5 | Virtual aspect æ¬Šé‡ |

### SemEval æ•¸æ“šé›†åƒæ•¸

| åƒæ•¸ | å€¼ | èªªæ˜ |
|------|-----|------|
| `epochs` | 30 | è¨“ç·´è¼ªæ•¸ |
| `batch_size` | 32 | æ‰¹æ¬¡å¤§å° |
| `learning_rate` | 2e-5 | å­¸ç¿’ç‡ |
| `dropout` | 0.3 | Dropout æ¯”ä¾‹ |
| `weight_decay` | 0.05 | L2 æ­£å‰‡åŒ– |
| `patience` | 12 | Early stopping è€å¿ƒå€¼ |
| `loss_type` | focal | Focal Loss (gamma=2.5) |
| `label_smoothing` | 0.05 | æ¨™ç±¤å¹³æ»‘ |
| `class_weights` | [1.0, 8.0, 1.0] | é¡åˆ¥æ¬Šé‡ (Neg/Neu/Pos) |
| `virtual_weight` | 0.5 | Virtual aspect æ¬Šé‡ |

**é—œéµå·®ç•°**ï¼š
- MAMS ä½¿ç”¨è¼ƒé«˜çš„ dropout (0.45) å’Œè¼ƒä½çš„ class_weights[1] (5.0)
- SemEval ä½¿ç”¨è¼ƒä½çš„ dropout (0.3) å’Œè¼ƒé«˜çš„ class_weights[1] (8.0)
- MAMS ä½¿ç”¨ focal_gamma=2.0ï¼ŒSemEval ä½¿ç”¨ focal_gamma=2.5

---

## ğŸ“Š å¯¦é©—çµæœè¨˜éŒ„

### è‡ªå‹•ç”Ÿæˆå°æ¯”è¡¨æ ¼

è¨“ç·´å®Œæˆå¾Œï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆå¯¦é©—çµæœå°æ¯”è¡¨æ ¼ï¼š

```bash
# è¨˜éŒ„æ‰€æœ‰å¯¦é©—ï¼ˆBaseline + Improvedï¼‰
python experiments/record_experiment_table.py

# åªè¨˜éŒ„ Baseline å¯¦é©—
python experiments/record_experiment_table.py --only-baseline

# åªè¨˜éŒ„ Improved å¯¦é©—
python experiments/record_experiment_table.py --only-improved

# æŒ‡å®šè¼¸å‡ºæª”æ¡ˆï¼ˆé è¨­ç‚º results/å¯¦é©—çµæœå ±å‘Š.mdï¼‰
python experiments/record_experiment_table.py --output results/MY_RESULTS.md
```

**ç”Ÿæˆçš„è¡¨æ ¼åŒ…å«**ï¼š
- æ¨¡å‹åç¨±å’Œé…ç½®
- Macro-F1 / Weighted-F1 / Accuracy
- å„é¡åˆ¥çš„ Precision / Recall / F1
- è¨“ç·´æ™‚é–“å’Œ epoch æ•¸
- è‡ªå‹•é«˜äº®æœ€ä½³çµæœ

---

## ğŸ”¬ Layer-wise Attention åˆ†æ

HBL æ¨¡å‹è¨“ç·´å®Œæˆå¾Œï¼Œå¯ä»¥åˆ†æå­¸åˆ°çš„å±¤ç´šæ¬Šé‡ï¼š

```bash
# æŸ¥çœ‹æ¨¡å‹è¼¸å‡ºçš„ layer_attention æ¬Šé‡
# æ¬Šé‡æœƒåœ¨è¨“ç·´éç¨‹ä¸­è‡ªå‹•è¨˜éŒ„åˆ° reports/experiment_results.json
```

**é æœŸåˆ†æ**ï¼š
- **Low å±¤æ¬Šé‡**ï¼šæ•æ‰è©å½™èªç¾©
- **Mid å±¤æ¬Šé‡**ï¼šæ•æ‰å¥æ³•çµæ§‹
- **High å±¤æ¬Šé‡**ï¼šæ•æ‰ä»»å‹™ç‰¹å®šç‰¹å¾µ

å¯ä»¥å°æ¯”ä¸åŒæ•¸æ“šé›†ï¼ˆSemEval vs MAMSï¼‰å’Œä¸åŒ BERT è®Šé«”ï¼ˆDistilBERT vs BERT-base vs RoBERTaï¼‰çš„æ¬Šé‡åˆ†ä½ˆï¼Œåˆ†ææ¨¡å‹çš„å­¸ç¿’åå¥½ã€‚

---

## ğŸ’¡ å¯¦é©—å»ºè­°

### æ¨è–¦å¯¦é©—é †åº

1. **MAMS åŸºç¤å¯¦é©—**ï¼ˆå»ºç«‹ä¸»è¦çµæœï¼Œå±•ç¾æ€§èƒ½æ¢¯åº¦ï¼‰
   ```bash
   python experiments/train_from_config.py --config configs/baseline_bert_cls_mams.yaml --dataset mams
   python experiments/train_from_config.py --config configs/hierarchical_bert_mams.yaml --dataset mams
   python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn_mams.yaml --dataset mams
   ```

2. **MAMS ä¸åŒ BERT è®Šé«”**ï¼ˆé©—è­‰æ–¹æ³•ç©©å®šæ€§ï¼‰
   ```bash
   python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn_mams_bertbase.yaml --dataset mams
   python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn_mams_roberta.yaml --dataset mams
   ```

3. **SemEval è£œå……å¯¦é©—**ï¼ˆé©—è­‰æ³›åŒ–æ€§ï¼‰
   ```bash
   python experiments/train_from_config.py --config configs/baseline_bert_cls.yaml --dataset restaurants
   python experiments/train_from_config.py --config configs/hierarchical_bert.yaml --dataset restaurants
   python experiments/train_from_config.py --config configs/hierarchical_bert_layerattn.yaml --dataset restaurants
   ```

4. **ç”Ÿæˆå®Œæ•´å ±å‘Š**
   ```bash
   python experiments/record_experiment_table.py
   ```

### é æœŸçµæœ

**MAMS æ•¸æ“šé›†**ï¼ˆ100% å¤šé¢å‘ï¼Œå±•ç¾éšå±¤ç‰¹å¾µå’Œå‹•æ…‹æ¬Šé‡çš„å„ªå‹¢ï¼‰ï¼š
- Baseline (BERT-CLS): Macro-F1 ~65-67%
- æ–¹æ³• 1 (Hierarchical BERT): Macro-F1 ~68-70% âœ… **+2-3% F1**
- æ–¹æ³• 2 (HBL): Macro-F1 ~70-72% âœ… **å†+1-2% F1**

**SemEval æ•¸æ“šé›†**ï¼ˆ20% å¤šé¢å‘ï¼Œæå‡å¹…åº¦è¼ƒå°ï¼‰ï¼š
- Baseline (BERT-CLS): Macro-F1 ~63-65%
- æ–¹æ³• 1 (Hierarchical BERT): Macro-F1 ~65-67% âœ… **+2% F1**
- æ–¹æ³• 2 (HBL): Macro-F1 ~66-68% âœ… **å†+1% F1**

---

## ğŸ› ï¸ å¸¸è¦‹å•é¡Œ

### Q1: è¨“ç·´ä¸­æ–·æ€éº¼è¾¦ï¼Ÿ

é…ç½®æ–‡ä»¶å·²å•Ÿç”¨ checkpoint ä¿å­˜ï¼Œå¯ä»¥å¾æœ€ä½³æ¨¡å‹æ¢å¾©ï¼š

```bash
# æª¢æŸ¥é»æœƒè‡ªå‹•ä¿å­˜åœ¨ results/*/checkpoints/best_model.pth
# å¦‚éœ€ç¹¼çºŒè¨“ç·´ï¼Œå¯ä»¥ä¿®æ”¹è¨“ç·´è…³æœ¬è¼‰å…¥ checkpoint
```

### Q2: å¦‚ä½•èª¿æ•´è¶…åƒæ•¸ï¼Ÿ

ä¿®æ”¹å°æ‡‰çš„ YAML é…ç½®æ–‡ä»¶ï¼Œä¾‹å¦‚ï¼š

```yaml
training:
  epochs: 30          # å¢åŠ è¨“ç·´è¼ªæ•¸
  batch_size: 32      # èª¿æ•´æ‰¹æ¬¡å¤§å°
  learning_rate: 2e-5 # èª¿æ•´å­¸ç¿’ç‡
  dropout: 0.45       # èª¿æ•´ Dropout

  # æå¤±å‡½æ•¸
  loss_type: "focal"
  focal_gamma: 2.0    # èª¿æ•´ Focal Loss gamma
  class_weights: [1.0, 5.0, 1.0]  # èª¿æ•´é¡åˆ¥æ¬Šé‡
```

### Q3: è¨˜æ†¶é«”ä¸è¶³æ€éº¼è¾¦ï¼Ÿ

1. é™ä½ batch_sizeï¼ˆä¾‹å¦‚å¾ 32 é™åˆ° 16ï¼‰
2. ä½¿ç”¨ DistilBERT è€Œé BERT-base æˆ– RoBERTa
3. å•Ÿç”¨æ¢¯åº¦ç´¯ç©ï¼ˆéœ€ä¿®æ”¹è¨“ç·´è…³æœ¬ï¼‰

### Q4: Laptops æ•¸æ“šé›†æ€§èƒ½è¼ƒå·®ï¼Ÿ

Laptops æ•¸æ“šé›†æ¨£æœ¬è¼ƒå°‘ï¼ˆ~900ï¼‰ä¸”é¡åˆ¥ä¸å¹³è¡¡åš´é‡ï¼Œå¯ä»¥ï¼š
1. èª¿æ•´ `class_weights` åƒæ•¸
2. å¢åŠ  `dropout` é˜²æ­¢éæ“¬åˆ
3. æ¸›å°‘è¨“ç·´è¼ªæ•¸æˆ–é™ä½ `patience`

---

## ğŸ“š é…ç½®æ–‡ä»¶èªªæ˜

### Baseline é…ç½®

| æ–‡ä»¶ | æ•¸æ“šé›† | BERT æ¨¡å‹ | èªªæ˜ |
|------|--------|-----------|------|
| `baseline_bert_cls.yaml` | SemEval | DistilBERT | æ¨™æº– BERT-CLS baseline |
| `baseline_bert_cls_mams.yaml` | MAMS | DistilBERT | MAMS å°ˆç”¨é…ç½® |

### æ–¹æ³• 1 é…ç½® (Hierarchical BERT)

| æ–‡ä»¶ | æ•¸æ“šé›† | BERT æ¨¡å‹ | èªªæ˜ |
|------|--------|-----------|------|
| `hierarchical_bert.yaml` | SemEval | DistilBERT | éšå±¤å¼BERT (å›ºå®šæ‹¼æ¥) |
| `hierarchical_bert_mams.yaml` | MAMS | DistilBERT | MAMS å°ˆç”¨é…ç½® |

### æ–¹æ³• 2 é…ç½® (HBL)

| æ–‡ä»¶ | æ•¸æ“šé›† | BERT æ¨¡å‹ | èªªæ˜ |
|------|--------|-----------|------|
| `hierarchical_bert_layerattn.yaml` | SemEval | DistilBERT | HBL åŸºç¤æ¨¡å‹ |
| `hierarchical_bert_layerattn_mams.yaml` | MAMS | DistilBERT | MAMS å°ˆç”¨é…ç½® |
| `hierarchical_bert_layerattn_mams_bertbase.yaml` | MAMS | BERT-base | é©—è­‰æ›´å¤§æ¨¡å‹ |
| `hierarchical_bert_layerattn_mams_roberta.yaml` | MAMS | RoBERTa | æœ€ä½³æ€§èƒ½é…ç½® â­ |

---

## ğŸ“ è«–æ–‡æ’°å¯«å»ºè­°

### å¯¦é©—ç« ç¯€çµæ§‹

1. **å¯¦é©—è¨­ç½®**
   - æ•¸æ“šé›†æè¿°ï¼ˆSemEval vs MAMSï¼‰
   - æ¨¡å‹é…ç½®ï¼ˆBaseline, Method 1, Method 2ï¼‰
   - è¶…åƒæ•¸è¨­ç½®ï¼ˆåƒè€ƒä¸Šæ–¹è¡¨æ ¼ï¼‰

2. **ä¸»è¦çµæœ**ï¼ˆMAMS æ•¸æ“šé›†ï¼‰
   - **Baseline vs Method 1**: è­‰æ˜éšå±¤ç‰¹å¾µæå–çš„æœ‰æ•ˆæ€§
   - **Method 1 vs Method 2**: è­‰æ˜ Layer-wise Attention çš„æ”¹é€²æ•ˆæœ
   - ä¸åŒ BERT è®Šé«”çš„å½±éŸ¿ï¼ˆDistilBERT vs BERT-base vs RoBERTaï¼‰

3. **è£œå……å¯¦é©—**ï¼ˆSemEval æ•¸æ“šé›†ï¼‰
   - é©—è­‰æ–¹æ³•åœ¨ä¸åŒæ•¸æ“šé›†ä¸Šçš„æ³›åŒ–æ€§

4. **åˆ†æèˆ‡è¨è«–**
   - **éšå±¤ç‰¹å¾µåˆ†æ**: Low/Mid/High å±¤çš„ä½œç”¨
   - **Layer-wise Attention æ¬Šé‡åˆ†æ**: æ¨¡å‹å­¸åˆ°çš„å±¤ç´šåå¥½
   - å¤šé¢å‘å ´æ™¯ä¸‹çš„æ€§èƒ½æå‡
   - éŒ¯èª¤æ¡ˆä¾‹åˆ†æ

### é—œéµè³£é»

âœ… **æ¸…æ™°çš„æ•…äº‹ç·šï¼ˆä¸‰å€‹å±¤æ¬¡ï¼‰**ï¼š
1. **Baseline (BERT-CLS)**: æ¨™æº–æ–¹æ³•ï¼Œæœ‰æ˜ç¢ºå¼•ç”¨
2. **Method 1 (Hierarchical BERT)**: æˆ‘å€‘çš„è²¢ç» - éšå±¤ç‰¹å¾µæå–
3. **Method 2 (HBL)**: æˆ‘å€‘çš„ä¸»è¦è²¢ç» - å‹•æ…‹æ¬Šé‡å­¸ç¿’

âœ… **é€æ­¥æå‡çš„æ€§èƒ½**ï¼š
- BERT-CLS â†’ Hierarchical BERT: +2-3% F1 (è­‰æ˜éšå±¤ç‰¹å¾µæœ‰æ•ˆ)
- Hierarchical BERT â†’ HBL: +1-2% F1 (è­‰æ˜å‹•æ…‹æ¬Šé‡å„ªæ–¼å›ºå®šæ‹¼æ¥)

âœ… **å¯¦é©—è¨­è¨ˆå®Œæ•´**ï¼š
- 3 å€‹æ•¸æ“šé›†ï¼ˆSemEval Restaurants/Laptops + MAMSï¼‰
- 3 å€‹ BERT è®Šé«”ï¼ˆDistilBERT + BERT-base + RoBERTaï¼‰
- åš´æ ¼çš„å…¬å¹³æ¯”è¼ƒï¼ˆçµ±ä¸€è¶…åƒæ•¸ï¼‰

âœ… **å‰µæ–°é»æœ‰æ˜ç¢ºæ”¯æ’**ï¼š
- éšå±¤ç‰¹å¾µæå–: åˆ©ç”¨ BERT ä¸åŒå±¤çš„ç‰¹æ€§
- Layer-wise Attention: åŸºæ–¼ UDify (EMNLP 2019) çš„æˆç†Ÿæ©Ÿåˆ¶

---

**æœ€å¾Œæ›´æ–°**: 2025-11-21
