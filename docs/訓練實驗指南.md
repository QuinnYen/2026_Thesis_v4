# HMAC-Net è¨“ç·´å¯¦é©—æŒ‡å—

æœ¬æŒ‡å—æ•´åˆäº†æ‰€æœ‰è¨“ç·´ç›¸é—œçš„èªªæ˜,åŒ…æ‹¬å¿«é€Ÿé–‹å§‹ã€Baselineå¯¦é©—ã€Multi-Aspectå¯¦ä½œç­‰å…§å®¹ã€‚

---

## ğŸ“‹ ç›®éŒ„

1. [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
2. [Baseline å¯¦é©—æŒ‡å—](#baseline-å¯¦é©—æŒ‡å—)
3. [å¥å­ç´šåˆ¥è¨“ç·´](#å¥å­ç´šåˆ¥è¨“ç·´)
4. [Multi-Aspect å¯¦ä½œè¨ˆåŠƒ](#multi-aspect-å¯¦ä½œè¨ˆåŠƒ)

---

## å¿«é€Ÿé–‹å§‹

### ğŸ¯ æ¦‚è¿°

æœ¬ç¯€å°‡å”åŠ©æ‚¨å¿«é€Ÿé–‹å§‹ä½¿ç”¨ **HMAC-Net with BERT** é€²è¡Œé¢å‘ç´šæƒ…æ„Ÿåˆ†æå¯¦é©—ã€‚

### ğŸ“‹ å‰ç½®æº–å‚™

#### 1. å®‰è£ä¾è³´

```bash
# å®‰è£æ‰€æœ‰å¿…è¦å¥—ä»¶
pip install -r requirements.txt

# å¦‚æœä½¿ç”¨ GPU
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### 2. é©—è­‰æ•¸æ“š

ç¢ºèªæ‚¨çš„æ•¸æ“šå·²æ­£ç¢ºæ”¾ç½®ï¼š

```bash
# æª¢æŸ¥æ•¸æ“šæª”æ¡ˆ
ls data/raw/semeval2014/

# æ‡‰è©²çœ‹åˆ°ï¼š
# Restaurants_Train_v2.xml
# Restaurants_Test_Data_phaseB.xml
# Laptop_Train_v2.xml
# Laptops_Test_Data_phaseB.xml
```

### ğŸš€ é–‹å§‹è¨“ç·´

#### æ–¹æ¡ˆ Aï¼šä½¿ç”¨ BERTï¼ˆæ¨è–¦ï¼Œæ•ˆæœæ›´å¥½ï¼‰

```bash
cd experiments

# è¨“ç·´é¤å»³é ˜åŸŸï¼ˆRestaurantï¼‰
python train_bert.py --domain restaurant --epochs 20 --batch_size 16

# è¨“ç·´ç­†é›»é ˜åŸŸï¼ˆLaptopï¼‰
python train_bert.py --domain laptop --epochs 20 --batch_size 16

# å¦‚æœè¨˜æ†¶é«”ä¸è¶³ï¼Œå¯ä»¥å‡çµ BERT
python train_bert.py --domain restaurant --freeze_bert --batch_size 32
```

#### æ–¹æ¡ˆ Bï¼šä½¿ç”¨ Multi-Aspect

```bash
# å®Œæ•´æ¨¡å‹
python train_multiaspect.py \
  --epochs 30 \
  --batch_size 16 \
  --lr 2e-5 \
  --dropout 0.3 \
  --use_pmac \
  --use_iarm \
  --loss_type focal \
  --focal_gamma 2.0 \
  --class_weights 1.0 3.0 1.0
```

### ğŸ“Š è¨“ç·´åƒæ•¸èªªæ˜

#### é‡è¦åƒæ•¸

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ | å»ºè­°å€¼ |
|------|------|--------|--------|
| `--domain` | æ•¸æ“šé›†é ˜åŸŸ | restaurant | restaurant æˆ– laptop |
| `--bert_model` | BERT æ¨¡å‹ | bert-base-uncased | bert-base-uncased |
| `--freeze_bert` | å‡çµ BERT | False | GPU è¨˜æ†¶é«”å°æ™‚ä½¿ç”¨ |
| `--batch_size` | æ‰¹æ¬¡å¤§å° | 16 | 16-32 |
| `--epochs` | è¨“ç·´è¼ªæ•¸ | 20 | 20-30 |
| `--lr` | å­¸ç¿’ç‡ | 2e-5 | 1e-5 åˆ° 3e-5 |

### ğŸ” å¸¸è¦‹å•é¡Œ

#### Q1: CUDA Out of Memory éŒ¯èª¤

**è§£æ±ºæ–¹æ³•ï¼š**

```bash
# æ–¹æ¡ˆ 1ï¼šæ¸›å°‘æ‰¹æ¬¡å¤§å°
python train_bert.py --batch_size 8

# æ–¹æ¡ˆ 2ï¼šå‡çµ BERT
python train_bert.py --freeze_bert --batch_size 32

# æ–¹æ¡ˆ 3ï¼šä½¿ç”¨ CPUï¼ˆè¼ƒæ…¢ï¼‰
CUDA_VISIBLE_DEVICES="" python train_bert.py
```

#### Q2: è¨“ç·´é€Ÿåº¦å¤ªæ…¢

**å»ºè­°ï¼š**

1. ä½¿ç”¨ GPUï¼ˆé€Ÿåº¦æå‡ 10-20 å€ï¼‰
2. å‡çµ BERT åƒæ•¸ï¼ˆ`--freeze_bert`ï¼‰
3. æ¸›å°‘è¨“ç·´è¼ªæ•¸ï¼ˆ`--epochs 10`ï¼‰
4. å¢åŠ æ‰¹æ¬¡å¤§å°ï¼ˆ`--batch_size 32`ï¼‰

### ğŸ“Š æœŸæœ›çµæœ

#### SemEval-2014 Restaurant

- **Accuracy**: 0.82-0.85
- **Macro F1**: 0.75-0.78

#### SemEval-2014 Laptop

- **Accuracy**: 0.76-0.79
- **Macro F1**: 0.71-0.74

---

## Baseline å¯¦é©—æŒ‡å—

### æ¦‚è¿°

æœ¬ç¯€èªªæ˜å¦‚ä½•ä½¿ç”¨æ–°å¢çš„ baseline æ”¯æ´é€²è¡Œå°æ¯”å¯¦é©—ã€‚

### å¯ç”¨çš„ Baseline æ¨¡å‹

| Baseline | èªªæ˜ | åƒæ•¸é‡ | ç”¨é€” |
|---------|------|--------|------|
| `bert_only` | æœ€åŸºç¤çš„ BERT baseline | 66.4M | æœ€åŸºç¤å°ç…§çµ„ |
| `bert_aaha` | BERT + AAHA (ç„¡ PMAC/IARM) | 93.5M | è­‰æ˜ PMAC/IARM çš„è²¢ç» |
| `bert_mean` | BERT + Mean Pooling | 66.4M | ç°¡å–®æ–¹æ³•å°ç…§ |

### å¿«é€Ÿé–‹å§‹

> **ç¡¬é«”é…ç½®**: RTX 3090 (24GB VRAM)
> **å„ªåŒ–**: ä½¿ç”¨æ›´å¤§çš„ batch size ä»¥å……åˆ†åˆ©ç”¨ GPU è¨˜æ†¶é«”

#### 1. è¨“ç·´ Baseline æ¨¡å‹

**Baseline 1: BERT Only (æœ€ç°¡å–®ï¼Œå¯ç”¨æœ€å¤§ batch)**
```bash
python experiments/train_multiaspect.py --baseline bert_only --epochs 30 --batch_size 32 --accumulation_steps 1 --lr 2e-5 --dropout 0.3 --loss_type focal --focal_gamma 2.0 --class_weights 1.0 3.0 1.0
```

**Baseline 2: BERT + AAHA (ä¸­ç­‰è¤‡é›œåº¦)**
```bash
python experiments/train_multiaspect.py --baseline bert_aaha --epochs 30 --batch_size 24 --accumulation_steps 1 --lr 2e-5 --dropout 0.3 --loss_type focal --focal_gamma 2.0 --class_weights 1.0 3.0 1.0
```

**Baseline 3: BERT + Mean Pooling (æœ€ç°¡å–®ï¼Œå¯ç”¨æœ€å¤§ batch)**
```bash
python experiments/train_multiaspect.py --baseline bert_mean --epochs 30 --batch_size 32 --accumulation_steps 1 --lr 2e-5 --dropout 0.3 --loss_type focal --focal_gamma 2.0 --class_weights 1.0 3.0 1.0
```

#### 2. è¨“ç·´ Full Model (å®Œæ•´æ¨¡å‹)

**Full Model (BERT + AAHA + PMAC + IARM) - æœ€è¤‡é›œ**
```bash
python experiments/train_multiaspect.py --use_pmac --use_iarm --epochs 30 --batch_size 20 --accumulation_steps 1 --lr 2e-5 --dropout 0.3 --gate_bias_init -3.0 --loss_type focal --focal_gamma 2.0 --class_weights 1.0 3.0 1.0
```

**Ablation: BERT + AAHA + PMAC (ç„¡ IARM)**
```bash
python experiments/train_multiaspect.py --use_pmac --epochs 30 --batch_size 24 --accumulation_steps 1 --lr 2e-5 --dropout 0.3 --gate_bias_init -3.0 --loss_type focal --focal_gamma 2.0 --class_weights 1.0 3.0 1.0
```

### å®Œæ•´çš„æ¶ˆèå¯¦é©—æ–¹æ¡ˆ

#### å¯¦é©—è¨­è¨ˆ

| å¯¦é©— ID | æ¨¡å‹é…ç½® | å‘½ä»¤ | é æœŸç”¨é€” |
|---------|---------|------|---------|
| **EXP-1** | BERT Only | `--baseline bert_only` | æœ€åŸºç¤ baseline |
| **EXP-2** | BERT + AAHA | `--baseline bert_aaha` | è­‰æ˜ AAHA çš„è²¢ç» |
| **EXP-3** | BERT + AAHA + PMAC | `--use_pmac` | è­‰æ˜ PMAC çš„è²¢ç» |
| **EXP-4** | Full (+ IARM) | `--use_pmac --use_iarm` | å®Œæ•´æ¨¡å‹ |

#### æ‰¹æ¬¡é‹è¡Œè…³æœ¬

å‰µå»º `run_all_baselines.sh`:

```bash
#!/bin/bash

# è¨­å®šå…±åŒåƒæ•¸
COMMON_ARGS="--epochs 30 --batch_size 16 --accumulation_steps 2 --lr 2e-5 --dropout 0.3 --loss_type focal --focal_gamma 2.0 --class_weights 1.0 3.0 1.0"

echo "é–‹å§‹é‹è¡Œæ‰€æœ‰ baseline å¯¦é©—..."

# EXP-1: BERT Only
echo "Running EXP-1: BERT Only..."
python experiments/train_multiaspect.py \
    --baseline bert_only \
    $COMMON_ARGS

# EXP-2: BERT + AAHA
echo "Running EXP-2: BERT + AAHA..."
python experiments/train_multiaspect.py \
    --baseline bert_aaha \
    $COMMON_ARGS

# EXP-3: BERT + AAHA + PMAC
echo "Running EXP-3: BERT + AAHA + PMAC..."
python experiments/train_multiaspect.py \
    --use_pmac \
    --gate_bias_init -3.0 \
    $COMMON_ARGS

# EXP-4: Full Model
echo "Running EXP-4: Full Model (PMAC + IARM)..."
python experiments/train_multiaspect.py \
    --use_pmac \
    --use_iarm \
    --gate_bias_init -3.0 \
    $COMMON_ARGS

echo "æ‰€æœ‰å¯¦é©—å®Œæˆï¼"
```

### å¯¦é©—çµæœæ¯”è¼ƒ

#### é æœŸçµæœæ ¼å¼

å¯¦é©—å®Œæˆå¾Œï¼Œçµæœæœƒä¿å­˜åœ¨ï¼š
```
results/experiments/
â”œâ”€â”€ YYYYMMDD_HHMMSS_baseline_bert_only_...
â”œâ”€â”€ YYYYMMDD_HHMMSS_baseline_bert_aaha_...
â”œâ”€â”€ YYYYMMDD_HHMMSS_pmac_noiarm_...
â””â”€â”€ YYYYMMDD_HHMMSS_pmac_iarm_...
```

#### çµæœå½™ç¸½è¡¨æ ¼

æ‰‹å‹•å½™ç¸½æˆ–ä½¿ç”¨è…³æœ¬ç”Ÿæˆè¡¨æ ¼ï¼š

| Model | Val F1 | Test F1 | Test Acc | Neg F1 | Neu F1 | Pos F1 | Improvement |
|-------|--------|---------|----------|--------|--------|--------|-------------|
| BERT Only | 0.55 | 0.58 | 0.70 | 0.55 | 0.35 | 0.85 | Baseline |
| + AAHA | 0.63 | 0.65 | 0.76 | 0.64 | 0.42 | 0.88 | +0.07 |
| + PMAC | 0.65 | 0.68 | 0.77 | 0.67 | 0.46 | 0.89 | +0.03 |
| + IARM (Full) | 0.67 | 0.70 | 0.78 | 0.69 | 0.50 | 0.90 | +0.02 |

### è«–æ–‡æ’°å¯«å»ºè­°

#### 1. å¯¦é©—è¨­ç½® (Experimental Setup)

```
We compare our proposed HMAC-Net against the following baselines:

1. BERT-Only: A vanilla BERT classifier without any aspect modeling
2. BERT-AAHA: BERT with hierarchical attention (AAHA) but without
   aspect composition or relation modeling

We conduct ablation studies by progressively adding our proposed modules:
- +PMAC: Adding Selective PMAC for aspect composition
- +IARM: Adding Transformer-based inter-aspect relation modeling
```

#### 2. çµæœè¡¨æ ¼ (Results Table)

```
Table 1: Performance comparison on SemEval-2014

| Method          | Acc   | F1    | Neg F1 | Neu F1 | Pos F1 |
|-----------------|-------|-------|--------|--------|--------|
| BERT-Only       | 70.0  | 58.0  | 55.0   | 35.0   | 85.0   |
| BERT-AAHA       | 76.0  | 65.0  | 64.0   | 42.0   | 88.0   |
| +PMAC (Ours)    | 77.0  | 68.0  | 67.0   | 46.0   | 89.0   |
| +IARM (Full)    | 78.0  | 70.0  | 69.0   | 50.0   | 90.0   |
```

#### 3. æ¶ˆèç ”ç©¶ (Ablation Study)

```
Table 2: Ablation study on the contribution of each module

| Configuration    | Test F1 | Î” F1  |
|------------------|---------|-------|
| BERT-AAHA        | 65.0    | -     |
| + PMAC           | 68.0    | +3.0  |
| + IARM           | 70.0    | +2.0  |

Our results show that each proposed module contributes positively
to the overall performance, with PMAC providing the largest gain (+3.0).
```

---

## å¥å­ç´šåˆ¥è¨“ç·´

### ğŸ¯ æ ¸å¿ƒå‰µæ–°

#### ç‚ºä»€éº¼é€™å€‹æ–¹æ³•å‰µæ–°ï¼Ÿ

å‚³çµ±çš„å¥å­ç´šåˆ¥æƒ…æ„Ÿåˆ†æï¼š
```
Text â†’ BERT â†’ [CLS] token â†’ Classifier â†’ Label
```

**æˆ‘å€‘çš„æ–¹æ³•ï¼ˆä¿ç•™ PMAC å’Œ IARM å‰µæ–°ï¼‰ï¼š**
```
Text â†’ BERT â†’ Implicit Aspect Discovery â†’ PMAC â†’ IARM â†’ Classifier â†’ Label
                â†“
         è‡ªå‹•ç™¼ç¾ 5 å€‹éš±å«çš„èªç¾©é¢å‘
         (ä¾‹å¦‚ï¼šæƒ…ç¯€ã€æ¼”æŠ€ã€æŠ€è¡“ã€æƒ…æ„Ÿç­‰)
```

**å„ªå‹¢ï¼š**
- âœ… ä¿ç•™åŸæœ‰çš„ PMAC å’Œ IARM å‰µæ–°æ¨¡çµ„
- âœ… å¯è§£é‡‹æ€§ï¼šå¯è¦–åŒ–æ¨¡å‹ç™¼ç¾äº†å“ªäº›èªç¾©é¢å‘
- âœ… é€šç”¨æ€§ï¼šé©ç”¨æ–¼ä»»ä½•æ–‡æœ¬åˆ†é¡ä»»å‹™

### ğŸ“š æ”¯æ´çš„è³‡æ–™é›†

#### Aspect-Based è³‡æ–™é›†ï¼ˆæœ‰æ˜ç¢º aspect æ¨™è¨»ï¼‰

| ä»£è™Ÿ | åç¨± | é ˜åŸŸ | é¡åˆ¥æ•¸ |
|------|------|------|--------|
| `semeval_rest` | SemEval-2014 Restaurant | é¤å»³è©•è«– | 3 |
| `semeval_laptop` | SemEval-2014 Laptop | ç”¢å“è©•è«– | 3 |

**è¨“ç·´å‘½ä»¤ï¼š**
```bash
python experiments/train_multiaspect.py --use_pmac --pmac_mode selective --use_iarm
```

#### Sentence-Level è³‡æ–™é›†ï¼ˆç„¡ aspect æ¨™è¨»ï¼‰

| ä»£è™Ÿ | åç¨± | é ˜åŸŸ | é¡åˆ¥æ•¸ |
|------|------|------|--------|
| `imdb` | IMDB Movie Reviews | é›»å½±è©•è«– | 2 |
| `sst2` | Stanford Sentiment Treebank | é›»å½±è©•è«– | 2 |
| `yelp` | Yelp Reviews | é¤å»³è©•è«– | 2 |
| `amazon` | Amazon Product Reviews | ç”¢å“è©•è«– | 5 |

**è¨“ç·´å‘½ä»¤ï¼š**
```bash
# ä½¿ç”¨ IMDB è³‡æ–™é›†
python experiments/train_sentence_level.py --dataset imdb --use_pmac --use_iarm

# ä½¿ç”¨ SST-2 è³‡æ–™é›†
python experiments/train_sentence_level.py --dataset sst2 --use_pmac --pmac_mode selective --use_iarm

# å¿«é€Ÿæ¸¬è©¦ï¼ˆé™åˆ¶æ¨£æœ¬æ•¸ï¼‰
python experiments/train_sentence_level.py --dataset imdb --use_pmac --use_iarm --limit 1000
```

### âš™ï¸ è¨“ç·´åƒæ•¸èªªæ˜

#### åŸºæœ¬åƒæ•¸

```bash
--dataset <code>              # è³‡æ–™é›†ä»£è™Ÿï¼ˆå¿…å¡«ï¼‰
--batch_size 16               # Batch size
--epochs 20                   # è¨“ç·´è¼ªæ•¸
--lr 2e-5                     # å­¸ç¿’ç‡
--limit 1000                  # é™åˆ¶æ¨£æœ¬æ•¸ï¼ˆå¿«é€Ÿæ¸¬è©¦ç”¨ï¼‰
```

#### æ¨¡å‹åƒæ•¸

```bash
--num_implicit_aspects 5      # éš±å« aspects æ•¸é‡
--use_pmac                    # å•Ÿç”¨ PMAC
--pmac_mode selective         # PMAC æ¨¡å¼ï¼šselectiveï¼ˆæ¨è–¦ï¼‰ã€sequentialã€pairwiseã€attention
--use_iarm                    # å•Ÿç”¨ IARM
--iarm_mode transformer       # IARM æ¨¡å¼ï¼štransformerã€gatã€bilinear
--fusion_strategy weighted_pooling  # èåˆç­–ç•¥ï¼šweighted_poolingï¼ˆæ¨è–¦ï¼‰ã€meanã€maxã€attention
```

---

## Multi-Aspect å¯¦ä½œè¨ˆåŠƒ

### ã€æ ¸å¿ƒç†å¿µã€‘

å°‡ SemEval-2014 æ•¸æ“šå¾ aspect-level è½‰æ›ç‚º sentence-level multi-aspectï¼Œè®“ PMAC å’Œ IARM å‰µæ–°æ¨¡çµ„çœŸæ­£ç™¼æ®ã€Œçµ„åˆå¤šå€‹é¢å‘ã€çš„ä½œç”¨ã€‚

### ä¸€ã€æ•¸æ“šåˆ†æçµæœ

#### SemEval-2014 Restaurant è¨“ç·´é›†çµ±è¨ˆ

ç¸½å¥å­æ•¸: 3,041 å¥

**Aspect åˆ†ä½ˆ:**
- 0 aspects:  1,020 å¥ (33.5%) â† å°‡è¢«éæ¿¾
- 1 aspect:   1,023 å¥ (33.6%) â† å–® aspectï¼Œä½œç‚ºå°ç…§çµ„
- 2 aspects:    572 å¥ (18.8%) â† Multi-aspect âœ“
- 3 aspects:    269 å¥ (8.8%)  â† Multi-aspect âœ“
- 4 aspects:    104 å¥ (3.4%)  â† Multi-aspect âœ“
- 5+ aspects:    53 å¥ (1.7%)  â† Multi-aspect âœ“

**é—œéµç™¼ç¾:**
- âœ… 32.8% çš„å¥å­åŒ…å« 2+ aspects (998 å¥)
- âœ… 14.0% çš„å¥å­åŒ…å« 3+ aspects (426 å¥)
- âœ… æœ‰è¶³å¤ çš„ multi-aspect æ•¸æ“šè¨“ç·´æ¨¡å‹

#### å…¸å‹ Multi-Aspect ç¯„ä¾‹

**ç¯„ä¾‹ 1: 2 aspects (positive + positive)**
```
Text: "Not only was the food outstanding, but the little perks were great."
Aspects:
  - food â†’ positive
  - perks â†’ positive

â†’ PMAC å¯ä»¥çµ„åˆ "food" å’Œ "perks" çš„èªç¾©
â†’ IARM å¯ä»¥å»ºæ¨¡å…©è€…çš„é—œä¿‚ï¼ˆéƒ½æ˜¯æ­£é¢ï¼Œç›¸äº’å¢å¼·ï¼‰
```

**ç¯„ä¾‹ 2: 3 aspects (mixed sentiments)**
```
Text: "The food is exceptional, with a capable kitchen which will proudly
       showcase its menu."
Aspects:
  - food â†’ positive
  - kitchen â†’ positive
  - menu â†’ neutral

â†’ PMAC å¯ä»¥æ¼¸é€²å¼çµ„åˆ 3 å€‹ aspects
â†’ IARM å¯ä»¥æ•æ‰æƒ…æ„Ÿä¸€è‡´æ€§/å°æ¯”æ€§
```

### äºŒã€æ•¸æ“šè½‰æ›æ–¹æ¡ˆ

#### æ–¹æ¡ˆ 2.1: å®Œå…¨ Multi-Aspectï¼ˆæ¨è–¦ï¼‰

**ç­–ç•¥**: åªä½¿ç”¨åŒ…å« 2+ aspects çš„å¥å­

**æ•¸æ“šé‡:**
- è¨“ç·´é›†: 998 å¥ (32.8% of 3041)
- æ¯å¥å¹³å‡ aspects: 2.8 å€‹
- ç¸½ aspect-sentiment pairs: ~2,800

**å„ªé»:**
- âœ… æ‰€æœ‰æ¨£æœ¬éƒ½èƒ½å±•ç¾ PMAC/IARM å‰µæ–°
- âœ… æ•¸æ“šä¸€è‡´æ€§é«˜
- âœ… è«–æ–‡æ•˜è¿°æ¸…æ™°

**ç¼ºé»:**
- âš ï¸ æ¨£æœ¬æ•¸æ¸›å°‘ (3061 â†’ 998)
- âš ï¸ è¨“ç·´é›£åº¦å¢åŠ ï¼ˆå¤šæ¨™ç±¤å­¸ç¿’ï¼‰

### ä¸‰ã€æ•¸æ“šæ ¼å¼è¨­è¨ˆ

#### ç•¶å‰æ ¼å¼ï¼ˆAspect-levelï¼‰

```json
{
    "text": "The food was great but service was slow.",
    "aspect": "food",
    "label": 1  // positive
}
```

å•é¡Œ: åŒä¸€å¥è©±æ‹†æˆ 2 å€‹ç¨ç«‹æ¨£æœ¬ï¼Œä¸Ÿå¤±äº† aspects é–“çš„é—œä¿‚

#### æ–°æ ¼å¼ï¼ˆSentence-level Multi-Aspectï¼‰

```json
{
    "text": "The food was great but service was slow.",
    "aspects": ["food", "service"],
    "labels": [1, 0],  // [positive, negative]
    "num_aspects": 2,
    "aspect_positions": [[4, 8], [27, 34]]  // å¯é¸ï¼Œaspect åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
}
```

**ç‰¹é»:**
- âœ… ä¿ç•™å¥å­å®Œæ•´æ€§
- âœ… aspects ä¹‹é–“å¯ä»¥å»ºç«‹é—œä¿‚
- âœ… æ›´ç¬¦åˆçœŸå¯¦æ‡‰ç”¨å ´æ™¯

### å››ã€å¯¦æ–½æ­¥é©Ÿ

#### æ­¥é©Ÿ 1: æ•¸æ“šè™•ç†ï¼ˆ2-3 å°æ™‚ï¼‰

1.1 å‰µå»º multi-aspect æ•¸æ“šåŠ è¼‰å™¨
- æ–‡ä»¶: `data/semeval_multiaspect.py`
- åŠŸèƒ½: è§£æ XMLã€éæ¿¾ 2+ aspects å¥å­ã€æ§‹é€  multi-aspect æ ¼å¼

1.2 å‰µå»º Dataset é¡
- æ–‡ä»¶: `data/multiaspect_dataset.py`
- åŠŸèƒ½: `__getitem__` è¿”å› multi-aspect batchã€å‹•æ…‹ padding

#### æ­¥é©Ÿ 2: æ¨¡å‹ä¿®æ”¹ï¼ˆ3-4 å°æ™‚ï¼‰

2.1 ä¿®æ”¹ HMACNetBERT
- æ–‡ä»¶: `experiments/train_multiaspect.py`
- ä¿®æ”¹é»: forward æ¥å— `[batch, N, ...]` è¼¸å…¥

2.2 å¯¦ç¾ PMAC Multi-Aspect
- æ–‡ä»¶: `models/pmac_enhanced.py`
- æ–°å¢: `forward_multi_aspect()` æ–¹æ³•ã€æ¼¸é€²å¼çµ„åˆé‚è¼¯

2.3 å¯¦ç¾ IARM Multi-Aspect
- æ–‡ä»¶: `models/iarm_enhanced.py`
- ä¿®æ”¹: æ”¯æŒ `[batch, N, dim]` è¼¸å…¥ã€Transformer é—œä¿‚å»ºæ¨¡

#### æ­¥é©Ÿ 3: è¨“ç·´å¾ªç’°ä¿®æ”¹ï¼ˆ1-2 å°æ™‚ï¼‰

3.1 ä¿®æ”¹æå¤±è¨ˆç®— - è™•ç† multi-labelã€æ‡‰ç”¨ mask

3.2 ä¿®æ”¹è©•ä¼° - Aspect-level metricsã€Sentence-level metrics

### äº”ã€é æœŸçµæœå’Œè«–æ–‡è²¢ç»

#### é æœŸæ€§èƒ½

**ä¿å®ˆä¼°è¨ˆ:**
- Multi-aspect BERT Baseline: 82-84% (aspect-level accuracy)
- BERT + PMAC: 84-86%
- BERT + PMAC + IARM: 85-87%

**å¦‚æœå‰µæ–°æ¨¡çµ„æœ‰æ•ˆï¼ˆç›®æ¨™ï¼‰:**
- Multi-aspect å®Œæ•´æ¨¡å‹: 87-90%
- â†’ è¶…è¶Šç•¶å‰ SOTA (87.88%)
- â†’ æ¥è¿‘æœ€ä½³ SOTA (89.96%)

#### è«–æ–‡å‰µæ–°é»

**å‰µæ–° 1: Sentence-level Multi-Aspect Modeling**
- âœ… å•é¡Œ: ç¾æœ‰å·¥ä½œå¤§å¤šæ˜¯ aspect-levelï¼ˆç¨ç«‹é æ¸¬ï¼‰
- âœ… æ–¹æ³•: æå‡º sentence-level å»ºæ¨¡ï¼ˆè¯åˆé æ¸¬ï¼‰
- âœ… å„ªå‹¢: æ›´ç¬¦åˆçœŸå¯¦å ´æ™¯ï¼Œæ•æ‰ aspects é–“é—œä¿‚

**å‰µæ–° 2: Progressive Multi-Aspect Composition (PMAC)**
- âœ… å•é¡Œ: å¤šå€‹ aspects èªç¾©å¦‚ä½•æœ‰æ•ˆçµ„åˆï¼Ÿ
- âœ… æ–¹æ³•: æ¼¸é€²å¼é–€æ§èåˆï¼Œå¤šå°ºåº¦ç‰¹å¾µæå–
- âœ… å‰µæ–°: é¦–æ¬¡æå‡º aspect æ¼¸é€²çµ„åˆæ©Ÿåˆ¶

**å‰µæ–° 3: Inter-Aspect Relation Modeling (IARM)**
- âœ… å•é¡Œ: Aspects ä¹‹é–“çš„é—œä¿‚å¦‚ä½•å»ºæ¨¡ï¼Ÿ
- âœ… æ–¹æ³•: Transformer/GAT å»ºæ¨¡ aspect é—œä¿‚åœ–
- âœ… å‰µæ–°: é¡¯å¼å»ºæ¨¡æƒ…æ„Ÿä¸€è‡´æ€§/å°æ¯”æ€§

---

## ğŸ’¡ å¯¦é©—å»ºè­°

### æœ€ä½³é…ç½®ï¼ˆåŸºæ–¼ç¶“é©—ï¼‰

**é¤å»³é ˜åŸŸï¼š**
```bash
python train_bert.py \
  --domain restaurant \
  --batch_size 16 \
  --epochs 25 \
  --lr 2e-5
```

**ç­†é›»é ˜åŸŸï¼š**
```bash
python train_bert.py \
  --domain laptop \
  --batch_size 16 \
  --epochs 30 \
  --lr 2e-5
```

### è¨˜æ†¶é«”å—é™ç’°å¢ƒ

```bash
python train_bert.py \
  --domain restaurant \
  --freeze_bert \
  --batch_size 32 \
  --epochs 20
```

---

**ç¥å¯¦é©—é †åˆ©ï¼ğŸš€**
