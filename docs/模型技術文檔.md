# ABSA 模型技術文檔

本文檔詳細描述四個階段的 Aspect-Based Sentiment Analysis (ABSA) 模型架構。

---

## 目錄

1. [Baseline: BERT-CLS](#1-baseline-bert-cls)
2. [Method 1: Hierarchical BERT](#2-method-1-hierarchical-bert)
3. [Method 1b: HBL (Layer-wise Attention)](#3-method-1b-hbl-layer-wise-attention)
4. [Method 2: IARN](#4-method-2-iarn-inter-aspect-relation-network)
5. [Method 3: HSA](#5-method-3-hsa-hierarchical-syntax-attention)
6. [Method 4: Unified-HIARN](#6-method-4-unified-hiarn) (統一模型)
7. [模型比較總覽](#7-模型比較總覽)
8. [統一訓練配置](#8-統一訓練配置)

---

## 1. Baseline: BERT-CLS

### 1.1 原理概述

BERT-CLS 是最基礎的 ABSA 方法，直接利用 BERT 預訓練模型的 `[CLS]` token 表示進行情感分類。這種方法假設 `[CLS]` token 已經編碼了整個句子對（文本-aspect）的語義信息。

### 1.2 系統架構

```
┌─────────────────────────────────────────────────────────────┐
│                      輸入處理                                │
│  Text: "The food is great but service is slow"              │
│  Aspect: "food"                                             │
│  → [CLS] text [SEP] aspect [SEP]                           │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                    BERT Encoder                             │
│  • 12 層 Transformer                                        │
│  • Hidden size: 768                                         │
│  • 輸出: [batch, seq_len, 768]                             │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│               [CLS] Token 提取                              │
│  • 提取第 0 位置的 hidden state                             │
│  • 維度: [batch, 768]                                      │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                    分類器                                    │
│  • Dropout(p=0.3)                                          │
│  • Linear(768 → 3)                                         │
│  • 輸出: [positive, neutral, negative]                     │
└─────────────────────────────────────────────────────────────┘
```

### 1.3 配置文件

**檔案位置**: `configs/unified_baseline.yaml`

```yaml
model:
  baseline: "bert_cls"
  bert_model: "bert-base-uncased"
  freeze_bert: false
  dropout: 0.3

training:
  batch_size: 32
  epochs: 30
  lr: 2.0e-5
  weight_decay: 0.01
  focal_gamma: 2.0
  loss_type: "focal"
```

### 1.4 技術關鍵細節

| 項目 | 說明 |
|------|------|
| **實現位置** | `experiments/baselines.py` (lines 30-133) |
| **類別名稱** | `BERT_CLS_Baseline` |
| **輸入格式** | Sentence-pair: `[CLS] text [SEP] aspect [SEP]` |
| **特徵維度** | 768 (BERT hidden size) |
| **處理方式** | 每個 aspect 獨立處理，無 aspect 間交互 |
| **優點** | 簡單高效、參數量少、訓練穩定 |
| **缺點** | 無法捕捉多層次語義、忽略 aspect 間關係 |

### 1.5 核心程式碼邏輯

```python
def forward(self, batch):
    for aspect_idx in range(max_aspects):
        # 1. BERT 編碼
        outputs = self.bert(input_ids, attention_mask)

        # 2. 提取 [CLS] token
        cls_output = outputs.last_hidden_state[:, 0, :]  # [batch, 768]

        # 3. 分類
        logits = self.classifier(self.dropout(cls_output))  # [batch, 3]
```

---

## 2. Method 1: Hierarchical BERT

### 2.1 原理概述

Hierarchical BERT 基於一個關鍵觀察：**BERT 的不同層捕捉不同層次的語言資訊**：
- **低層 (1-4)**: 詞彙/句法特徵（詞形、詞性）
- **中層 (5-8)**: 語義特徵（片語意義、語義組合）
- **高層 (9-12)**: 任務特定特徵（情感傾向、上下文推理）

通過融合多層特徵，模型能夠同時利用不同粒度的語言表示。

### 2.2 系統架構

```
┌─────────────────────────────────────────────────────────────┐
│                    BERT Encoder (12 層)                     │
│  output_hidden_states=True                                  │
│  輸出: 13 個 hidden states (embedding + 12 layers)          │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                 多層次特徵提取                               │
│  ┌──────────────┬──────────────┬──────────────┐            │
│  │  Low (1-4)   │  Mid (5-8)   │  High (9-12) │            │
│  │  4×768=3072  │  4×768=3072  │  4×768=3072  │            │
│  └──────────────┴──────────────┴──────────────┘            │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                   層級融合網路                               │
│  ┌──────────────────────────────────────────────┐          │
│  │ Low Fusion:  Linear(3072→768) + LN + ReLU    │          │
│  │ Mid Fusion:  Linear(3072→768) + LN + ReLU    │          │
│  │ High Fusion: Linear(3072→768) + LN + ReLU    │          │
│  └──────────────────────────────────────────────┘          │
│                          ↓                                  │
│  Concatenate: [low; mid; high] → 2304 維                   │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                      分類器                                  │
│  • Linear(2304 → 768) + LayerNorm + ReLU + Dropout         │
│  • Linear(768 → 3)                                         │
└─────────────────────────────────────────────────────────────┘
```

### 2.3 配置文件

**檔案位置**: `configs/unified_hierarchical.yaml`

```yaml
model:
  improved: "hierarchical"
  bert_model: "bert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  dropout: 0.4  # 比 baseline 略高，防止過擬合

training:
  batch_size: 32
  epochs: 30
  lr: 2.0e-5
  focal_gamma: 2.0
```

### 2.4 技術關鍵細節

| 項目 | 說明 |
|------|------|
| **實現位置** | `experiments/improved_models.py` (lines 35-203) |
| **類別名稱** | `HierarchicalBERT` |
| **層級劃分** | Low: [1,2,3,4], Mid: [5,6,7,8], High: [9,10,11,12] |
| **特徵維度** | 每層 768 → 融合後 768 × 3 = 2304 |
| **參數增量** | ~3× baseline (主要來自融合層) |
| **創新點** | 利用 BERT 內在的層級結構 |

### 2.5 理論依據

BERT 層級特徵的語言學解釋：

| 層級 | 捕捉的資訊 | 情感分析中的作用 |
|------|-----------|-----------------|
| Layer 1-4 | 詞彙形態、基本句法 | 識別情感詞（excellent, terrible） |
| Layer 5-8 | 短語結構、語義組合 | 理解修飾關係（not good, very bad） |
| Layer 9-12 | 上下文推理、任務特定 | 綜合判斷、處理複雜表達 |

---

## 3. Method 1b: HBL (Layer-wise Attention)

### 3.1 原理概述

HBL (Hierarchical BERT + Layer-wise Attention) 是 Hierarchical BERT 的改進版本，基於 **UDify (EMNLP 2019)** 的 Layer-wise Attention 機制。核心改進是將固定的層級拼接替換為**可學習的動態權重**：

- **Hierarchical BERT**: `concat([low, mid, high])` → 固定 1:1:1 權重
- **HBL**: `β₁×low + β₂×mid + β₃×high` → 動態學習權重

### 3.2 系統架構

```
┌─────────────────────────────────────────────────────────────┐
│                    BERT Encoder (12 層)                     │
│  output_hidden_states=True                                  │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                 多層次特徵提取 (同 Hierarchical BERT)        │
│  ┌──────────────┬──────────────┬──────────────┐            │
│  │  Low (1-4)   │  Mid (5-8)   │  High (9-12) │            │
│  │  → 768 dim   │  → 768 dim   │  → 768 dim   │            │
│  └──────────────┴──────────────┴──────────────┘            │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│              ⭐ Layer-wise Attention (核心改進)              │
│                                                             │
│  α = [w_low, w_mid, w_high]  # 可學習參數 (初始化: 0.5, 1.0, 1.5)
│  β = softmax(α)              # 歸一化權重                    │
│                                                             │
│  h = β₁ × low + β₂ × mid + β₃ × high  # 加權組合            │
│  輸出: 768 維 (vs Hierarchical BERT 的 2304 維)             │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                      分類器                                  │
│  • Linear(768 → 768) + LayerNorm + ReLU + Dropout          │
│  • Linear(768 → 3)                                         │
└─────────────────────────────────────────────────────────────┘
```

### 3.3 配置文件

**檔案位置**: `configs/unified_hbl.yaml`

```yaml
model:
  improved: "hierarchical_layerattn"
  bert_model: "bert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  dropout: 0.4

training:
  batch_size: 32
  epochs: 30
  lr: 2.0e-5
  focal_gamma: 2.0
```

### 3.4 技術關鍵細節

| 項目 | 說明 |
|------|------|
| **實現位置** | `experiments/improved_models.py` (HierarchicalBERT_LayerAttn) |
| **類別名稱** | `HierarchicalBERT_LayerAttn` |
| **與 Hierarchical BERT 差異** | 動態加權 vs 固定拼接 |
| **分類器輸入維度** | 768 (vs Hierarchical BERT 的 2304) |
| **參數量** | 較少（分類器維度減小） |
| **可解釋性** | 可視化學到的層級權重 |

### 3.5 與 Hierarchical BERT 對比

| 特性 | Hierarchical BERT | HBL |
|------|-------------------|-----|
| **融合方式** | 固定拼接 concat | 動態加權 weighted sum |
| **權重** | 1:1:1 固定 | softmax([w₁,w₂,w₃]) 可學習 |
| **特徵維度** | 2304 | 768 |
| **分類器參數** | 較多 | 較少 |
| **可解釋性** | 無 | 可視化權重 |
| **預期效果** | 基準 | +0.5~1.0% Macro-F1 |

### 3.6 參考文獻

> Kondratyuk, D., & Straka, M. (2019). **75 Languages, 1 Model: Parsing Universal Dependencies Universally**. In Proceedings of EMNLP-IJCNLP 2019 (pp. 2779-2795).

---

## 4. Method 2: IARN (Inter-Aspect Relation Network)

### 4.1 原理概述

IARN 的核心創新在於**顯式建模 aspect 之間的關係**。在多 aspect 場景中（如 MAMS 數據集），不同 aspect 的情感可能相互影響：
- **對比關係**: "food is great but service is terrible"
- **因果關係**: "slow service made the experience frustrating"
- **一致關係**: "both food and ambiance are excellent"

IARN 通過 Multi-head Attention 讓每個 aspect 能夠「看到」其他 aspect，並通過 Gating 機制決定是否採納其他 aspect 的資訊。

### 3.2 系統架構

```
┌─────────────────────────────────────────────────────────────┐
│              BERT + 層級特徵提取 (同 Hierarchical)           │
│  每個 aspect 獲得: [low; mid; high] → 2304 維               │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│              堆疊所有 Aspect 特徵                            │
│  aspect_features: [batch, max_aspects, 2304]                │
│                                                             │
│  ┌─────┬─────┬─────┬─────┐                                 │
│  │ A1  │ A2  │ A3  │ ... │  (max_aspects 個)               │
│  └─────┴─────┴─────┴─────┘                                 │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│            Aspect-to-Aspect Multi-head Attention            │
│                                                             │
│  Q = K = V = aspect_features                                │
│  attention_output = Softmax(QK^T/√d) × V                   │
│                                                             │
│  ┌─────────────────────────────────────────┐               │
│  │     A1 ←→ A2 ←→ A3 ←→ A4               │               │
│  │     ↑_____↑_____↑_____↑                 │               │
│  │         互相注意                         │               │
│  └─────────────────────────────────────────┘               │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│              Relation-aware Gating                          │
│                                                             │
│  combined = [self_features; context_features]  # 4608 維   │
│  gate = σ(MLP(combined))                       # [0, 1]    │
│                                                             │
│  fused = gate × self + (1-gate) × context                  │
│                                                             │
│  gate ≈ 1: 主要依賴自身特徵                                 │
│  gate ≈ 0: 主要依賴其他 aspect 提供的上下文                  │
│  gate ≈ 0.5: 平衡兩者                                       │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                      分類器                                  │
│  • Linear(2304 → 768) + LayerNorm + ReLU + Dropout         │
│  • Linear(768 → 3)                                         │
└─────────────────────────────────────────────────────────────┘
```

### 3.3 配置文件

**檔案位置**: `configs/unified_iarn.yaml`

```yaml
model:
  improved: "iarn"
  bert_model: "bert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  num_heads: 4  # Multi-head attention 頭數
  dropout: 0.3

training:
  batch_size: 32
  epochs: 30
  lr: 2.0e-5
  focal_gamma: 2.0
```

### 3.4 技術關鍵細節

| 項目 | 說明 |
|------|------|
| **實現位置** | `experiments/improved_models.py` (lines 453-674) |
| **類別名稱** | `InterAspectRelationNetwork` |
| **注意力機制** | Multi-head Self-Attention (4 heads) |
| **Gating 維度** | 4608 → 1 (per aspect) |
| **輸出額外資訊** | attention_weights, gate_values |
| **最佳適用** | 多 aspect 場景 (MAMS 數據集) |

### 3.5 Gating 機制詳解

Gating 機制是 IARN 的關鍵設計，它學習如何平衡「自身資訊」和「其他 aspect 資訊」：

```python
# 計算 gate 值
combined = torch.cat([self_features, context_features], dim=-1)  # [B, A, 4608]
gate = torch.sigmoid(self.gate_mlp(combined))                    # [B, A, 1]

# 融合
fused = gate * self_features + (1 - gate) * context_features
```

**Gate 值的解讀**：

| Gate 值 | 含義 | 典型場景 |
|---------|------|---------|
| > 0.7 | 高度獨立，依賴自身 | 單 aspect 或獨立陳述 |
| 0.3-0.7 | 需要上下文輔助 | 對比句、關聯表達 |
| < 0.3 | 高度依賴其他 aspect | 強關聯場景 |

### 3.6 可解釋性輸出

IARN 返回額外的診斷資訊：

```python
extras = {
    'aspect_attention_weights': [batch, max_aspects, max_aspects],
    'gate_values': [batch, max_aspects],
    'avg_gate': scalar
}
```

這些資訊可用於：
- 視覺化 aspect 間的注意力分布
- 分析模型對不同類型句子的處理策略
- 錯誤分析與模型調試

---

## 4. Method 3: HSA (Hierarchical Syntax Attention)

### 4.1 原理概述

HSA 引入**語言學層級結構**來增強情感分析：
- **Token 層**: 單詞級別的情感訊號
- **Phrase 層**: 片語級別的語義組合（如 "not good", "very bad"）
- **Clause 層**: 子句級別的複雜表達（如 "although X, but Y"）

與 Hierarchical BERT（利用模型層級）和 IARN（利用 aspect 關係）不同，HSA 專注於**語法結構驅動的特徵聚合**。

### 4.2 系統架構

```
┌─────────────────────────────────────────────────────────────┐
│                    BERT Encoding                            │
│  輸出: hidden_states [batch, seq_len, 768]                  │
│        aspect_repr (來自 [CLS] 或 aspect 位置)              │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│          Level 1: Token-level Attention                     │
│  ┌────────────────────────────────────────────┐            │
│  │ Query: aspect_repr                          │            │
│  │ Key/Value: token embeddings                 │            │
│  │ 輸出: weighted token features [batch, 768]  │            │
│  └────────────────────────────────────────────┘            │
│  捕捉: 直接情感詞 (excellent, terrible, amazing)           │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│          Level 2: Phrase-level (1-hop Syntax)               │
│  ┌────────────────────────────────────────────┐            │
│  │ Adjacency Matrix: window_size=3 (±1 鄰居)   │            │
│  │ Syntax Aggregation:                         │            │
│  │   phrase_features = A × token_features      │            │
│  │                                             │            │
│  │ Hierarchical Syntax Layer:                  │            │
│  │   • Multi-head attention                    │            │
│  │   • Syntax-guided gating                    │            │
│  │   • Feed-forward + LayerNorm                │            │
│  │                                             │            │
│  │ Aspect Attention: 同 Level 1               │            │
│  └────────────────────────────────────────────┘            │
│  捕捉: 修飾語組合 (not good, very delicious)               │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│          Level 3: Clause-level (2-hop Syntax)               │
│  ┌────────────────────────────────────────────┐            │
│  │ Adjacency Matrix: window_size=5 (±2 鄰居)   │            │
│  │ 結構同 Level 2，但聚合範圍更大               │            │
│  └────────────────────────────────────────────┘            │
│  捕捉: 複雜表達 (although X, Y is still Z)                 │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│              Hierarchical Fusion                            │
│                                                             │
│  level_weights = softmax([w1, w2, w3])  # 可學習權重        │
│                                                             │
│  concat = [w1×token; w2×phrase; w3×clause]  # [B, 2304]    │
│  fused = Linear(2304→768)(concat)                          │
│                                                             │
│  權重示例: [0.25, 0.35, 0.40] (clause 層最重要)             │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                      分類器                                  │
│  • Linear(768 → 384) + LayerNorm + GELU + Dropout          │
│  • Linear(384 → 3)                                         │
└─────────────────────────────────────────────────────────────┘
```

### 4.3 配置文件

**檔案位置**: `configs/unified_hsa.yaml`

```yaml
model:
  improved: "hsa"
  bert_model: "bert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  num_syntax_layers: 2  # 語法傳播深度
  dropout: 0.3

training:
  batch_size: 32
  epochs: 30
  lr: 2.0e-5
  focal_gamma: 2.0
```

### 4.4 技術關鍵細節

| 項目 | 說明 |
|------|------|
| **實現位置** | `models/hierarchical_syntax.py` (lines 211-505) |
| **類別名稱** | `HierarchicalSyntaxAttention` |
| **核心模組** | `SyntaxAwareAttention`, `HierarchicalSyntaxLayer` |
| **鄰接矩陣** | 滑動窗口近似語法依賴 |
| **層級融合** | Learnable softmax weights |

### 4.5 核心模組詳解

#### 4.5.1 SyntaxAwareAttention

```python
class SyntaxAwareAttention(nn.Module):
    """Aspect-aware attention with optional syntax distance weighting"""

    def forward(self, hidden_states, aspect_repr, syntax_mask=None):
        # hidden_states: [batch, seq_len, hidden]
        # aspect_repr: [batch, 1, hidden]

        queries = self.query_proj(aspect_repr)      # [B, 1, H]
        keys = self.key_proj(hidden_states)         # [B, L, H]
        values = self.value_proj(hidden_states)     # [B, L, H]

        scores = torch.matmul(queries, keys.transpose(-1, -2)) / self.scale

        if syntax_mask is not None:
            scores = scores + syntax_mask  # 偏向語法相近的詞

        weights = F.softmax(scores, dim=-1)
        output = torch.matmul(weights, values)

        return output.squeeze(1), weights
```

#### 4.5.2 HierarchicalSyntaxLayer

```python
class HierarchicalSyntaxLayer(nn.Module):
    """Aggregates neighbor information via syntax-guided gating"""

    def forward(self, hidden_states, adjacency_matrix):
        # 1. Self-attention
        attn_output, _ = self.neighbor_attention(
            hidden_states, hidden_states, hidden_states
        )

        # 2. Syntax-guided aggregation
        syntax_context = torch.matmul(adjacency_matrix, hidden_states)

        # 3. Gating
        combined = torch.cat([attn_output, syntax_context], dim=-1)
        gate = torch.sigmoid(self.syntax_gate(combined))
        fused = gate * attn_output + (1 - gate) * syntax_context

        # 4. Feed-forward + residual
        output = self.ffn(fused) + hidden_states
        return self.layer_norm(output)
```

#### 4.5.3 鄰接矩陣生成

```python
def _create_syntax_adjacency(self, seq_len, window_size, attention_mask):
    """Create sliding-window based adjacency matrix"""

    adjacency = torch.zeros(seq_len, seq_len)

    for i in range(seq_len):
        start = max(0, i - window_size // 2)
        end = min(seq_len, i + window_size // 2 + 1)
        adjacency[i, start:end] = 1.0

    # Normalize by degree
    degree = adjacency.sum(dim=-1, keepdim=True)
    adjacency = adjacency / degree.clamp(min=1)

    # Apply attention mask
    adjacency = adjacency * attention_mask.unsqueeze(-1)

    return adjacency
```

### 4.6 三層語言學對應

| 層級 | 窗口大小 | 聚合範圍 | 語言學意義 | 捕捉的情感模式 |
|------|---------|---------|-----------|---------------|
| Token | - | 單詞 | 詞彙語義 | good, bad, excellent |
| Phrase | 3 (±1) | 相鄰詞 | 修飾關係 | not good, very bad |
| Clause | 5 (±2) | 更寬範圍 | 子句結構 | although X, but Y |

### 4.7 可解釋性輸出

```python
extras = {
    'level_weights': [0.25, 0.35, 0.40],  # 各層重要性
    'token_attention': [batch, max_aspects, seq_len],
    'phrase_attention': [batch, max_aspects, seq_len],
    'clause_attention': [batch, max_aspects, seq_len]
}
```

---

## 5. Method 4: Unified-HIARN

### 5.1 原理概述

Unified-HIARN 的核心創新在於**根據數據集的多面向比例自動選擇策略**（數據集級別，非樣本級別）：

- **多面向比例 > 閾值 (50%)**：使用 **IARN 模式** (aspect 間交互)
- **多面向比例 ≤ 閾值**：使用 **Hierarchical 模式** (純階層特徵)

這實現了：
1. **統一模型**：無需針對不同數據集手動選擇模型
2. **結果一致性**：Hierarchical 模式與獨立 HierarchicalBERT 架構完全一致

### 5.2 系統架構

```
┌─────────────────────────────────────────────────────────────┐
│                    數據加載階段                              │
│  計算多面向比例: n_multi / n_total                          │
│  調用 model.set_mode(multi_aspect_ratio)                    │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│              BERT + 階層特徵提取 (共享)                      │
│  Low (L1-4) + Mid (L5-8) + High (L9-12) → concat           │
│  每層: [CLS] × 4 layers → 3072 維 → fusion → 768 維         │
│  最終: [low; mid; high] = 2304 維                           │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│              策略選擇 (數據集級別)                           │
│                                                             │
│  if multi_aspect_ratio > 0.5:                              │
│      mode = 'iarn'      → Aspect-to-Aspect Attention       │
│  else:                                                      │
│      mode = 'hierarchical' → 純階層特徵 (與獨立模型一致)    │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                      分類器                                  │
│  • Linear(2304 → 768) + LayerNorm + ReLU + Dropout         │
│  • Linear(768 → 3)                                         │
└─────────────────────────────────────────────────────────────┘
```

### 5.3 配置文件

**檔案位置**: `configs/unified_hiarn.yaml`

```yaml
model:
  improved: "unified_hiarn"
  bert_model: "bert-base-uncased"
  freeze_bert: false
  hidden_dim: 768
  dropout: 0.4  # 與獨立 Hierarchical 一致
  multi_aspect_threshold: 0.5  # 多面向比例閾值

training:
  batch_size: 32
  epochs: 30
  lr: 2.0e-5
  focal_gamma: 2.0
```

### 5.4 技術關鍵細節

| 項目 | 說明 |
|------|------|
| **實現位置** | `experiments/improved_models.py` (UnifiedHIARN class) |
| **類別名稱** | `UnifiedHIARN` |
| **策略選擇** | 數據集級別 (訓練開始時決定) |
| **閾值參數** | `multi_aspect_threshold` (default: 0.5) |
| **Hierarchical 分支** | 與獨立 HierarchicalBERT 完全一致 (concat, 非 mean) |
| **適用場景** | 所有 ABSA 數據集（統一模型） |

### 5.5 模式選擇機制

```python
def set_mode(self, multi_aspect_ratio: float):
    """訓練開始時調用，根據數據集特徵選擇模式"""
    if multi_aspect_ratio > self.multi_aspect_threshold:
        self._mode = 'iarn'
        # MAMS (100% multi-aspect) → 使用 IARN
    else:
        self._mode = 'hierarchical'
        # Restaurants/Laptops (~30% multi-aspect) → 使用 Hierarchical

# 在 forward 中根據 mode 選擇特徵
if self._mode == 'iarn':
    # Aspect-to-Aspect Attention + Gating
    final_features = iarn_features
else:
    # 純階層特徵 (與獨立 HierarchicalBERT 一致)
    final_features = hierarchical_features
```

### 5.6 可解釋性輸出

```python
extras = {
    'mode': 'hierarchical' or 'iarn',  # 當前運行模式
    'multi_aspect_ratio': float,        # 數據集的多面向比例
    'n_multi_aspect_samples': int,      # 批次中多 aspect 樣本數
    'n_single_aspect_samples': int,     # 批次中單 aspect 樣本數
    # IARN 模式額外輸出:
    'aspect_attention_weights': tensor, # Aspect 間注意力權重
    'iarn_gate_values': tensor          # IARN 內部 gate 值
}
```

### 5.7 預期結果對比

| 數據集 | 多面向比例 | 選擇模式 | 預期結果 |
|--------|-----------|----------|----------|
| Restaurants | ~30% | Hierarchical | ≈ 獨立 HierarchicalBERT |
| Laptops | ~25% | Hierarchical | ≈ 獨立 HierarchicalBERT |
| Rest16 | ~35% | Hierarchical | ≈ 獨立 HierarchicalBERT |
| Lap16 | ~40% | Hierarchical | ≈ 獨立 HierarchicalBERT |
| MAMS | 100% | IARN | ≈ 獨立 IARN |

---

## 6. 模型比較總覽

### 6.1 架構對比

| 特性 | BERT-CLS | Hierarchical | IARN | HSA | Unified-HIARN |
|------|----------|--------------|------|-----|---------------|
| **核心思想** | 直接使用 [CLS] | 融合 BERT 多層特徵 | 建模 aspect 間關係 | 語法驅動的層級注意力 | 動態融合 H+I |
| **特徵來源** | 最後一層 | 12 層分組融合 | 12 層 + aspect 交互 | BERT + 語法聚合 | 12 層 + 自適應 |
| **Aspect 處理** | 獨立 | 獨立 | 交互式 | 獨立 + 語法感知 | 自適應交互 |
| **額外結構** | 無 | 3 個融合網路 | 注意力 + Gating | 語法層 + 融合層 | 雙分支 + Fusion |

### 6.2 參數量對比

| 模型 | 相對參數量 | 主要增量來源 |
|------|-----------|-------------|
| BERT-CLS | 1× (baseline) | - |
| Hierarchical | ~3× | 層級融合網路 |
| IARN | ~4× | 注意力 + Gating |
| HSA | ~3.5× | 語法層 + 融合層 |
| Unified-HIARN | ~4.5× | 雙分支 + Fusion Gate |

### 6.3 適用場景

| 模型 | 最佳場景 | 優勢 | 劣勢 |
|------|---------|------|------|
| BERT-CLS | 簡單任務、baseline | 快速、穩定 | 特徵單一 |
| Hierarchical | 單 aspect 為主 | 多粒度特徵 | 無 aspect 交互 |
| IARN | 多 aspect 場景 (MAMS) | 捕捉 aspect 關係 | 單 aspect 場景效益低 |
| HSA | 複雜語法表達 | 語法感知、可解釋 | 計算量較大 |
| **Unified-HIARN** | **所有場景（統一模型）** | **自適應、無需選擇** | 參數量較大 |

### 6.4 實驗結果 (Macro-F1 %)

基於實際實驗數據（2025-11-24）：

| 數據集 | BERT-CLS | Hierarchical | IARN | HSA | 最佳模型 |
|--------|----------|--------------|------|-----|----------|
| **Restaurants** | 72.22 | **72.97** | 72.92 | 72.17 | Hierarchical |
| **Laptops** | 69.63 | 70.56 | 65.02 | **72.33** | HSA |
| **MAMS** | 83.05 | 83.29 | **84.56** | 82.74 | IARN |

**Per-class F1 分析 (Neutral 類別)**：

| 數據集 | BERT-CLS | Hierarchical | IARN | HSA |
|--------|----------|--------------|------|-----|
| Restaurants | 52.03 | 53.30 | **54.18** | 53.53 |
| Laptops | 51.52 | 52.20 | 41.48 | **58.90** |
| MAMS | 85.06 | 85.69 | **87.41** | 85.18 |

**關鍵發現**：
- Restaurants: 各模型差異不大（~1%），Hierarchical 略勝
- Laptops: HSA 在 Neutral 類別上優勢明顯（+6.7% vs Hierarchical）
- MAMS: IARN 表現最佳（+1.5% vs baseline），驗證了 aspect 交互建模的有效性
- IARN 在單 aspect 為主的數據集（Restaurants/Laptops）上表現較差，但在多 aspect 數據集（MAMS）上優勢明顯

---

## 6. 統一訓練配置

### 6.1 數據配置

```yaml
data:
  max_text_len: 128       # 最大文本長度
  max_aspect_len: 10      # 最大 aspect 長度
  min_aspects: 2          # 最少 aspect 數量（過濾用）
  max_aspects: 8          # 最多 aspect 數量
  include_single_aspect: true
  use_augmented: false
```

### 6.2 訓練配置

```yaml
training:
  batch_size: 32
  accumulation_steps: 1   # 梯度累積
  epochs: 30
  lr: 2.0e-5              # BERT 標準微調學習率
  weight_decay: 0.01
  grad_clip: 1.0
  patience: 10            # Early stopping
  use_scheduler: true
  warmup_ratio: 0.1       # 學習率預熱
  loss_type: "focal"      # Focal loss 處理類別不平衡
  focal_gamma: 2.0
  class_weights: "auto"   # 自動計算類別權重
```

### 6.3 損失函數

所有模型使用 **Focal Loss** 處理類別不平衡：

```python
FL(p_t) = -α_t × (1 - p_t)^γ × log(p_t)
```

其中：
- `γ = 2.0`: 聚焦參數，讓模型更關注難分類樣本
- `α_t`: 類別權重，自動根據數據分布計算

---

## 附錄：文件參考

| 組件 | 檔案路徑 |
|------|---------|
| Baseline 模型 | `experiments/baselines.py` |
| 改進模型 (Method 1-2) | `experiments/improved_models.py` |
| HSA 模型 (Method 3) | `models/hierarchical_syntax.py` |
| Baseline 配置 | `configs/unified_baseline.yaml` |
| Hierarchical 配置 | `configs/unified_hierarchical.yaml` |
| IARN 配置 | `configs/unified_iarn.yaml` |
| HSA 配置 | `configs/unified_hsa.yaml` |
| **Unified-HIARN 配置** | `configs/unified_hiarn.yaml` |
| 訓練腳本 | `experiments/train_from_config.py` |
| 核心訓練邏輯 | `experiments/train_multiaspect.py` |
