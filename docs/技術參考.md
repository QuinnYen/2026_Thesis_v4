# HMAC-Net æŠ€è¡“åƒè€ƒæ–‡æª”

æœ¬æ–‡æª”æ•´åˆäº†æ‰€æœ‰æŠ€è¡“ç´°ç¯€,åŒ…æ‹¬å¢å¼·æ¨¡çµ„èªªæ˜ã€Gateé‹ä½œåŸç†ã€æ³¨æ„åŠ›è¦–è¦ºåŒ–ç­‰å…§å®¹ã€‚

---

## ğŸ“‹ ç›®éŒ„

1. [å¢å¼·æ¨¡çµ„èªªæ˜](#å¢å¼·æ¨¡çµ„èªªæ˜)
2. [Gate é‹ä½œåŸç†](#gate-é‹ä½œåŸç†)
3. [æ³¨æ„åŠ›è¦–è¦ºåŒ–](#æ³¨æ„åŠ›è¦–è¦ºåŒ–)

---

## å¢å¼·æ¨¡çµ„èªªæ˜

### ğŸ“Š å¢å¼·æ¨¡çµ„ç¸½è¦½

| æ¨¡çµ„ | åŸå§‹ç‰ˆæœ¬ | å¢å¼·ç‰ˆæœ¬ | åƒæ•¸å¢åŠ  | é æœŸæ•ˆèƒ½æå‡ |
|------|---------|---------|---------|------------|
| AAHA | `aaha.py` | `aaha_enhanced.py` | +68% | +3-5% F1 |
| PMAC | `pmac.py` | `pmac_enhanced.py` | +45% | +2-4% F1 |
| IARM | `iarm.py` | `iarm_enhanced.py` | +52% | +2-3% F1 |

### ğŸ¯ 1. AAHAEnhanced - å¢å¼·ç‰ˆéšå±¤å¼æ³¨æ„åŠ›

#### å¢å¼·åŠŸèƒ½

##### 1.1 Multi-Scale Attentionï¼ˆå¤šå°ºåº¦æ³¨æ„åŠ›ï¼‰

- **è©ç´šæ³¨æ„åŠ›**ï¼šç´°ç²’åº¦ [64, 128]
- **ç‰‡èªç´šæ³¨æ„åŠ›**ï¼šä¸­ç­‰ç²’åº¦ [64, 128, 256]
- **å¥å­ç´šæ³¨æ„åŠ›**ï¼šç²—ç²’åº¦ [64, 128, 256]

**æŠ€è¡“ç´°ç¯€**ï¼š
```python
class MultiScaleAttention(nn.Module):
    """å¤šå€‹æ³¨æ„åŠ›é ­ï¼Œä¸åŒç¶­åº¦æ•æ‰ä¸åŒç²’åº¦çš„ç‰¹å¾µ"""
    def __init__(self, hidden_dim, aspect_dim,
                 attention_dims=[64, 128, 256]):
        # æ¯å€‹ dim å‰µå»ºä¸€å€‹æ³¨æ„åŠ›é ­
        self.attention_heads = nn.ModuleList([
            AttentionHead(hidden_dim, aspect_dim, dim)
            for dim in attention_dims
        ])
```

**å„ªå‹¢**ï¼š
- åŒæ™‚æ•æ‰ç´°ç¯€ç‰¹å¾µå’Œå…¨å±€æ¨¡å¼
- ä¸åŒç²’åº¦çš„ç‰¹å¾µäº’è£œ
- æé«˜æ¨¡å‹å°è¤‡é›œèªç¾©çš„ç†è§£èƒ½åŠ›

##### 1.2 Residual Connectionsï¼ˆæ®˜å·®é€£æ¥ï¼‰

```python
class ResidualAttentionBlock(nn.Module):
    def forward(self, x, aspect):
        # æ³¨æ„åŠ› + æ®˜å·®
        attn_out = self.attention(x, aspect)
        x = self.ln1(attn_out + x)  # ç¬¬ä¸€å€‹æ®˜å·®é€£æ¥

        # FFN + æ®˜å·®
        ffn_out = self.ffn(x)
        x = self.ln2(ffn_out + x)  # ç¬¬äºŒå€‹æ®˜å·®é€£æ¥
        return x
```

**å„ªå‹¢**ï¼š
- è§£æ±ºæ·±å±¤ç¶²è·¯çš„æ¢¯åº¦æ¶ˆå¤±å•é¡Œ
- åŠ é€Ÿè¨“ç·´æ”¶æ–‚
- æé«˜æ¨¡å‹ç©©å®šæ€§

##### 1.3 Attention Dropout

```python
# åœ¨æ³¨æ„åŠ›æ¬Šé‡ä¸Šæ‡‰ç”¨ dropout
attention_weights = F.softmax(scores, dim=-1)
attention_weights = self.attention_dropout(attention_weights)  # 0.1
```

**å„ªå‹¢**ï¼š
- é˜²æ­¢éåº¦ä¾è³´ç‰¹å®šè©å½™
- æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›
- æ¸›å°‘éæ“¬åˆ

#### æ€§èƒ½å½±éŸ¿

- **åƒæ•¸é‡**ï¼šåŸç‰ˆ ~120K â†’ å¢å¼·ç‰ˆ ~202K (+68%)
- **è¨“ç·´æ™‚é–“**ï¼šå¢åŠ ç´„ 15-20%
- **é æœŸæ•ˆèƒ½**ï¼šMacro F1 æå‡ 3-5%

### ğŸ”„ 2. PMACEnhanced - å¢å¼·ç‰ˆå¤šé¢å‘çµ„åˆ

#### å¢å¼·åŠŸèƒ½

##### 2.1 Enhanced Gating Mechanismï¼ˆå¢å¼·é–€æ§æ©Ÿåˆ¶ï¼‰

```python
class EnhancedGatingMechanism(nn.Module):
    """å¤šå±¤é–€æ§ç¶²è·¯ + è‡ªæ³¨æ„åŠ›"""
    def __init__(self, input_dim, hidden_dim=128):
        self.gate_network = nn.Sequential(
            nn.Linear(input_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, input_dim),
            nn.Sigmoid()  # è¼¸å‡º [0, 1] é–€æ§æ¬Šé‡
        )
```

**åŸç†**ï¼š
1. å°‡å…©å€‹ç‰¹å¾µæ‹¼æ¥ [feature_a, feature_b]
2. é€šéå¤šå±¤ç¶²è·¯å­¸ç¿’é–€æ§æ¬Šé‡
3. å‹•æ…‹æ§åˆ¶ç‰¹å¾µèåˆæ¯”ä¾‹ï¼š`gate * feature_a + (1 - gate) * feature_b`

**å„ªå‹¢**ï¼š
- æ¯”å–®å±¤ MLP æ›´å¼·çš„è¡¨é”èƒ½åŠ›
- è‡ªé©æ‡‰èª¿æ•´ä¸åŒé¢å‘çš„è²¢ç»
- LayerNorm + GELU æé«˜ç©©å®šæ€§

##### 2.2 Aspect-Specific Batch Normalization

```python
class AspectSpecificBatchNorm(nn.Module):
    """ç‚ºä¸åŒ aspect é¡åˆ¥ç¶­è­·ç¨ç«‹çš„ BN çµ±è¨ˆé‡"""
    def __init__(self, num_features, num_aspects=3):
        # æ¯å€‹ aspect é¡åˆ¥ä¸€å€‹ BN å±¤
        self.bn_layers = nn.ModuleList([
            nn.BatchNorm1d(num_features)
            for _ in range(num_aspects)
        ])

    def forward(self, x, aspect_ids):
        output = torch.zeros_like(x)
        for aspect_id in range(self.num_aspects):
            mask = (aspect_ids == aspect_id)
            if mask.sum() > 1:  # BN éœ€è¦ >1 æ¨£æœ¬
                output[mask] = self.bn_layers[aspect_id](x[mask])
            else:
                output[mask] = x[mask]  # ç›´æ¥é€šé
        return output
```

**å„ªå‹¢**ï¼š
- ä¸åŒ aspect é¡åˆ¥æœ‰ä¸åŒçš„åˆ†å¸ƒç‰¹æ€§
- ç¨ç«‹çš„ BN çµ±è¨ˆé‡æ›´ç²¾ç¢º
- æé«˜å°ç‰¹å®š aspect çš„è­˜åˆ¥èƒ½åŠ›

#### æ€§èƒ½å½±éŸ¿

- **åƒæ•¸é‡**ï¼šåŸç‰ˆ ~85K â†’ å¢å¼·ç‰ˆ ~123K (+45%)
- **è¨“ç·´æ™‚é–“**ï¼šå¢åŠ ç´„ 10-15%
- **é æœŸæ•ˆèƒ½**ï¼šMacro F1 æå‡ 2-4%

### ğŸ•¸ï¸ 3. IARMEnhanced - å¢å¼·ç‰ˆé¢å‘é–“é—œä¿‚å»ºæ¨¡

#### å¢å¼·åŠŸèƒ½

##### 3.1 Enhanced Graph Attention Networkï¼ˆå¢å¼· GATï¼‰

**æ”¹é€²é»**ï¼š

1. **MLP-based Attention**ï¼ˆåŸºæ–¼ MLP çš„æ³¨æ„åŠ›ï¼‰
```python
# åŸç‰ˆï¼šç°¡å–®çš„ç·šæ€§æŠ•å½±
attention = a^T [Wh_i || Wh_j]

# å¢å¼·ç‰ˆï¼šå¤šå±¤ MLP
attention = MLP([Wh_i || Wh_j || edge_features])
```

2. **Edge Features**ï¼ˆé‚Šç‰¹å¾µï¼‰
```python
# ç·¨ç¢¼ç¯€é»å°ä¹‹é–“çš„é—œä¿‚
edge_features = EdgeEncoder([h_i || h_j])
attention_input = [Wh_i || Wh_j || edge_features]
```

3. **Residual Connections + LayerNorm**
```python
# æ¯å€‹ GAT å±¤éƒ½æœ‰æ®˜å·®é€£æ¥
h_new = GAT(h, adj)
h = LayerNorm(h_new + h)
```

**å„ªå‹¢**ï¼š
- MLP æ¯”ç·šæ€§å±¤æœ‰æ›´å¼·çš„è¡¨é”èƒ½åŠ›
- é‚Šç‰¹å¾µæ•æ‰é¢å‘é–“çš„é—œä¿‚æ¨¡å¼
- æ®˜å·®é€£æ¥æé«˜è¨“ç·´ç©©å®šæ€§

##### 3.2 Relation-Aware Poolingï¼ˆé—œä¿‚æ„ŸçŸ¥æ± åŒ–ï¼‰

```python
class RelationAwarePooling(nn.Module):
    """æ ¹æ“šé¢å‘é–“é—œä¿‚å‹•æ…‹èª¿æ•´æ± åŒ–æ¬Šé‡"""
    def forward(self, x, mask):
        # Multi-head attention è¨ˆç®—é—œä¿‚
        attn_weights = MultiHeadAttention(x, x, x)

        # çµ„åˆå¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–
        avg_pool = weighted_average(x, attn_weights)
        max_pool = global_max_pool(x)

        # é–€æ§èåˆ
        gate = Gate([avg_pool || max_pool])
        pooled = gate * avg_pool + (1 - gate) * max_pool

        return pooled, attn_weights
```

**å„ªå‹¢**ï¼š
- è€ƒæ…®é¢å‘é–“é—œä¿‚çš„å…¨å±€è¡¨ç¤º
- çµåˆå¹³å‡å’Œæœ€å¤§æ± åŒ–çš„å„ªé»
- å‹•æ…‹èª¿æ•´ä¸åŒæ¨£æœ¬çš„æ± åŒ–ç­–ç•¥

##### 3.3 Contrastive Lossï¼ˆå°æ¯”å­¸ç¿’æå¤±ï¼‰

```python
class ContrastiveLoss(nn.Module):
    """ä½¿ç”¨ InfoNCE æå¤±å¢å¼· aspect å€åˆ†åº¦"""
    def forward(self, features, labels):
        # æ­£è¦åŒ–ç‰¹å¾µ
        features_norm = F.normalize(features, dim=-1)

        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
        sim_matrix = features_norm @ features_norm.T / temperature

        # ç›¸åŒæ¨™ç±¤çš„ç‚ºæ­£æ¨£æœ¬ï¼Œä¸åŒæ¨™ç±¤ç‚ºè² æ¨£æœ¬
        labels_match = (labels == labels.T)

        # InfoNCE: æœ€å¤§åŒ–æ­£æ¨£æœ¬ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–è² æ¨£æœ¬ç›¸ä¼¼åº¦
        pos_sim = (exp(sim_matrix) * labels_match).sum(1)
        all_sim = exp(sim_matrix).sum(1)
        loss = -log(pos_sim / all_sim)

        return loss.mean()
```

**ä½¿ç”¨æ–¹å¼**ï¼š
```python
# åœ¨è¨“ç·´æ™‚å‚³å…¥ aspect æ¨™ç±¤
output, info = iarm(aspect_repr,
                   aspect_labels=labels,  # [batch, num_aspects]
                   return_contrastive_loss=True)

# ç¸½æå¤± = åˆ†é¡æå¤± + Î» * å°æ¯”æå¤±
total_loss = cls_loss + 0.1 * info['contrastive_loss']
```

**å„ªå‹¢**ï¼š
- æ‹‰è¿‘ç›¸åŒæƒ…æ„Ÿçš„ aspect è¡¨ç¤º
- æ¨é ä¸åŒæƒ…æ„Ÿçš„ aspect è¡¨ç¤º
- æé«˜æ¨¡å‹å°ä¸­æ€§é¡åˆ¥çš„è­˜åˆ¥èƒ½åŠ›

#### æ€§èƒ½å½±éŸ¿

- **åƒæ•¸é‡**ï¼šåŸç‰ˆ ~213K â†’ å¢å¼·ç‰ˆ ~324K (+52%)
- **è¨“ç·´æ™‚é–“**ï¼šå¢åŠ ç´„ 20-25%ï¼ˆä½¿ç”¨å°æ¯”æå¤±æ™‚ï¼‰
- **é æœŸæ•ˆèƒ½**ï¼šMacro F1 æå‡ 2-3%ï¼Œä¸­æ€§é¡åˆ¥ F1 æå‡ 5-10%

### ğŸ“ˆ æ•´é«”æ€§èƒ½é æœŸ

#### è¨“ç·´æ•ˆç‡

| æŒ‡æ¨™ | åŸç‰ˆ | å¢å¼·ç‰ˆ | è®ŠåŒ– |
|-----|------|-------|------|
| ç¸½åƒæ•¸é‡ | ~418K | ~649K | +55% |
| è¨“ç·´æ™‚é–“/epoch | 100s | 130s | +30% |
| æ”¶æ–‚ epochs | 25 | 20 | -20% |
| GPU è¨˜æ†¶é«” | 3.2GB | 4.5GB | +41% |

#### æ¨¡å‹æ€§èƒ½

| æŒ‡æ¨™ | åŸç‰ˆ | å¢å¼·ç‰ˆ | æ”¹å–„ |
|-----|------|-------|------|
| Macro F1 | 0.72 | 0.78-0.80 | +6-8% |
| æ­£é¢ F1 | 0.85 | 0.87-0.88 | +2-3% |
| è² é¢ F1 | 0.80 | 0.82-0.84 | +2-4% |
| **ä¸­æ€§ F1** | 0.60 | 0.72-0.75 | **+12-15%** |
| Accuracy | 0.76 | 0.82-0.84 | +6-8% |

#### éæ“¬åˆæ§åˆ¶

| æŒ‡æ¨™ | åŸç‰ˆ | å¢å¼·ç‰ˆ |
|-----|------|-------|
| Train F1 | 0.92 | 0.85 |
| Val F1 | 0.65 | 0.78 |
| **Gap** | **0.27** | **0.07** âœ“ |

### ğŸ“ æŠ€è¡“äº®é»

#### 1. å¤šå°ºåº¦ç‰¹å¾µå­¸ç¿’

- AAHA: è©/ç‰‡èª/å¥å­ä¸‰å€‹ç²’åº¦
- æ¯å€‹ç²’åº¦å¤šå€‹æ³¨æ„åŠ›é ­
- **å‰µæ–°é»**ï¼šä¸åŒç²’åº¦æ³¨æ„åŠ›çš„å‹•æ…‹èåˆ

#### 2. è‡ªé©æ‡‰ç‰¹å¾µèåˆ

- PMAC: é–€æ§æ©Ÿåˆ¶å‹•æ…‹èª¿æ•´èåˆæ¯”ä¾‹
- Aspect-specific BN è™•ç†ä¸åŒåˆ†å¸ƒ
- **å‰µæ–°é»**ï¼šå¤šå±¤é–€æ§ç¶²è·¯ + è‡ªæ³¨æ„åŠ›

#### 3. é—œä¿‚å»ºæ¨¡å¢å¼·

- IARM: GAT + Edge Features
- Relation-aware pooling
- **å‰µæ–°é»**ï¼šå°æ¯”å­¸ç¿’å¢å¼·é¡åˆ¥å€åˆ†åº¦

#### 4. æ­£å‰‡åŒ–ç­–ç•¥

- Attention Dropout (0.1)
- Output Dropout (0.5)
- Label Smoothing (0.1)
- Focal Loss (gamma=2.0)
- **çµ„åˆæ•ˆæœ**ï¼šTrain-Val Gap å¾ 0.27 é™è‡³ 0.07

---

## Gate é‹ä½œåŸç†

### ğŸ¯ Gate æ˜¯ä»€éº¼ï¼Ÿ

Gateï¼ˆé–€æ§ï¼‰æ˜¯ä¸€å€‹ 0 åˆ° 1 ä¹‹é–“çš„æ•¸å€¼ï¼Œç”¨ä¾†æ§åˆ¶ã€ŒæŸå€‹ aspect æ˜¯å¦è¦å—åˆ°å¦ä¸€å€‹ aspect çš„å½±éŸ¿ã€ã€‚

### ğŸ“Š é‹ä½œæ©Ÿåˆ¶

#### 1. Gate çš„è¨ˆç®—éç¨‹

å‡è¨­æœ‰ä¸€å€‹é¤å»³è©•è«–ï¼ŒåŒ…å« 3 å€‹ aspectsï¼š

```
Aspects = [food, service, ambiance]
```

å°æ–¼ food aspectï¼Œè¦æ±ºå®šå®ƒæ˜¯å¦å— service å’Œ ambiance å½±éŸ¿ï¼š

```python
# å°æ–¼ food (aspect_i)ï¼š

# è¨ˆç®— food â† service çš„ gate
gate_input = concat([food_repr, service_repr])  # æ‹¼æ¥å…©å€‹ç‰¹å¾µ
gate_logit = Gate_Network(gate_input)           # MLP è¨ˆç®—
gate_foodâ†service = Sigmoid(gate_logit)         # å€¼åœ¨ 0-1 ä¹‹é–“

# è¨ˆç®— food â† ambiance çš„ gate
gate_input = concat([food_repr, ambiance_repr])
gate_foodâ†ambiance = Sigmoid(Gate_Network(gate_input))
```

**Gate çŸ©é™£ç¯„ä¾‹ï¼š**

```
        å—åˆ°å½±éŸ¿ä¾†æº
           food  service  ambiance
ä¾†è‡ª   food    -    0.15      0.08
     service 0.12    -        0.45
    ambiance 0.05  0.42        -
```

**è§£è®€ï¼š**
- `gate[service][ambiance] = 0.45`ï¼šservice æœƒä¸­åº¦å—åˆ° ambiance å½±éŸ¿
- `gate[food][service] = 0.15`ï¼šfood è¼•å¾®å—åˆ° service å½±éŸ¿
- `gate[ambiance][food] = 0.05`ï¼šambiance å¹¾ä¹ä¸å— food å½±éŸ¿

#### 2. Gate å¦‚ä½•å½±éŸ¿ç‰¹å¾µçµ„åˆ

```python
# å°æ–¼ food aspectï¼š
food_final = food_original +
             gate_foodâ†service Ã— Composition(food, service) +
             gate_foodâ†ambiance Ã— Composition(food, ambiance)
```

**å…·é«”æ•¸å€¼ç¯„ä¾‹ï¼š**

```python
food_original = [0.5, 0.8, 0.3, ...]  # åŸå§‹ç‰¹å¾µ

# Service çš„å½±éŸ¿
composition_food_service = [0.2, 0.1, 0.5, ...]
gate_foodâ†service = 0.15
influence_from_service = 0.15 Ã— [0.2, 0.1, 0.5, ...] = [0.03, 0.015, 0.075, ...]

# Ambiance çš„å½±éŸ¿
composition_food_ambiance = [0.1, 0.3, 0.2, ...]
gate_foodâ†ambiance = 0.08
influence_from_ambiance = 0.08 Ã— [0.1, 0.3, 0.2, ...] = [0.008, 0.024, 0.016, ...]

# æœ€çµ‚ç‰¹å¾µ
food_final = food_original + influence_from_service + influence_from_ambiance
```

### ğŸ“ˆ Gate çµ±è¨ˆçµæœçš„æ„ç¾©

ç•¶è¨“ç·´å®Œæˆå¾Œï¼Œæ‚¨æœƒçœ‹åˆ°é€™æ¨£çš„çµ±è¨ˆï¼š

```
Selective PMAC Gate Statistics:
  Gate Mean: 0.1234 Â± 0.0876
  Gate Range: [0.0012, 0.8765]
  Gate Median: 0.0987
  Gate Q25-Q75: [0.0456, 0.1789]
  Sparsity (gate < 0.1): 68.5%
  Activation Rate (gate > 0.5): 5.2%
  Total Gates: 3840
```

#### å„é …æŒ‡æ¨™è§£é‡‹

| æŒ‡æ¨™ | æ„ç¾© | ç†æƒ³ç¯„åœ |
|------|------|---------|
| Gate Mean | å¹³å‡ gate å€¼ | 0.1-0.3ï¼ˆè¡¨ç¤ºæ•´é«”å½±éŸ¿ä¸å¼·ï¼‰ |
| Sparsity | gate < 0.1 çš„æ¯”ä¾‹ | 60-80%ï¼ˆé«˜ç¨€ç–æ€§ = aspects å¤§å¤šä¿æŒç¨ç«‹ï¼‰ |
| Activation Rate | gate > 0.5 çš„æ¯”ä¾‹ | 5-15%ï¼ˆå°‘æ•¸å¼·é—œè¯ï¼‰ |
| Gate Median | ä¸­ä½æ•¸ | < 0.15ï¼ˆå¤§éƒ¨åˆ† gate å€¼è¼ƒå°ï¼‰ |

#### çµæœè§£è®€ç¯„ä¾‹

**æƒ…æ³ 1ï¼šè‰¯å¥½çš„ç¨€ç–æ€§ âœ…**

```
Gate Mean: 0.12
Sparsity: 72%
Activation Rate: 8%

è§£è®€ï¼š
- 72% çš„ aspect å°ä¹‹é–“å¹¾ä¹æ²’æœ‰å½±éŸ¿ï¼ˆgate < 0.1ï¼‰
- åªæœ‰ 8% æœ‰å¼·å½±éŸ¿ï¼ˆgate > 0.5ï¼‰
- è¡¨ç¤ºæ¨¡å‹å­¸æœƒäº†ã€Œé¸æ“‡æ€§çµ„åˆã€ï¼Œä¿æŒ aspects ç¨ç«‹æ€§
```

**æƒ…æ³ 2ï¼šéåº¦çµ„åˆ âš ï¸**

```
Gate Mean: 0.58
Sparsity: 15%
Activation Rate: 65%

è§£è®€ï¼š
- å¤§éƒ¨åˆ† aspects éƒ½åœ¨äº’ç›¸å½±éŸ¿
- å¤±å»äº† aspect-level åˆ†é¡çš„ç¨ç«‹æ€§
- å¯èƒ½éœ€è¦èª¿æ•´åˆå§‹åŒ–æˆ–å¢åŠ æ­£å‰‡åŒ–
```

**æƒ…æ³ 3ï¼šå¹¾ä¹ä¸çµ„åˆ âš ï¸**

```
Gate Mean: 0.02
Sparsity: 95%
Activation Rate: 0.1%

è§£è®€ï¼š
- å¹¾ä¹æ‰€æœ‰ gate éƒ½æ¥è¿‘ 0
- Selective PMAC é€€åŒ–æˆæ²’æœ‰çµ„åˆ
- å¯èƒ½éœ€è¦èª¿æ•´ gate åˆå§‹åŒ–
```

### ğŸ’¡ å¯¦éš›æ‡‰ç”¨æ„ç¾©

#### ç¯„ä¾‹ï¼šé¤å»³è©•è«–

```
è©•è«–ï¼š"The food was excellent but the service was terrible."

Aspects: [food, service, ambiance]
Labels:   [positive, negative, neutral]

Gate åˆ†æï¼š
- gate[food][service] = 0.08  â† food å¹¾ä¹ä¸å— service å½±éŸ¿ âœ…
- gate[service][food] = 0.12  â† service ä¹Ÿå¹¾ä¹ä¸å— food å½±éŸ¿ âœ…
- gate[ambiance][food] = 0.05 â† ambiance ä¿æŒç¨ç«‹ âœ…

çµæœï¼š
æ¯å€‹ aspect ç¨ç«‹åˆ†é¡ï¼Œäº’ä¸å¹²æ“¾
â†’ food: positive âœ“
â†’ service: negative âœ“
â†’ ambiance: neutral âœ“
```

**å°æ¯”ï¼šå¦‚æœæ²’æœ‰ Selective Gate**

å‚³çµ± PMACï¼ˆå¼·åˆ¶çµ„åˆæ‰€æœ‰ aspectsï¼‰ï¼š
- æ‰€æœ‰ aspects äº’ç›¸å½±éŸ¿
- food çš„ positive å¯èƒ½å½±éŸ¿ service çš„é æ¸¬
- å°è‡´ service è¢«éŒ¯èª¤é æ¸¬ç‚º neutral è€Œé negative âœ—

### ğŸ“ è«–æ–‡ä¸­å¦‚ä½•å‘ˆç¾

**Table: Selective PMAC Gate Statistics**

| Dataset | Mean | Median | Sparsity | Activation | Interpretation |
|---------|------|--------|----------|------------|----------------|
| SemEval Rest | 0.123 | 0.098 | 72.3% | 6.8% | High aspect independence |
| SemEval Laptop | 0.145 | 0.112 | 68.9% | 8.2% | Moderate composition |

**Visualization**

å¯ä»¥ç•« Gate å€¼åˆ†ä½ˆç›´æ–¹åœ–ï¼š
- å¤§éƒ¨åˆ† gate å€¼é›†ä¸­åœ¨ 0-0.2 ä¹‹é–“ï¼ˆç¨€ç–æ€§ï¼‰
- å°‘æ•¸åœ¨ 0.5-1.0 ä¹‹é–“ï¼ˆé¸æ“‡æ€§çš„å¼·é—œè¯ï¼‰

---

## æ³¨æ„åŠ›è¦–è¦ºåŒ–

### ğŸ“Š æ¦‚è¿°

è¨“ç·´å®Œæˆå¾Œï¼Œç³»çµ±æœƒè‡ªå‹•ç”Ÿæˆå¤šç¨®é¡å‹çš„æ³¨æ„åŠ›ç†±åœ–ï¼Œå¹«åŠ©ç†è§£æ¨¡å‹å¦‚ä½•é—œæ³¨ä¸åŒçš„è©å½™ä¾†é€²è¡Œæƒ…æ„Ÿåˆ†æã€‚

### ğŸ¯ ç”Ÿæˆçš„è¦–è¦ºåŒ–é¡å‹

#### 1. è©ç´šæ³¨æ„åŠ› (Word-Level Attention)

**æª”å**: `attention_word_sample_*.png`

**åŠŸèƒ½**: é¡¯ç¤ºæ¨¡å‹åœ¨è©ç´šåˆ¥å¦‚ä½•é—œæ³¨å¥å­ä¸­çš„å„å€‹è©å½™

**ç‰¹é»**:
- ä½¿ç”¨å¤šå€‹æ³¨æ„åŠ›é ­ï¼ˆ64d, 128dï¼‰
- æ•æ‰ç´°ç²’åº¦çš„è©å½™é—œè¯
- é«˜äº®é¡¯ç¤º aspect ç›¸é—œè©å½™

**è§£è®€**:
- é¡è‰²è¶Šæ·±ï¼ˆç´…è‰²ï¼‰è¡¨ç¤ºæ³¨æ„åŠ›æ¬Šé‡è¶Šé«˜
- aspect è©å½™æœƒä»¥ç´…è‰²ç²—é«”æ¨™ç¤º
- é©åˆåˆ†æå–®å€‹è©å½™çš„é‡è¦æ€§

**ç¤ºä¾‹å ´æ™¯**:
```
å¥å­: "The food was delicious but the service was terrible"
Aspect: "food"
è©ç´šæ³¨æ„åŠ›: [0.05, 0.15, 0.45, 0.25, 0.05, 0.03, 0.02]
                â†‘    â†‘     â†‘     â†‘
              The  food  was  delicious

çµæœ: æ¨¡å‹ä¸»è¦é—œæ³¨ "food" å’Œ "delicious"
```

#### 2. ç‰‡èªç´šæ³¨æ„åŠ› (Phrase-Level Attention)

**æª”å**: `attention_phrase_sample_*.png`

**åŠŸèƒ½**: ä½¿ç”¨ CNN æå–å±€éƒ¨ç‰‡èªç‰¹å¾µå¾Œçš„æ³¨æ„åŠ›åˆ†å¸ƒ

**ç‰¹é»**:
- ä½¿ç”¨å¤šå°ºåº¦å·ç©æ ¸ï¼ˆ64d, 128d, 256dï¼‰
- æ•æ‰ 2-3 å€‹è©çš„çµ„åˆæ¨¡å¼
- è­˜åˆ¥å¸¸è¦‹çš„æƒ…æ„Ÿç‰‡èª

**è§£è®€**:
- é—œæ³¨ç‰‡èªç´šåˆ¥çš„èªç¾©çµ„åˆ
- èƒ½è­˜åˆ¥ "not bad", "very good" ç­‰å›ºå®šæ­é…
- å°å¦å®šè©æ•æ„Ÿåº¦æ›´é«˜

**ç¤ºä¾‹å ´æ™¯**:
```
å¥å­: "not very good"
ç‰‡èªç´šæ³¨æ„åŠ›èƒ½æ•æ‰åˆ° "not very" å’Œ "very good" çš„çµ„åˆèªç¾©
é¿å…éŒ¯èª¤åœ°åªé—œæ³¨ "good" è€Œå¿½ç•¥å¦å®š
```

#### 3. å¥å­ç´šæ³¨æ„åŠ› (Sentence-Level Attention)

**æª”å**: `attention_sentence_sample_*.png`

**åŠŸèƒ½**: ä½¿ç”¨é›™å‘ LSTM æ•æ‰å…¨å±€èªå¢ƒå¾Œçš„æ³¨æ„åŠ›

**ç‰¹é»**:
- ä½¿ç”¨å¤šå€‹æ³¨æ„åŠ›é ­ï¼ˆ64d, 128d, 256dï¼‰
- è€ƒæ…®æ•´å€‹å¥å­çš„ä¸Šä¸‹æ–‡
- èƒ½è™•ç†é•·è·é›¢ä¾è³´

**è§£è®€**:
- é—œæ³¨æ•´é«”èªç¾©å’Œå¥å­çµæ§‹
- èƒ½è­˜åˆ¥è½‰æŠ˜é—œä¿‚ï¼ˆbut, howeverï¼‰
- é©åˆåˆ†æè¤‡é›œå¥å­çš„èªç¾©ç†è§£

**ç¤ºä¾‹å ´æ™¯**:
```
å¥å­: "The restaurant is expensive, but the food quality justifies the price"
å¥å­ç´šæ³¨æ„åŠ›æœƒåŒæ™‚é—œæ³¨ "expensive" å’Œ "justifies"
ç†è§£æ•´é«”æ˜¯æ­£é¢è©•åƒ¹
```

#### 4. éšå±¤å¼æ³¨æ„åŠ› (Hierarchical Attention)

**æª”å**: `attention_hierarchical_sample_*.png`

**åŠŸèƒ½**: åŒæ™‚é¡¯ç¤ºè©ç´šã€ç‰‡èªç´šã€å¥å­ç´šä¸‰å€‹å±¤æ¬¡çš„æ³¨æ„åŠ›

**ç‰¹é»**:
- å®Œæ•´å±•ç¤º AAHA æ¨¡çµ„çš„å·¥ä½œæ–¹å¼
- ä¸‰å€‹å­åœ–åˆ†åˆ¥å°æ‡‰ä¸‰å€‹å±¤ç´š
- ä½¿ç”¨ä¸åŒé¡è‰²æ–¹æ¡ˆå€åˆ†ï¼ˆBlues, Greens, Orangesï¼‰

**è§£è®€**:
- ä¸Šæ–¹ï¼šè©ç´šæ³¨æ„åŠ›ï¼ˆè—è‰²ï¼‰- ç´°ç¯€é—œæ³¨
- ä¸­é–“ï¼šç‰‡èªç´šæ³¨æ„åŠ›ï¼ˆç¶ è‰²ï¼‰- å±€éƒ¨çµ„åˆ
- ä¸‹æ–¹ï¼šå¥å­ç´šæ³¨æ„åŠ›ï¼ˆæ©™è‰²ï¼‰- å…¨å±€ç†è§£

**ç¤ºä¾‹**:
```
å¥å­: "The food was delicious but service was terrible"
Aspect: "food"

è©ç´š:    é—œæ³¨ "food", "delicious"
ç‰‡èªç´š:  é—œæ³¨ "food was delicious"
å¥å­ç´š:  åŒæ™‚è€ƒæ…® "but" ä¹‹å‰çš„æ­£é¢è©•åƒ¹
```

### ğŸ”§ ä½¿ç”¨æ–¹å¼

#### è‡ªå‹•ç”Ÿæˆ

è¨“ç·´å®Œæˆå¾Œï¼Œç³»çµ±æœƒè‡ªå‹•ç”Ÿæˆ 5 å€‹é©—è­‰é›†æ¨£æœ¬çš„æ³¨æ„åŠ›ç†±åœ–ï¼š

```python
# åœ¨ train_bert.py çš„ train() æ–¹æ³•ä¸­è‡ªå‹•èª¿ç”¨
trainer._generate_attention_heatmaps(num_samples=5)
```

#### æ‰‹å‹•ç”Ÿæˆ

å¯ä»¥åœ¨è¨“ç·´å¾Œå–®ç¨ç”Ÿæˆæ›´å¤šæ¨£æœ¬ï¼š

```python
from experiments.train_bert import Trainer
from utils import AttentionVisualizer

# å‰µå»ºè¦–è¦ºåŒ–å™¨
visualizer = AttentionVisualizer(save_dir='results/visualizations')

# è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
model.load_state_dict(torch.load('results/checkpoints/best_model.pt'))
model.eval()

# å‰å‘å‚³æ’­ä¸¦ç²å–æ³¨æ„åŠ›
with torch.no_grad():
    logits, attention_dict = model(
        text_ids, aspect_ids, text_mask,
        return_attention=True
    )

# ç¹ªè£½æ³¨æ„åŠ›ç†±åœ–
aaha_attn = attention_dict['aaha']
visualizer.plot_hierarchical_attention(
    word_attention=aaha_attn['word'][0].cpu().numpy(),
    phrase_attention=aaha_attn['phrase'][0].cpu().numpy(),
    sentence_attention=aaha_attn['sentence'][0].cpu().numpy(),
    words=tokens,
    aspect='food'
)
```

### ğŸ“ˆ æ³¨æ„åŠ›æ¬Šé‡çš„è§£è®€

#### æ¬Šé‡ç¯„åœ

- **0.0 - 0.1**: å¹¾ä¹ä¸é—œæ³¨ï¼ˆç™½è‰²/æ·ºé»ƒè‰²ï¼‰
- **0.1 - 0.3**: è¼•å¾®é—œæ³¨ï¼ˆé»ƒè‰²ï¼‰
- **0.3 - 0.5**: ä¸­ç­‰é—œæ³¨ï¼ˆæ©™è‰²ï¼‰
- **0.5 - 1.0**: é«˜åº¦é—œæ³¨ï¼ˆæ·±æ©™è‰²/ç´…è‰²ï¼‰

#### æ­£å¸¸æ¨¡å¼

1. **é›†ä¸­å‹**: æ¬Šé‡é›†ä¸­åœ¨ 2-3 å€‹é—œéµè©
   - é©åˆç°¡å–®å¥å­
   - æƒ…æ„Ÿè©æ˜ç¢º

2. **åˆ†æ•£å‹**: æ¬Šé‡åˆ†æ•£åœ¨å¤šå€‹è©
   - è¤‡é›œå¥å­
   - éœ€è¦ç¶œåˆå¤šå€‹ç·šç´¢

3. **å°ç¨±å‹**: aspect å‰å¾Œè©å‡è¡¡é—œæ³¨
   - ä¿®é£¾è©è±å¯Œ
   - ä¸Šä¸‹æ–‡é‡è¦

#### ç•°å¸¸æ¨¡å¼ï¼ˆéœ€è¦è­¦æƒ•ï¼‰

1. **éåº¦é›†ä¸­**: åªé—œæ³¨ 1 å€‹è©ï¼ˆæ¬Šé‡ > 0.8ï¼‰
   - å¯èƒ½éæ“¬åˆ
   - å¿½ç•¥ä¸Šä¸‹æ–‡

2. **éåº¦åˆ†æ•£**: æ‰€æœ‰è©æ¬Šé‡ç›¸è¿‘ï¼ˆ< 0.2ï¼‰
   - æ³¨æ„åŠ›æ©Ÿåˆ¶å¤±æ•ˆ
   - æ¨¡å‹å›°æƒ‘

3. **éŒ¯èª¤é—œæ³¨**: é—œæ³¨ç„¡é—œè©å½™
   - éœ€è¦æª¢æŸ¥è¨“ç·´æ•¸æ“š
   - å¯èƒ½éœ€è¦èª¿æ•´ dropout

### ğŸ¨ è¦–è¦ºåŒ–é…ç½®

#### é¡è‰²æ–¹æ¡ˆ

```python
# åœ¨ visualization.py ä¸­é…ç½®
color_maps = {
    'word': 'Blues',      # è©ç´šï¼šè—è‰²
    'phrase': 'Greens',   # ç‰‡èªç´šï¼šç¶ è‰²
    'sentence': 'Oranges', # å¥å­ç´šï¼šæ©™è‰²
    'default': 'YlOrRd'   # é»˜èªï¼šé»ƒ-æ©™-ç´…
}
```

#### åœ–è¡¨å¤§å°

```python
# å–®ä¸€æ³¨æ„åŠ›ç†±åœ–
figsize=(14, 3)

# éšå±¤å¼æ³¨æ„åŠ›ï¼ˆ3 å€‹å­åœ–ï¼‰
figsize=(14, 8)

# å¤šå°ºåº¦æ³¨æ„åŠ›ï¼ˆå¤šå€‹å°ºåº¦ï¼‰
figsize=(16, 3 * num_scales)
```

#### DPI è¨­ç½®

```python
# é«˜è³ªé‡è¼¸å‡º
plt.savefig(save_path, dpi=300, bbox_inches='tight')
```

### ğŸ” æ¡ˆä¾‹åˆ†æ

#### æ¡ˆä¾‹ 1: ç°¡å–®æ­£é¢è©•åƒ¹

```
å¥å­: "The food is delicious"
Aspect: "food"
çœŸå¯¦æ¨™ç±¤: æ­£é¢
é æ¸¬æ¨™ç±¤: æ­£é¢

è©ç´šæ³¨æ„åŠ›:
  The: 0.05
  food: 0.30
  is: 0.10
  delicious: 0.55  â† æœ€é«˜æ¬Šé‡

åˆ†æ: æ¨¡å‹æ­£ç¢ºé—œæ³¨æƒ…æ„Ÿè© "delicious"ï¼Œé æ¸¬æº–ç¢º
```

#### æ¡ˆä¾‹ 2: å«å¦å®šçš„è² é¢è©•åƒ¹

```
å¥å­: "The service is not good"
Aspect: "service"
çœŸå¯¦æ¨™ç±¤: è² é¢
é æ¸¬æ¨™ç±¤: è² é¢

ç‰‡èªç´šæ³¨æ„åŠ›:
  The: 0.05
  service: 0.25
  is: 0.10
  not: 0.35  â† å¦å®šè©
  good: 0.25

åˆ†æ: ç‰‡èªç´šæ³¨æ„åŠ›æ­£ç¢ºæ•æ‰ "not good" çµ„åˆï¼Œè­˜åˆ¥å¦å®š
```

#### æ¡ˆä¾‹ 3: è½‰æŠ˜å¥

```
å¥å­: "The food was delicious but the service was terrible"
Aspect: "food"
çœŸå¯¦æ¨™ç±¤: æ­£é¢
é æ¸¬æ¨™ç±¤: æ­£é¢

å¥å­ç´šæ³¨æ„åŠ›:
  food: 0.30
  delicious: 0.40
  but: 0.15  â† è½‰æŠ˜è©
  service: 0.05
  terrible: 0.05

åˆ†æ: å¥å­ç´šæ³¨æ„åŠ›ç†è§£ "but" å‰å¾Œåˆ†åˆ¥è©•åƒ¹ä¸åŒ aspect
```

#### æ¡ˆä¾‹ 4: ä¸­æ€§è©•åƒ¹ï¼ˆå›°é›£ï¼‰

```
å¥å­: "The restaurant has parking"
Aspect: "parking"
çœŸå¯¦æ¨™ç±¤: ä¸­æ€§
é æ¸¬æ¨™ç±¤: ä¸­æ€§

è©ç´šæ³¨æ„åŠ›:
  restaurant: 0.20
  has: 0.15
  parking: 0.35

åˆ†æ: ç„¡æ˜ç¢ºæƒ…æ„Ÿè©ï¼Œæ¨¡å‹é€šéä½ç½®ä¿¡åº¦åˆ¤æ–·ç‚ºä¸­æ€§
```

### ğŸ“Š è¼¸å‡ºæª”æ¡ˆçµæ§‹

è¨“ç·´å®Œæˆå¾Œï¼Œè¦–è¦ºåŒ–æª”æ¡ˆæœƒä¿å­˜åœ¨ä»¥ä¸‹ä½ç½®ï¼š

```
results/visualizations/
â”œâ”€â”€ attention_word_sample_1.png       # æ¨£æœ¬ 1 è©ç´šæ³¨æ„åŠ›
â”œâ”€â”€ attention_word_sample_2.png       # æ¨£æœ¬ 2 è©ç´šæ³¨æ„åŠ›
â”œâ”€â”€ ...
â”œâ”€â”€ attention_phrase_sample_1.png     # æ¨£æœ¬ 1 ç‰‡èªç´šæ³¨æ„åŠ›
â”œâ”€â”€ attention_phrase_sample_2.png     # æ¨£æœ¬ 2 ç‰‡èªç´šæ³¨æ„åŠ›
â”œâ”€â”€ ...
â”œâ”€â”€ attention_sentence_sample_1.png   # æ¨£æœ¬ 1 å¥å­ç´šæ³¨æ„åŠ›
â”œâ”€â”€ attention_sentence_sample_2.png   # æ¨£æœ¬ 2 å¥å­ç´šæ³¨æ„åŠ›
â”œâ”€â”€ ...
â”œâ”€â”€ attention_hierarchical_sample_1.png  # æ¨£æœ¬ 1 å®Œæ•´éšå±¤å¼
â”œâ”€â”€ attention_hierarchical_sample_2.png  # æ¨£æœ¬ 2 å®Œæ•´éšå±¤å¼
â””â”€â”€ ...
```

### ğŸ”¬ ç ”ç©¶ç”¨é€”

#### æ¨¡å‹è§£é‡‹æ€§

- ç†è§£æ¨¡å‹æ±ºç­–éç¨‹
- é©—è­‰æ³¨æ„åŠ›æ©Ÿåˆ¶æœ‰æ•ˆæ€§
- ç™¼ç¾æ¨¡å‹åè¦‹

#### éŒ¯èª¤åˆ†æ

- åˆ†æé æ¸¬éŒ¯èª¤çš„æ¨£æœ¬
- æ‰¾å‡ºæ¨¡å‹å¼±é»
- æŒ‡å°æ¨¡å‹æ”¹é€²

#### è«–æ–‡æ’°å¯«

- æä¾›è¦–è¦ºåŒ–è­‰æ“š
- å±•ç¤ºæ¨¡å‹å„ªå‹¢
- èˆ‡ baseline å°æ¯”

### ğŸ“ æœ€ä½³å¯¦è¸

#### 1. é¸æ“‡ä»£è¡¨æ€§æ¨£æœ¬

- æ­£é¢ã€ä¸­æ€§ã€è² é¢å„é¸å¹¾å€‹
- åŒ…å«ç°¡å–®å’Œè¤‡é›œå¥å­
- åŒ…å«æˆåŠŸå’Œå¤±æ•—æ¡ˆä¾‹

#### 2. å°æ¯”ä¸åŒæ¨¡å‹

```python
# ä¿å­˜ä¸åŒæ¨¡å‹çš„æ³¨æ„åŠ›åœ–åˆ°ä¸åŒç›®éŒ„
visualizer_baseline = AttentionVisualizer('results/baseline/attention')
visualizer_enhanced = AttentionVisualizer('results/enhanced/attention')
```

#### 3. è¨˜éŒ„è§€å¯Ÿçµæœ

åœ¨è¨“ç·´å ±å‘Šä¸­æ·»åŠ æ³¨æ„åŠ›åˆ†æï¼š
```
ã€æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æã€‘
1. è©ç´šæ³¨æ„åŠ›: ä¸»è¦é—œæ³¨æƒ…æ„Ÿè©å’Œ aspect
2. ç‰‡èªç´šæ³¨æ„åŠ›: èƒ½è­˜åˆ¥å¦å®šå’Œä¿®é£¾çµæ§‹
3. å¥å­ç´šæ³¨æ„åŠ›: ç†è§£å…¨å±€èªå¢ƒå’Œè½‰æŠ˜é—œä¿‚
```

---

**æ³¨æ„åŠ›è¦–è¦ºåŒ–æ˜¯ç†è§£å’Œæ”¹é€²æ¨¡å‹çš„é‡è¦å·¥å…·ï¼** ğŸ¨
