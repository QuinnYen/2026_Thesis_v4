# HPNet (2021) - è«–æ–‡å¿«é€Ÿåƒè€ƒ

**å®Œæ•´æ¨™é¡Œ**: A hierarchical and parallel framework for End-to-End Aspect-based Sentiment Analysis

**ä½œè€…**: Ding Xiao, Feiyang Ren, Xiaoxuan Pang, Ming Cai, et al.

**ç™¼è¡¨**: Neurocomputing 465 (2021) 549-560

**æ ¸å¿ƒæ©Ÿæ§‹**: Zhejiang University

---

## ğŸ“‹ è«–æ–‡æ¦‚è¿°

### ç ”ç©¶å•é¡Œ
**End-to-End Aspect-Based Sentiment Analysis (E2E-ABSA)** åŒ…å«å…©å€‹å­ä»»å‹™ï¼š
1. **Aspect Extraction (AE)**: è­˜åˆ¥è©•åƒ¹å°è±¡ (aspect terms)
2. **Polarity Classification (PC)**: é æ¸¬æƒ…æ„Ÿæ¥µæ€§ (positive/negative/neutral)

### ç ”ç©¶å‹•æ©Ÿ
éå¾€ç ”ç©¶ç™¼ç¾ joint models çš„æ€§èƒ½ä¸€ç›´ä¸å¦‚ pipeline å’Œ collapsed modelsã€‚HPNet è¦æ¢ç´¢ joint model çš„æ½›åŠ›ã€‚

### æ ¸å¿ƒè§€å¯Ÿ
1. AEå’ŒPCåˆ†åˆ¥å±¬æ–¼ **syntactic task** å’Œ **semantic task**ï¼Œæ‡‰è©²éƒ¨ç½²åœ¨ä¸åŒçš„ç¥ç¶“ç¶²è·¯å±¤
2. æ·±åº¦ç¥ç¶“ç¶²è·¯çš„ä¸åŒå±¤æœ‰ä¸åŒçš„èªè¨€å­¸è¡¨ç¤ºèƒ½åŠ›
3. ç¾æœ‰æ–¹æ³•åªç”¨æœ€å¾Œä¸€å±¤ï¼Œå¿½ç•¥äº† BERT çš„éšå±¤ç‰¹æ€§

---

## ğŸ¯ æ ¸å¿ƒæ–¹æ³•

### ä»»å‹™å®šç¾©

**è¼¸å…¥**: å¥å­ S = [sâ‚, sâ‚‚, ..., sâ‚™]

**è¼¸å‡º**:
- Aspect labels: E = [eâ‚, eâ‚‚, ..., eâ‚™], where eâ‚œ âˆˆ {B, I, O}
- Polarity labels: P = [pâ‚, pâ‚‚, ..., pâ‚™], where pâ‚œ âˆˆ {pos, neg, neu}

**ç¯„ä¾‹**:
```
Sentence: "Great food but the service was dreadful!"
Aspects:  "food" (positive), "service" (negative)
```

### å…©ç¨®æ¨¡å‹æ¶æ§‹

#### 1. HPNet-S (Specific-Layer Joint Model)

**æ ¸å¿ƒæ€æƒ³**: æ‰‹å‹•ç‚ºå…©å€‹å­ä»»å‹™é¸æ“‡ç‰¹å®šå±¤

**æ¶æ§‹**:
```
BERT Backbone Network
â”œâ”€â”€ Layer t (ä¸­é–“å±¤) â†’ CRF Layer â†’ Aspect Extraction
â””â”€â”€ Layer l (æœ€å¾Œå±¤) â†’ Classification Layer â†’ Polarity Classification
```

**å±¤é¸æ“‡ç­–ç•¥**:
- **Aspect Extraction**: ä½¿ç”¨ä¸­é–“å±¤ (å¦‚ Layer 9)
  - ç†ç”±: Syntactic information åœ¨ä¸­é–“å±¤æœ€æ˜é¡¯
- **Polarity Classification**: ä½¿ç”¨æœ€å¾Œå±¤ (Layer 12)
  - ç†ç”±: Semantic information åœ¨é«˜å±¤èšåˆ

**æ•¸å­¸è¡¨ç¤º**:
```python
# Aspect Extraction (ä½¿ç”¨ Layer t)
P = hâ‚œWâ‚œ + bâ‚œ                    # CRF scores
lossâ‚â‚›â‚š = -log p(Y|X)            # CRF loss

# Polarity Classification (ä½¿ç”¨ Layer l)
U = softmax(hâ‚—Wâ‚› + bâ‚›)           # Sentiment scores
lossâ‚›â‚‘â‚™ = -Î£Î£ uÂ·log(Ã»)           # Cross-entropy loss

# ç¸½æå¤±
loss = lossâ‚â‚›â‚š + lossâ‚›â‚‘â‚™ + Î»||Î¸||Â²
```

#### 2. HPNet-M (Multiple-Layer Joint Model)

**æ ¸å¿ƒæ€æƒ³**: å‹•æ…‹å­¸ç¿’æ‰€æœ‰å±¤çš„æ¬Šé‡ï¼Œç‚ºå…©å€‹ä»»å‹™åˆ†åˆ¥çµ„åˆ

**æ¶æ§‹**:
```
BERT Backbone Network (All Layers)
â”œâ”€â”€ Weighted Combination â†’ CRF Layer â†’ Aspect Extraction
â”‚   Mâ‚ = câ‚ Â· Î£(wâ‚áµ¢ Â· háµ¢)
â”‚
â””â”€â”€ Weighted Combination â†’ Classification Layer â†’ Polarity Classification
    Mâ‚› = câ‚› Â· Î£(wâ‚›áµ¢ Â· háµ¢)
```

**æ•¸å­¸è¡¨ç¤º**:
```python
# Aspect Extraction çš„å¤šå±¤çµ„åˆ
Mâ‚ = câ‚ Â· Î£áµ¢â‚Œâ‚Ë¡ (wâ‚áµ¢ Â· háµ¢)
where: wâ‚ = [wâ‚â‚, wâ‚â‚‚, ..., wâ‚â‚—] (softmax-normalized)
       câ‚: trainable scalar (initialized to 1)

# Polarity Classification çš„å¤šå±¤çµ„åˆ  
Mâ‚› = câ‚› Â· Î£áµ¢â‚Œâ‚Ë¡ (wâ‚›áµ¢ Â· háµ¢)
where: wâ‚› = [wâ‚›â‚, wâ‚›â‚‚, ..., wâ‚›â‚—] (softmax-normalized)
       câ‚›: trainable scalar (initialized to 1)
```

**é—œéµç‰¹é»**:
- æ¯å€‹å­ä»»å‹™æœ‰ç¨ç«‹çš„æ¬Šé‡é›† (wâ‚ å’Œ wâ‚›)
- æ¬Šé‡é€šéåå‘å‚³æ’­è‡ªå‹•å­¸ç¿’
- å— ELMo (Peters et al. 2018) å•Ÿç™¼

### å‰µæ–°é»

#### 1. éšå±¤å¼çµæ§‹ (Hierarchical Structure)
- åˆ©ç”¨ BERT çš„éšå±¤ç‰¹æ€§
- ç‚ºä¸åŒä»»å‹™é¸æ“‡åˆé©çš„å±¤ç´š
- åŸºæ–¼èªè¨€å­¸è­‰æ“š (Jawahar, Tenney, Hewitt ç­‰ç ”ç©¶)

#### 2. å¹³è¡ŒåŸ·è¡Œ (Parallel Execution)
- è¨“ç·´å’Œæ¨ç†éƒ½å¹³è¡ŒåŸ·è¡Œå…©å€‹å­ä»»å‹™
- **é—œéµæŠ€å·§**: è®“æ¨¡å‹é æ¸¬æ¯å€‹è©çš„sentimentï¼Œè€Œéåªé æ¸¬aspect termsçš„sentiment
- è§£æ±º target-polarity mismatch å•é¡Œ
- æå‡æ¨ç†ååé‡

#### 3. è¯åˆå­¸ç¿’ (Joint Learning)
- å…±äº«åŒä¸€å€‹ BERT backbone
- ç¢ºä¿å…©å€‹å­ä»»å‹™çš„é—œè¯æ€§å’Œå…±æ€§
- é¿å… pipeline çš„èª¤å·®å‚³æ’­

---

## ğŸ“Š æ•¸æ“šé›†

### 1. Restaurant Dataset
- **ä¾†æº**: SemEval 2014, 2015, 2016 restaurant domain çš„è¯é›†
- **è¦æ¨¡**:
  - Train: 3,452 å¥
  - Test: 973 å¥
  - Aspects: 4,821 (train) + 1,351 (test)

### 2. Laptop Dataset
- **ä¾†æº**: SemEval 2014 Task 4
- **è¦æ¨¡**:
  - Train: 2,163 å¥
  - Test: 638 å¥
  - Aspects: 2,041 (train) + 654 (test)

### 3. Twitter Dataset
- **ä¾†æº**: Mitchell et al.
- **è¦æ¨¡**: 6,940 å¥
- **ç‰¹é»**: ç„¡ train-test splitï¼Œä½¿ç”¨ 10-fold cross-validation

---

## ğŸ”§ å¯¦é©—è¨­ç½®

### æ¨¡å‹åƒæ•¸

| åƒæ•¸ | è¨­å®š |
|------|------|
| BERT æ¨¡å‹ | BERT-base (12 layers, 768 dim) |
| æœ€å¤§å¥é•· | 80 words |
| Batch Size | 32 |
| åˆå§‹åŒ– | xavier uniform |
| L2 æ­£å‰‡åŒ– | Î» = 0.01 |
| Dropout | 0.1 |
| å­¸ç¿’ç‡ (ä¸€èˆ¬) | 2e-5 |
| å­¸ç¿’ç‡ (æ¬Šé‡) | 5e-3 (HPNet-M çš„ câ‚, câ‚›, wâ‚, wâ‚›) |
| Epochs | 5 |

### Baseline æ¨¡å‹

**Joint Models**:
- CMLA+ (Wang et al. 2017)
- MTL-E2E (Li et al. 2019)

**Collapsed Models**:
- MATEPC (He et al. 2019)
- MNN (Li & Lu 2019)
- BERT-GLCLD (Li et al. 2020)

**Pipeline Models**:
- BERT-PT (Xu et al. 2019)

---

## ğŸ“ˆ å¯¦é©—æˆæœ

### ä¸»è¦çµæœ (E2E-ABSA)

#### Restaurant Dataset

| Model | F1 Score |
|-------|----------|
| CMLA+ | 39.18% |
| MTL-E2E | 64.44% |
| MATEPC | 63.13% |
| MNN | 70.98% |
| BERT-PT | 71.47% |
| BERT-GLCLD | 72.16% |
| **HPNet-S(9,12)** | **73.23%** |
| **HPNet-M** | **73.28%** â­ |

#### Laptop Dataset

| Model | F1 Score |
|-------|----------|
| CMLA+ | 30.09% |
| MTL-E2E | 55.59% |
| MATEPC | 47.99% |
| MNN | 58.90% |
| BERT-PT | 56.90% |
| BERT-GLCLD | 57.27% |
| **HPNet-S(9,12)** | **59.25%** |
| **HPNet-M** | **59.33%** â­ |

#### Twitter Dataset (10-fold CV)

| Model | F1 Score |
|-------|----------|
| CMLA+ | 40.14% |
| MTL-E2E | 52.48% |
| MATEPC | 50.74% |
| MNN | 55.97% |
| BERT-PT | 57.77% |
| **HPNet-S(9,12)** | **58.97%** |
| **HPNet-M** | **59.21%** â­ |

### å–®ä»»å‹™æ€§èƒ½

#### Aspect Extraction (AE)

| Model | Restaurant | Laptop |
|-------|-----------|--------|
| MTL-E2E | 83.36% | 78.57% |
| CNN + WIN | 88.21% | 83.27% |
| BERT-GLCLD | **91.14%** | 77.42% |
| BAT | 81.50% | 85.57% |
| HPNet-S(9,12) | 88.69% | 84.49% |
| **HPNet-M** | 87.65% | **86.13%** â­ |

#### Polarity Classification (PC)

| Model | Restaurant | Laptop |
|-------|-----------|--------|
| BiGCN | 73.48% | 71.84% |
| G-ATT-U | 72.65% | 72.23% |
| MAN | 71.31% | 73.20% |
| BAT | 79.24% | 76.50% |
| HPNet-S(9,12) | 79.04% | 72.67% |
| **HPNet-M** | **79.34%** â­ | **76.65%** â­ |

### ä½¿ç”¨ BERT-large çš„çµæœ

| Model | Restaurant | Laptop | Twitter |
|-------|-----------|--------|---------|
| HPNet-S (base) | 73.23% | 59.25% | 58.97% |
| HPNet-S (large) | 74.45% (+1.22%) | 60.55% (+1.30%) | 59.44% (+0.47%) |
| HPNet-M (base) | 73.28% | 59.33% | 59.21% |
| HPNet-M (large) | 74.61% (+1.33%) | 60.53% (+1.20%) | 59.52% (+0.31%) |

---

## ğŸ“Š è©•ä¼°æŒ‡æ¨™

### ä¸»è¦æŒ‡æ¨™

**F1 Score (Macro)**:
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

Macro-F1 = (F1_pos + F1_neg + F1_neu) / 3
```

### E2E-ABSA è©•ä¼°æ¨™æº–

**å®Œå…¨åŒ¹é… (Exact Match)**:
- Aspect boundary å¿…é ˆå®Œå…¨æ­£ç¢º (B, I, O tags)
- Sentiment polarity å¿…é ˆå®Œå…¨æ­£ç¢º
- åªæœ‰å…©è€…éƒ½å°æ‰ç®—æ­£ç¢º

**ç¯„ä¾‹**:
```
Gold:  "Great [food]_pos but the [service]_neg was dreadful"
Pred:  "Great [food]_pos but the [service]_neg was dreadful"
â†’ å®Œå…¨æ­£ç¢º âœ…

Pred:  "Great [food]_neg but the [service]_neg was dreadful"  
â†’ sentiment éŒ¯èª¤ âŒ

Pred:  "Great [food and]_pos but the [service]_neg was dreadful"
â†’ boundary éŒ¯èª¤ âŒ
```

---

## ğŸ” æ·±å…¥åˆ†æ

### 1. HPNet-S çš„å±¤é¸æ“‡å¯¦é©—

**Restaurant Dataset**:

| AE Layer | PC Layer | F1 Score |
|----------|----------|----------|
| 6 | 12 | 72.14% |
| 9 | 12 | **73.23%** â­ |
| 12 | 12 | 71.66% |

**çµè«–**: Layer 9 (syntactic) + Layer 12 (semantic) çµ„åˆæœ€ä½³

### 2. HPNet-M çš„æ¬Šé‡åˆ†å¸ƒå¯è¦–åŒ–

**Restaurant Dataset**:
- **AE weights**: Layer 9 æ¬Šé‡æœ€é«˜
- **PC weights**: Layer 12 æ¬Šé‡æœ€é«˜
- èˆ‡ HPNet-S çš„æœ€ä½³é…ç½®ä¸€è‡´ âœ…

**Laptop Dataset**:
- **AE weights**: Layer 9 æ¬Šé‡æœ€é«˜
- **PC weights**: Layer 8 æ¬Šé‡æœ€é«˜
- é¡¯ç¤º PC å¯èƒ½åœ¨ AE ä¹‹å‰å­¸å¥½

**Twitter Dataset**:
- **AE weights**: Layer 6, 9 æ¬Šé‡é«˜
- **PC weights**: åˆ†æ•£åœ¨æ‰€æœ‰å±¤
- ç¬¦åˆ Tenney et al. "semantics spread across entire model"

### 3. Attention æ¬Šé‡å¯è¦–åŒ–

**è§€å¯Ÿ (ä»¥ "Great food but the service was dreadful!" ç‚ºä¾‹)**:

- **Layer 1**: éš¨æ©Ÿä¸”å‡å‹»åˆ†å¸ƒ
- **Layer 9**: 
  - Attention é›†ä¸­åœ¨ [SEP] token
  - ç¬¦åˆ "vertical pattern" (Kovaleva et al.)
  - è¡¨ç¤ºå­¸ç¿’ syntactic information
- **Layer 10**: 
  - "food" é—œæ³¨ "great"
  - "service" é—œæ³¨ "dreadful"
  - é–‹å§‹è¿½è¹¤ semantic relations
- **Layer 12**: 
  - é—œæ³¨æ¨™é»ç¬¦è™Ÿå’Œ [SEP]
  - å·²å®Œæˆæ‰€æœ‰ syntactic/semantic è™•ç†

### 4. Ablation Study

**ç§»é™¤ Joint Training çš„å½±éŸ¿**:

| Model | AE F1 | PC F1 | E2E F1 |
|-------|-------|-------|--------|
| HPNet-M (Full) | 87.65% | 79.34% | 73.28% |
| HPNet-M (-AE Joint) | - | 78.69% | - |
| HPNet-M (-PC Joint) | 86.98% | - | - |

**çµè«–**: Joint training å°å…©å€‹å­ä»»å‹™éƒ½æœ‰å¹«åŠ©

---

## ğŸ’¡ é—œéµæ´å¯Ÿ

### 1. BERT å±¤ç´šçš„èªè¨€å­¸ç‰¹æ€§

| å±¤ç´š | ç‰¹æ€§ | ä»»å‹™é©åˆåº¦ |
|------|------|-----------|
| **Lower (1-4)** | Linear word order | - |
| **Middle (5-9)** | **Syntactic info** | **Aspect Extraction** â­ |
| **Higher (10-12)** | **Semantic info** | **Polarity Classification** â­ |

### 2. Joint Model çš„å„ªå‹¢

âœ… **å„ªé»**:
- å…±äº«è¡¨ç¤ºï¼Œæ¸›å°‘åƒæ•¸
- å…©å€‹ä»»å‹™äº’ç›¸ä¿ƒé€²
- é¿å… pipeline çš„èª¤å·®å‚³æ’­

âœ… **HPNet çš„æ”¹é€²**:
- å¹³è¡ŒåŸ·è¡Œ â†’ æå‡ååé‡
- éšå±¤å¼è¨­è¨ˆ â†’ ä»»å‹™ç‰¹å®šå±¤é¸æ“‡
- è§£æ±º target-polarity mismatch

### 3. ç‚ºä»€éº¼ HPNet-M å„ªæ–¼ HPNet-S?

1. **æ›´éˆæ´»**: è‡ªå‹•å­¸ç¿’æ¬Šé‡ vs. æ‰‹å‹•é¸å±¤
2. **æ›´å…¨é¢**: åˆ©ç”¨æ‰€æœ‰å±¤ vs. åªç”¨ç‰¹å®šå±¤
3. **æ›´æ³›åŒ–**: ä¸åŒæ•¸æ“šé›†è‡ªå‹•é©æ‡‰
4. **ä½†ä»£åƒ¹**: æ›´å¤šåƒæ•¸ (æ¯å€‹ä»»å‹™ 12 å€‹æ¬Šé‡ + 2 å€‹ scalar)

---

## ğŸ”„ èˆ‡ä½ çš„ç ”ç©¶å°æ¯”

### ç›¸ä¼¼ä¹‹è™•

| ç‰¹æ€§ | HPNet | ä½ çš„ç ”ç©¶ |
|------|-------|---------|
| åˆ©ç”¨ BERT éšå±¤ç‰¹å¾µ | âœ… | âœ… |
| èªè­˜å±¤ç´šèªç¾©å·®ç•° | âœ… | âœ… |
| å¼•ç”¨ç›¸åŒç†è«–åŸºç¤ | Jawahar, Tenney | Jawahar, Tenney |
| æœ‰æ¬Šé‡å­¸ç¿’æ©Ÿåˆ¶ | HPNet-M | HBL (å·²æ”¾æ£„) |

### é—œéµå·®ç•°

| ç¶­åº¦ | HPNet | ä½ çš„ç ”ç©¶ |
|------|-------|---------|
| **ä»»å‹™** | E2E-ABSA (AE + PC) | Aspect-Level Classification |
| **å­ä»»å‹™æ•¸** | 2 å€‹ | 1 å€‹ |
| **Aspect ä¾†æº** | æ¨¡å‹é æ¸¬ | å·²çŸ¥/çµ¦å®š |
| **å±¤ç´šè¨­è¨ˆ** | Task-specific | Unified semantic hierarchy |
| **ç ”ç©¶é‡é»** | Parallel execution | **Fusion strategies** â­ |
| **æ‡‰ç”¨å ´æ™¯** | æ··åˆå ´æ™¯ | **100% å¤šé¢å‘** â­ |

### ä½ çš„ç¨ç‰¹è²¢ç» â­

1. âœ… **ç³»çµ±æ€§èåˆç­–ç•¥æ¯”è¼ƒ** (4ç¨®æ–¹æ³•)
   - Concatenation
   - Weighted Average
   - Gated Fusion
   - Multi-head Fusion

2. âœ… **çµ±ä¸€çš„èªç¾©å±¤ç´šåŠƒåˆ†**
   - Low/Mid/High = è©æ³•/èªç¾©/ä»»å‹™

3. âœ… **å¤šé¢å‘å ´æ™¯å°ˆé–€å„ªåŒ–**
   - MAMS: 100% å¤šé¢å‘

4. âœ… **æ·±å…¥åˆ†æ**
   - Ablation study (å–®å±¤ç´šè²¢ç»)
   - å±¤ç´šå°ä¸åŒæƒ…æ„Ÿé¡åˆ¥çš„å½±éŸ¿

---

## ğŸ“š åƒè€ƒæ–‡ç»å¼•ç”¨æ ¼å¼

```bibtex
@article{xiao2021hpnet,
  title={A hierarchical and parallel framework for End-to-End Aspect-based Sentiment Analysis},
  author={Xiao, Ding and Ren, Feiyang and Pang, Xiaoxuan and Cai, Ming and Wang, Qianyu and He, Ming and Peng, Jiawei and Fu, Hao},
  journal={Neurocomputing},
  volume={465},
  pages={549--560},
  year={2021},
  publisher={Elsevier}
}
```

---

## ğŸ“ è«–æ–‡æ’°å¯«æ™‚å¦‚ä½•å¼•ç”¨

### Related Work æ®µè½ç¯„ä¾‹

```markdown
Chen et al. (2021) æå‡º HPNetï¼Œç‚º End-to-End ABSA è¨­è¨ˆäº†éšå±¤å¼æ¡†æ¶ã€‚
HPNet åŒæ™‚è™•ç† aspect extraction å’Œ sentiment classification å…©å€‹å­ä»»å‹™ï¼Œ
ä¸¦ç‚ºé€™å…©å€‹ä»»å‹™åˆ†åˆ¥å­¸ç¿’å¯è¨“ç·´çš„å±¤ç´šæ¬Šé‡ã€‚ä»–å€‘çš„ HPNet-M æ¨¡å‹è­‰æ˜äº†
BERT ä¸åŒå±¤å° syntactic å’Œ semantic ä»»å‹™çš„ä¸åŒè²¢ç»ã€‚

æˆ‘å€‘çš„å·¥ä½œèˆ‡ HPNet çš„é—œéµå€åˆ¥åœ¨æ–¼ï¼š(1) ä»»å‹™ç¯„åœä¸åŒï¼Œæˆ‘å€‘å°ˆæ³¨æ–¼
aspect-level sentiment classification (aspect å·²çŸ¥)ï¼Œè€Œ HPNet æ˜¯è¯åˆ
å­¸ç¿’æ¡†æ¶ï¼›(2) ç ”ç©¶é‡é»ä¸åŒï¼Œæˆ‘å€‘ç³»çµ±æ€§æ¯”è¼ƒäº† 4 ç¨®èåˆç­–ç•¥ï¼Œè€Œ HPNet
ä¸»è¦æ¢ç´¢ joint model çš„å¹³è¡ŒåŸ·è¡Œæ©Ÿåˆ¶ï¼›(3) å±¤ç´šè¨­è¨ˆå“²å­¸ä¸åŒï¼Œæˆ‘å€‘æå‡º
çµ±ä¸€çš„èªç¾©å±¤ç´šåŠƒåˆ†ï¼Œè€Œ HPNet æ¡ç”¨ task-specific å±¤é¸æ“‡ã€‚
```

---

## âœ… å¿«é€Ÿç¸½çµ

| é …ç›® | å…§å®¹ |
|------|------|
| **ä»»å‹™** | End-to-End ABSA (Aspect Extraction + Polarity Classification) |
| **æ ¸å¿ƒæ–¹æ³•** | HPNet-S (æ‰‹å‹•é¸å±¤) + HPNet-M (å­¸ç¿’æ¬Šé‡) |
| **é—œéµå‰µæ–°** | éšå±¤å¼è¨­è¨ˆ + å¹³è¡ŒåŸ·è¡Œ + è¯åˆå­¸ç¿’ |
| **æ•¸æ“šé›†** | Restaurant, Laptop, Twitter |
| **æœ€ä½³çµæœ** | Restaurant: 73.28%, Laptop: 59.33%, Twitter: 59.21% |
| **è©•ä¼°æŒ‡æ¨™** | Macro F1 Score (exact match) |
| **ç™¼è¡¨å¹´ä»½** | 2021 |
| **å½±éŸ¿åŠ›** | é¦–å€‹åœ¨ E2E-ABSA ä¸Šè¶…è¶Š pipeline/collapsed çš„ joint model |

---

**æ–‡æª”å»ºç«‹æ™‚é–“**: 2025-11-21
**æœ€å¾Œæ›´æ–°**: 2025-11-21
