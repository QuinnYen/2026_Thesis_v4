中原大學
資訊管理學系
碩士學位論文
階層式組合網絡
用於面向級情感分析
```
Hierarchical Composition Network (HMAC-Net)
```
for Aspect-Level Sentiment Analysis
指導教授：洪智力 博士
研 究 生：顏 寬 學生
中華民國 114 年 11 月
I
摘要
```
面向級情感分析(Aspect-Based Sentiment Analysis， ABSA)旨在識別文本中針
```
對特定評價面向的情感極性。然而，當句子包含多個評價面向時，現有方法面臨三個
關鍵挑戰:缺乏階層式語義建模機制、缺乏選擇性組合策略，以及缺乏顯式的面向間關
係建模。
```
提出階層式多面向組合網路(Hierarchical Multi-Aspect Composition Network，
```
```
HMAC-Net)，透過三個創新模組系統性地解決上述挑戰。首先，面向感知階層式注意力
```
```
模組(AAHA)在詞級、片語級與句子級建立面向感知的注意力機制，並透過動態層級融
```
```
合整合多層次資訊。其次，漸進式多面向組合模組(PMAC)採用可學習的門控網路實現
```
選擇性組合，使語義相關的面向能夠有效融合而語義獨立的面向保持獨立性。第三，
```
面向間關係建模模組(IARM)採用 Transformer 架構顯式建模面向間的高階交互關係，透
```
過多頭自注意力機制捕捉複雜依賴模式。
本研究在 SemEval-2014 Restaurant 與 Laptop 資料集上進行系統性實驗驗證。實驗
結果顯示，HMAC-Net 相較於現有主流方法取得顯著效能提升。消融實驗驗證 PMAC 與
IARM 模組對整體效能均有實質貢獻。本研究的主要貢獻在於提出整合式架構，系統性
地解決階層式語義建模、選擇性組合與關係建模等關鍵問題，為多面向情感分析領域
提供新的理論視角與技術方案。
關鍵詞：面向級情感分析、階層式注意力機制、多面向組合、面向間關係建模
II
Abstract
```
Aspect-Based Sentiment Analysis (ABSA) identifies sentiment polarity toward specific aspects
```
in text. However, existing methods face three key challenges when handling multiple aspects:
lack of hierarchical semantic modeling, absence of selective composition strategies, and
insufficient explicit inter-aspect relation modeling.
```
This research proposes the Hierarchical Multi-Aspect Composition Network (HMAC-Net) with
```
```
three innovative modules. First, the Aspect-Aware Hierarchical Attention (AAHA) module
```
establishes aspect-aware attention at word, phrase, and sentence levels. Second, the
```
Progressive Multi-Aspect Composition (PMAC) module employs learnable gating networks
```
for selective composition, enabling effective fusion of semantically related aspects while
maintaining independence of unrelated aspects. Third, the Inter-Aspect Relation Modeling
```
(IARM) module adopts a Transformer-based architecture to explicitly model high-order inter-
```
aspect interactions.
Systematic experiments on SemEval-2014 Restaurant and Laptop datasets demonstrate that
HMAC-Net achieves significant performance improvements over existing methods. Ablation
studies verify substantial contributions from both PMAC and IARM modules. This research
provides an integrated architecture addressing hierarchical semantic modeling, selective
composition, and relation modeling for multi-aspect sentiment analysis.
Keywords：Aspect-Based Sentiment Analysis, Hierarchical Attention, Multi-Aspect
Composition, Inter-Aspect Relations
III
致謝
回在這段漫長而充實的研究旅程中，幸得諸多師長、同儕與親友的鼎力相助，使
本論文得以順利完成。
首先，我要向我的指導教授洪智力老師表達最誠摯的感謝。洪老師在我研究過程中給
予的悉心指導、專業建議與無限耐心，是這篇論文得以成形的關鍵。每次的討論都充
滿啟發，每一個疑惑都獲得解答。他不僅在學術上引領我探索情感分析的奧妙，更在
研究態度與方法論上為我樹立了典範。
我要特別感謝資管系提供的優質學習環境與研究資源。系上完善的設備、豐富的
學術資料庫，為我的研究奠定了堅實的基礎。系辦的行政人員們總是耐心解答各種問
題，讓我能夠專注於研究而無後顧之憂。
感謝所有在課堂上傳授知識的老師們，您們淵博的學識與獨到的見解拓展了我的
視野，讓我得以在跨領域情感分析的研究中融合多元觀點。特別是在機器學習、自然
語言處理與資料科學領域的課程，為我的研究提供了不可或缺的理論支撐。
最後，我要向我的家人表達最深的感謝。感謝您們始終如一的支持與理解，在我
面臨挫折時給予的鼓勵，以及在我熬夜趕稿時的體貼關懷。您們是我堅持不懈的動力
源泉，沒有您們的支持，我無法全心投入這段研究歷程。
這篇論文凝聚了許多人的心血與智慧，雖然篇幅有限，無法一一列舉所有曾給予
我幫助的人，但您們的恩情將永遠銘記於心。在此，向所有曾經幫助過我的人致上最
真摯的謝意。
顏 寬 謹誌
於 中原大學資訊管理學系研究所
中華民國 一百一十五 年 七 月
IV
目錄
摘要 ............................................................................................................................. ..... I
Abstract .................................................................................................................................. II
致謝 ............................................................................................................................. ..... III
目錄 ............................................................................................... ................................... IV
圖目錄 .................................................................................................................................. VI
表目錄 .................................................................................................................................. VII
第一章 緒論 ........................................................................................................................ 00
1.1 研究背景與動機 ......................................................................................... 00
1.2 研究問題 ...................................................................................... 00
1.3 研究目的 ...................................................................................... 00
1.4 論文架構 .............................................................................................................. 00
第二章 文獻探討................................................................................................................... 00
2.1 階層式注意力機制於情感分析之應用 ................................................... 00
2.2 多面向特徵組合與融合方法 .................................................................... 00
2.3 面向間關係建模技術 .............................................................................. 00
2.4 面向級情感分析基準模型 ..................................................................... 00
第三章 研究方法 ................................................................................................... 00
3.1 研究問題定義 ................................................................................................... 00
3.2 HMAC-Net 整體架構 ........................................................................ 00
3.3 AAHA 模組：階層式面向感知注意力 .................................................... 00
3.4 PMAC 模組：漸進式多面向組合 ............................................................. 00
3.5 IARM 模組：面向間關係建模 ................................................................... 00
3.6 模型訓練與優化 .............................................................................................. 00
第四章 實驗設計 ................................................................................................... 00
4.1 實驗資料集 ............................................................................................ 00
4.2 評估指標 ............................................................................................ 00
4.3 基準模型 ............................................................................................ 00
4.4 實驗環境與參數設定 ............................................................................................ 00
V
第五章 實驗結果 ................................................................................................... 00
5.1 整體效能比較 ............................................................................................ 00
5.2 消融實驗分析 ............................................................................................ 00
5.3 PMAC 門控機制深入分析 ..................................................................... 00
5.4 注意力機制視覺化分析 ..................................................................................... 00
5.5 錯誤分析與案例研究 ............................................................................................ 00
5.6 討 論 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0 0
第六章 結論與未來研究方向 ..................................................................................... 00
6.1 研究貢獻總結 ............................................................................................ 00
6.2 研究限制 ............................................................................................ 00
6.3 未來研究方向 ............................................................................................ 00
參考文獻 ................................................................................ ................... 00
附錄 A：GAI 工具接露 ................................................................................................. 00
VI
圖目錄
===
表目錄
===
1
第一章 緒論
1.1 研究背景與動機
1.1.1 情感分析的發展與應用價值
隨著資訊科技與社交媒體平台的蓬勃發展，使用者在網路上產生的評論、意見與
情緒表達呈現指數級增長。企業組織、政府機構與研究人員對於從這些大量非結構化
文本中擷取公眾情感與態度的需求日益迫切。情感分析作為自然語言處理領域的重要
分支，已成為將使用者生成內容轉化為有價值商業洞察的關鍵技術。根據 Rodríguez-
```
Ibáñez 等人(2023)的研究指出，從 2008 年至 2022 年間，包含社交網絡情感分析主題
```
的學術論文數量以每年 34%的幾何成長率增加，顯示該領域受到學術界與產業界的高度
重視。該研究進一步統計，在僅僅五年內便有超過 8000 項相關專利獲得核准，同時在
十五年間累積發表超過 2300 篇學術論文，充分反映情感分析技術在實務應用上的廣泛
需求與商業價值。
情感分析技術的應用範疇橫跨多個重要領域，展現其顯著的實務價值。在金融市
場領域，投資機構透過分析社交媒體與新聞報導中的情感傾向，預測股票價格走勢與
市場趨勢，協助制定投資決策。在客戶關係管理方面，企業系統性地分析客戶評論與
反饋意見，即時掌握顧客滿意度變化，快速回應負面評價，提升服務品質與客戶忠誠
度。品牌行銷團隊運用情感分析技術監測品牌形象與產品口碑，評估行銷活動成效，
調整傳播策略以優化品牌聲量。政治領域中，情感分析被用於監測公眾輿論動向，預
測選舉結果，評估政策方案的民意支持度。產品開發團隊則從使用者評論中萃取改進
建議，識別產品缺陷與功能需求，指導下一代產品設計。這些多元化的應用場景均仰
賴準確理解使用者對特定主題或產品的情感傾向，以支援關鍵決策制定與策略規劃。
1.1.2 從句子級到面向級情感分析的演進
傳統的句子級情感分析方法在處理真實世界的複雜評論時面臨顯著局限性。句子
級情感分析將整個句子或文章視為單一分析單元，僅能產生一個整體性的情感極性判
斷，無法捕捉文本中針對不同面向或屬性的細緻情感差異。在實務應用中，使用者的
評論往往包含對同一產品或服務的多個面向的評價，且這些面向可能承載截然不同的
情感態度。以餐廳評論為例，一則評論可能同時表達「這家餐廳的食物品質優異，但
服務態度令人失望」，其中對於「食物」面向持正面情感，而對「服務」面向則呈現負
面情感。傳統句子級方法往往將此類混合情感的評論歸類為中性或產生錯誤判斷，導
致企業無法獲得有價值的細粒度分析結果，難以針對具體面向制定改進措施。
```
為解決此一根本性問題，面向級情感分析(Aspect-Based Sentiment Analysis，
```
```
ABSA)應運而生，代表情感分析技術從粗粒度判斷向細粒度分析的重要演進。ABSA 技術
```
針對文本中的特定面向或屬性進行獨立的情感分類，能夠識別並分析使用者對不同面
```
向的情感態度，從而提供更精確且具實務價值的分析結果。根據 Tiwari 等人(2023)的
```
系統性文獻回顧，ABSA 可依據應用需求在不同層級進行分析，包括面向級、句子級與
2
文章級，其中面向級分析被視為特徵級情感分析，能夠從評論文本中提取多個特徵並
針對特定領域進行深入的情境理解。ABSA 任務主要依賴於文本評論的句法特徵，透過
識別面向詞彙及其相關的情感描述詞，建立面向與情感之間的精確對應關係。此技術
使得企業能夠針對產品或服務的各個具體面向獲得清晰的使用者反饋，進而制定更有
針對性的改進策略，顯著提升情感分析結果對於實務決策的支援價值。
1.1.3 多面向情感分析的技術挑戰
儘管 ABSA 技術在單一面向的情感分析上已取得顯著進展，但當句子中同時存在多
個評價面向時，技術挑戰變得更加複雜且具挑戰性。在真實應用場景中，使用者評論
往往包含對產品或服務多個面向的意見表達，這些面向之間可能存在複雜的語義關聯
與依賴關係，使得多面向情感分析成為一個需要深入探討的研究問題。根據 Zhang 等
```
人(2022)的系統性研究指出，與僅關注單一情感的傳統 ABSA 任務不同，近年來許多研
```
究開始聚焦於複合型 ABSA 任務，這類任務不僅需要提取多個情感元素，更需要識別這
些元素之間的對應關係與依賴性，以捕捉更完整的面向級情感資訊。這種從單一元素
分析向多元素關聯分析的演進，揭示了多面向情感分析面臨的三大核心技術挑戰。
1 第一個核心挑戰：在於如何針對每篇評價面向準確提取相關的上下文資訊。在包含
多個面向的句子中，不同面向所對應的情感描述詞彙往往分散於句子的不同位置，
且這些詞彙與特定面向的語義關聯強度各異。模型需要具備精確的注意力機制，能
夠識別並聚焦於與目標面向高度相關的詞彙，同時有效抑制來自其他面向或無關詞
彙的干擾。這要求注意力機制不僅能夠捕捉詞級的語義資訊，更需要理解片語級與
句子級的複雜語義結構，才能在多面向並存的情境下實現準確的上下文提取。
2 第二個核心挑戰：涉及多個面向特徵表示的有效組合與融合策略。當句子包含多個
評價面向時，模型需要決定如何處理這些面向之間的關係。某些面向在語義上相互
```
獨立，其情感判斷應該保持獨立性，避免相互干擾;然而某些面向在語義上存在關
```
聯，其情感表達可能相互影響，需要進行適當的資訊交互與融合。傳統方法往往採
用簡單的拼接或平均策略來組合多個面向的特徵，這種一刀切的做法忽略了面向間
關係的異質性，導致語義相關的面向無法充分交互，而語義獨立的面向卻產生不必
```
要的干擾。Zhang 等人(2022)指出，複合型 ABSA 任務的目標不僅是提取多個情感
```
元素，更關鍵的是以成對、三元組甚至四元組的格式來預測這些元素，這需要模型
能夠學習面向間的選擇性組合策略，判斷哪些面向應該產生交互、哪些應該保持獨
立。
3 第三個核心挑戰：是如何建模面向之間的交互關係與依賴性。在多面向評論中，某
些面向的情感判斷可能受到其他面向的影響，形成複雜的依賴關係。例如在餐廳評
論中，使用者對於價格面向的情感評價往往與食物品質面向相關聯，高品質的食物
可能使得使用者更能接受較高的價格。這種面向間的關係建模需要模型能夠捕捉高
階的交互特徵，理解不同面向之間的語義關聯模式。然而現有研究多聚焦於單一面
```
向的情感分析，對於顯式建模面向間關係的探討仍然不足。Zhang 等人(2022)進一
```
步強調，為了評估模型在處理多面向場景的能力，研究者提出了具有挑戰性的
3
MAMS 資料集，其中每個句子至少包含兩個具有不同情感極性的面向，這種資料集
的出現凸顯了多面向情感分析的複雜性，以及對更先進技術方法的迫切需求。綜合
而言，這三大技術挑戰構成了多面向情感分析研究的核心問題，也為本研究提出整
合式解決方案奠定了動機基礎。
1.1.4 現有方法的不足與研究動機
儘管深度學習技術的發展為 ABSA 任務帶來顯著的效能提升，特別是注意力機制與
預訓練語言模型的廣泛應用已使得 ABSA 方法達到新的效能水準，但在處理多面向情感
分析場景時，現有方法仍存在三個關鍵的技術不足，這些不足構成本研究提出整合式
解決方案的核心動機。深入分析現有 ABSA 文獻可發現，多數方法在設計時聚焦於單一
面向的情感分析，對於多面向並存情境下的特殊需求考慮不足，導致在實務應用中面
臨效能瓶頸。本小節系統性地探討這些方法論上的局限性，為後續章節提出的 HMAC-
Net 架構建立必要性基礎。
1. 第一個關鍵不足在於現有方法普遍採用平面式的注意力機制設計，未能充分利用
```
文本的多層次語義結構。根據 Cheng 等人(2025)的研究回顧，早期基於注意力機
```
制的 ABSA 方法如 Tang 等人提出的深度記憶網路，以及 Wang 等人結合注意力與
LSTM 的方法，雖然能夠有效建立面向詞彙與上下文之間的關聯，但這些方法主要
在單一語義層次上計算注意力權重，缺乏對詞級、片語級與句子級等多個粒度語
義資訊的階層式建模能力。即便後續研究如 Ma 等人提出的交互式注意力網路嘗試
透過交互學習來改善面向與上下文的表示，Chen 等人提出的多重注意力機制試圖
捕捉遠距離的情感特徵，這些方法本質上仍停留在平面式的單層注意力框架內，
無法系統性地整合不同語義粒度的資訊。這種平面式設計的根本問題在於忽略了
人類理解情感的認知過程實際上是一個由局部到整體、由淺層到深層的階層式處
理過程，單一層次的注意力機制難以完整捕捉這種複雜的語義理解模式。
2. 第二個顯著不足體現在特徵組合策略的簡化處理上，現有方法缺乏針對多面向場
景設計的選擇性組合機制。當句子包含多個評價面向時，傳統方法通常採用簡單
的特徵拼接或平均池化策略來處理多個面向的表示，這種一刀切的組合方式忽略
了面向間關係的異質性特徵。從理論角度分析，某些語義相關的面向之間應該產
生資訊交互以捕捉其關聯模式，例如在餐廳評論中價格與食物品質的評價往往相
```
互關聯;然而某些語義獨立的面向則應該保持表示的獨立性，避免無關資訊的相互
```
干擾，例如餐廳的裝潢風格與服務效率通常為獨立的評價維度。現有方法缺乏學
習這種選擇性組合策略的能力，無法自動判斷哪些面向應該產生交互、哪些應該
保持獨立，導致在多面向場景下產生次優的特徵表示。更重要的是，這種簡化的
組合策略缺乏可解釋性，無法為使用者提供關於面向間關係的洞察，限制了模型
在實務決策支援上的應用價值。
3. 第三個關鍵不足在於對面向間關係的顯式建模能力不足。Cheng 等人(2025)的研究
指出，儘管語法導向的方法如基於圖卷積網路的模型嘗試利用依存句法樹來建模
詞彙間的句法關係，這些方法主要聚焦於句法層面的結構資訊，對於面向級別的
4
語義關係建模著墨較少。現有的注意力機制雖然在一定程度上能夠捕捉面向與上
下文詞彙之間的關聯，但對於多個面向之間的高階交互模式缺乏系統性的建模設
計。在多面向情感分析場景中，面向間可能存在複雜的依賴關係與相互影響，這
種關係的建模對於準確理解整體情感至關重要。然而由於現有方法將面向視為相
對獨立的分析單元，缺乏專門設計的模組來顯式學習與表示面向間的關係結構，
導致無法充分利用面向間關聯資訊來提升情感分類的準確性。此外，依存句法分
析工具本身存在準確性限制，在複雜句式中容易產生錯誤，這些錯誤會透過管線
式處理傳播至後續模組，進一步影響最終的分析效能。
綜合上述三個方面的技術不足，本研究認為有必要提出一個整合式的 ABSA 架構，
該架構需要同時解決階層式語義建模、選擇性多面向組合，以及面向間關係顯式建模
等關鍵問題。這三個問題相互關聯且共同影響多面向情感分析的效能，單獨優化其中
一個方面難以達到理想的整體效果，因此需要一個統一的框架來協同處理這些挑戰。
基於此動機，本研究提出 HMAC-Net 模型，透過 AAHA 模組實現階層式面向感知注意力
機制，透過 PMAC 模組設計可學習的選擇性組合策略，以及透過 IARM 模組建立面向間
的顯式關係建模，期望在多面向情感分析任務上達到優於現有方法的效能表現，並為
ABSA 領域提供新的理論視角與技術貢獻。
5
1.2 研究問題
基於前述對多面向情感分析技術挑戰與現有方法不足的深入分析，本研究聚焦於
解決三個相互關聯的核心研究問題，這些問題共同構成多面向情感分析領域亟待突破
的關鍵議題。
➢ 研究問題一:如何設計階層式組合機制以有效整合多個面向的情感特徵?
現有 ABSA 方法在處理包含多個評價面向的句子時，普遍採用平面式的特徵組合策
略，僅在單一語義層次上進行特徵融合，無法充分利用文本的多層次語義結構。
本研究問題探討如何建立階層式的特徵組合框架，從詞級、片語級到句子級逐步
整合多個面向的情感特徵，使模型能夠在不同語義粒度上進行漸進式的資訊聚
合。此問題的核心在於設計一個系統性的機制，使得低層次的局部語義特徵能夠
有效地傳遞與組合至高層次的全局語義表示，從而建構更豐富且精確的多面向情
感特徵表示。解決此問題將有助於克服現有方法在語義理解深度上的限制，提供
更符合人類認知過程的階層式處理範式。
➢ 研究問題二:面向感知的注意力機制如何提升模型對不同面向的區辨能力?
在多面向情感分析場景中，不同評價面向往往對應於句子中不同的語義區域，模
型需要具備針對特定面向動態調整注意力分配的能力。本研究問題探討如何設計
面向感知的注意力機制，使模型能夠根據當前處理的目標面向，自動識別並聚焦
於文本中與該面向高度相關的詞彙與片語，同時有效抑制來自其他面向或無關資
訊的干擾。此問題的挑戰在於如何將面向資訊有效地融入注意力計算過程，使得
注意力權重的分配不僅考慮詞彙間的一般性語義關聯，更能體現特定面向的語義
偏好。解決此問題將使模型具備更強的面向區辨能力，能夠在多面向並存的複雜
情境下準確提取各個面向的情感資訊，從而提升整體分析的精確度與可靠性。
➢ 研究問題三:面向間的關係建模如何改善情感分類的準確性?
在真實的使用者評論中，多個評價面向之間往往存在複雜的語義關聯與依賴關
係，某些面向的情感判斷可能受到其他面向的影響。本研究問題探討如何顯式建
模面向間的交互關係，捕捉面向之間的高階依賴模式，並將這些關係資訊融入情
感分類過程以提升預測準確性。此問題的核心在於設計有效的關係建模機制，既
能夠學習面向間的成對交互關係，也能夠捕捉多個面向之間的高階關聯模式，同
時保持計算效率以支援實務應用。解決此問題將使模型不再將面向視為相互獨立
的分析單元，而是能夠理解並利用面向間的語義關聯，提供更符合真實語義結構
的情感分析結果，特別是在處理具有複雜面向依賴關係的評論時能展現顯著的效
能提升。
這三個研究問題並非獨立存在，而是相互關聯且共同影響多面向情感分析的整體效
能。研究問題一聚焦於特徵組合的階層式設計，研究問題二關注面向感知的注意力機
制，研究問題三則探討面向間的關係建模，三者共同構成本研究提出整合式解決方案
的理論基礎。
6
1.3 研究目的
針對前述三個核心研究問題，本研究的主要目的在於提出一個整合式的深度學習
架構，透過創新的模組設計協同解決多面向情感分析面臨的關鍵挑戰，並在標準評測
資料集上驗證所提方法的有效性與優越性。具體而言，本研究包含以下三個相互支撐
的研究目的。
➢ 第一個研究目的為提出 HMAC-Net 模型架構，建構一個階層式多面向組合網路，整
合階層式注意力機制與多面向處理機制於統一框架之中。HMAC-Net 架構的設計理
念在於將多面向情感分析的複雜任務分解為三個協同運作的核心模組，每個模組
針對一個特定的研究問題提供解決方案，同時透過精心設計的資訊流動機制使得
各模組能夠有機整合，形成端到端的可訓練系統。此架構不僅需要在理論上具備
解決前述研究問題的能力，更需要在實作上保持合理的計算複雜度，以支援實務
應用場景的效能需求。透過此整合式架構的建立，本研究期望為多面向情感分析
提供一個系統性的解決範式，超越現有方法的片段式改進，達到整體效能的顯著
提升。
➢ 第二個研究目的為實現三大創新模組，分別開發 AAHA、PMAC 與 IARM 模組以解決
對應的研究問題。AAHA 模組針對研究問題二，設計面向感知階層式注意力機制，
在詞級、片語級與句子級三個語義層次上建立面向感知的注意力計算，並透過動
態層級融合策略整合多層次的注意力結果，使模型能夠根據不同面向的特性調整
注意力的語義粒度偏好。PMAC 模組針對研究問題一，設計漸進式多面向選擇性組
合機制，透過可學習的門控網路自動判斷面向間是否應該產生交互，實現選擇性
的特徵組合策略，使語義相關的面向能夠有效交互而語義獨立的面向保持獨立
性。IARM 模組針對研究問題三，採用基於 Transformer 的架構顯式建模面向間的
關係，透過多頭自注意力機制捕捉面向間的複雜交互模式，生成關係增強的面向
表示。這三個模組的設計不僅需要在技術上具備創新性，更需要在實驗上展現對
整體效能的實質貢獻，透過系統性的消融實驗量化各模組的效能增益。
➢ 第三個研究目的為驗證模型效能，在標準 ABSA 評測資料集上進行系統性實驗，評
估 HMAC-Net 模型的有效性與相較於現有方法的優越性。本研究將採用 SemEval-
2014 Workshop 的 Restaurant 與 Laptop 資料集作為主要評測基準，這些資料集已
被學術界廣泛採用作為 ABSA 任務的標準評測平台，具備完整的面向級情感標註且
涵蓋不同領域的評論文本。實驗設計將包含與代表性基準模型的效能比較，以證
```
明 HMAC-Net 相較於現有主流方法的改進幅度;系統性的消融實驗，透過逐一移除
```
```
各核心模組來量化 AAHA、PMAC 與 IARM 各自對整體效能的貢獻度;以及深入的機制
```
分析，透過視覺化注意力權重分布、分析 PMAC 門控值的稀疏性特徵、檢視 IARM
學習到的面向間關係模式，提供模型決策過程的可解釋性證據。透過完整的實驗
驗證，本研究期望不僅證明所提方法在量化指標上的優越性，更能透過質性分析
7
展現各創新模組的作用機制，為 ABSA 領域提供具有理論價值與實務參考意義的研
究成果。
綜合而言，本研究透過提出 HMAC-Net 整合式架構，實現三個協同運作的創新模組，並
進行系統性的實驗驗證，期望在理論、方法與實證三個層面為多面向情感分析研究做
出貢獻，推動該領域向更精確、更可解釋的方向發展。
1.4 論文架構
本論文共分為五章，各章節內容與安排如下
➢ 第一章 緒論，建立本研究的基礎脈絡與研究框架。
➢ 第二章 文獻探討，系統性回顧與本研究三大核心模組密切相關的理論基礎與方法
發展，為 HMAC-Net 的設計提供學術依據並指出現有研究缺口。
➢ 第三章 研究方法，詳細闡述 HMAC-Net 模型的整體架構設計與各核心模組的技術
細節，提供完整的方法論描述以確保研究的可重現性。
➢ 第四章 實驗設計，完整描述本研究實驗的各項設定，確保研究結果的嚴謹性與可
重現性。
➢ 第五章 實驗結果，分析呈現完整的實驗結果並進行深入分析，驗證 HMAC-Net 模
型的有效性與各創新模組的貢獻。
➢ 第六章 結論，總結本研究的主要成果，討論研究限制，並提出後續研究方向。
透過上述六個章節的有機組織，本論文形成從問題提出、理論探討、方法設計、實驗
驗證到結論總結的完整研究脈絡，系統性地呈現 HMAC-Net 模型在多面向情感分析領域
的理論創新與實證貢獻。
8
第二章 文獻探討
2.1 階層式注意力機制於情感分析之應用
2.1.1 注意力機制基礎理論
傳統的基於循環神經網路的情感分析方法在處理面向級情感分類任務時面臨顯著
```
的局限性。早期的長短期記憶網路(Long Short-Term Memory， LSTM)模型雖然能夠有
```
效捕捉文本的序列資訊，但其將整個句子編碼為單一固定長度的向量表示，無法識別
句子中哪些部分對於特定面向的情感判斷最為關鍵。這種整體性的編碼方式導致模型
難以在包含多個評價面向的句子中準確區分不同面向的情感極性。例如在句子「The
appetizers are ok， but the service is slow.」中，對於「taste」面向應該關注
「appetizers are ok」的部分，而對於「service」面向則應該聚焦於「service is
slow」，然而標準 LSTM 模型缺乏這種選擇性關注的能力，往往將所有詞彙同等對待，
導致情感分類的準確性受限。
```
為解決此一根本性問題，注意力機制(Attention Mechanism)被引入面向級情感分
```
```
析領域，開啟了 ABSA 方法論的重要演進。Wang 等人(2016)在其開創性研究中首次系統
```
性地將注意力機制與面向資訊相結合，提出了 Attention-based LSTM for Aspect-
```
level Sentiment Classification(ATAE-LSTM)模型，標誌著 ABSA 研究從被動接受整
```
體句子表示轉向主動選擇相關上下文資訊的重要轉折。該研究的核心洞察在於認識到
句子的情感極性不僅取決於內容本身，更高度依賴於所關注的特定面向。當不同面向
被考慮時，模型應該能夠動態調整其對句子不同部分的關注程度，從而為每個面向提
取最相關的語義資訊。
```
Wang 等人(2016)提出的方法包含三個關鍵技術創新，共同構成了現代基於注意力
```
```
的 ABSA 方法的理論基礎。第一個創新是首次提出面向嵌入(Aspect Embedding)的概
```
念，將每個面向表示為一個可學習的低維稠密向量，使得面向資訊能夠以連續向量的
形式參與神經網路的計算過程。具體而言，對於面向 i，其嵌入表示為𝑣ai ∈ Rda
```
，其中 d_a 為面向嵌入維度，所有面向嵌入組成矩陣 A ∈ ℝ^(d_a×|A|)。該研究指
```
出，這是首次在 ABSA 任務中引入可學習的面向嵌入表示，為後續研究奠定了重要基
礎。第二個創新是設計了面向導向的注意力機制，該機制能夠根據輸入的面向動態計
算句子中各個詞彙的重要性權重。注意力機制的計算過程如下:首先將 LSTM 各時間步
```
的隱藏狀態 H ∈ ℝ^(d×N)與面向嵌入 v_a 進行聯合編碼，計算中間表示 M = tanh([W_h
```
```
H; W_v v_a ⊗ e_N])，其中 W_h ∈ ℝ^(d×d)和 W_v ∈ ℝ^(d_a×d_a)為投影矩陣，⊗運算
```
```
表示將面向向量重複 N 次以匹配句子長度;接著通過注意力權重層計算每個詞的重要性
```
```
分數α = softmax(w^T M)，其中 w ∈ ℝ^(d+d_a)為可學習參數;最終得到加權後的句
```
子表示 r = Hα^T ∈ ℝ^d。這種設計使得模型能夠自動學習哪些詞彙與給定面向最相
關，從而實現選擇性的資訊提取。第三個創新是提出了 ATAE-LSTM 架構，該架構進一
步強化了面向資訊的利用方式。除了在注意力計算中使用面向嵌入外，ATAE-LSTM 還將
面向嵌入與每個詞的詞嵌入進行拼接作為 LSTM 的輸入，使得 LSTM 的隱藏狀態從一開
9
始就包含面向資訊。這種雙重融合策略確保了面向與詞彙之間的相互依賴關係能夠在
更深層次上被建模，顯著提升了模型對面向特定情感模式的捕捉能力。
```
Wang 等人(2016)的方法在 SemEval-2014 資料集上進行了全面實驗驗證，結果顯示
```
ATAE-LSTM 相較於不使用注意力機制的基準模型取得了顯著的效能提升，證明了注意力
機制在面向級情感分析任務中的有效性。該研究的實驗設計比較了三種模型變體:僅使
用注意力的 AT-LSTM、僅將面向嵌入加入 LSTM 輸入的 AE-LSTM，以及結合兩者的 ATAE-
LSTM，結果表明完整結合注意力機制與面向嵌入的 ATAE-LSTM 達到了最佳效能。這一
發現證實了面向資訊的多層次融合對於提升 ABSA 任務表現的重要性。此外，研究透過
注意力權重的視覺化分析展示了模型確實能夠根據不同面向關注句子的不同部分，例
如在處理「Staffs are not that friendly， but the taste covers all.」這一句
子時，當面向為「food」時模型主要關注「taste covers all」，而當面向為
「service」時則關注「staffs are not that friendly」，充分驗證了注意力機制的
可解釋性與合理性。
```
儘管 Wang 等人(2016)的工作為基於注意力的 ABSA 方法建立了堅實基礎，但該方
```
法仍存在兩個關鍵局限性，這些不足為後續研究指明了改進方向。首先，ATAE-LSTM 採
用的是單層平面式注意力機制，僅在句子的整體表示層次上計算注意力權重，未能充
分利用文本的多層次語義結構。人類理解情感的認知過程實際上是一個由詞級到片語
級再到句子級的階層式處理過程，單一層次的注意力機制難以全面捕捉這種複雜的語
義理解模式。例如在處理否定表達「not good」時，詞級注意力可能關注「good」這
一情感詞，但若缺乏片語級的理解則可能錯誤判斷為正面情感，忽略了否定詞「not」
對整體語義的關鍵影響。其次，該方法在計算注意力時僅考慮當前面向與句子的關
係，未能建模不同面向之間的相互作用與依賴關係。在真實應用場景中，多個面向的
情感判斷往往相互關聯，例如餐廳評論中「價格」與「食物品質」的評價可能存在補
償效應，高品質可能使消費者更能接受較高價格，但 ATAE-LSTM 的架構無法捕捉這種
面向間的交互模式。這些局限性凸顯了發展更精細化的階層式注意力機制以及面向間
關係建模方法的必要性，為本研究提出的 AAHA 模組與後續章節探討的方法論奠定了改
進動機。
2.1.2 階層式注意力設計方法
```
儘管 Wang 等人(2016)提出的單層注意力機制為 ABSA 任務帶來了顯著的效能提
```
升，但單一層次的注意力計算架構在捕捉文本的多層次語義結構方面存在根本性局
限。文本具有天然的階層式組織結構，詞彙組成片語，片語構成句子，句子形成段落
與文章，而人類理解文本語義的認知過程同樣遵循由局部到整體、由底層到高層的階
層式處理模式。單層注意力機制僅在句子的整體表示層次上計算權重，無法系統性地
建模不同語義粒度之間的層級關係，導致模型難以充分利用文本的結構化特徵。例如
在處理複雜句式時，詞級的局部語義模式與句子級的全局語境資訊往往需要在不同抽
象層次上分別處理，單層注意力機制缺乏這種分層處理能力，容易在面對長句或包含
多重修飾成分的文本時產生語義理解偏差。
10
```
為解決單層注意力機制的結構性不足，階層式注意力網路(Hierarchical
```
```
Attention Network)的概念被提出，標誌著注意力機制從平面式單層設計向結構化多
```
```
層架構的重要演進。Yang 等人(2016)在文本分類任務中首次系統性地提出並實現了階
```
層式注意力架構，該研究的核心貢獻在於認識到文本表示的建構過程應當鏡像文本的
```
天然階層結構。其提出的 Hierarchical Attention Network(HAN)模型包含兩個關鍵的
```
設計理念，共同構成了階層式注意力方法的理論基礎。第一個理念是結構對應性原
則，即神經網路的架構設計應當反映數據的內在組織結構。由於文本由詞彙形成句
子，句子再聚合為文檔，HAN 模型相應地採用兩階段的表示建構策略:首先在詞級別建
立句子表示，然後在句子級別聚合生成文檔表示。這種由下而上的階層式建構過程使
得模型能夠在不同抽象層次上分別提取和整合語義資訊，更符合人類的認知處理模
式。第二個理念是差異化重要性原則，即文檔中不同的詞彙與句子對於分類任務的貢
獻程度存在顯著差異，且這種重要性具有高度的上下文依賴性。同一個詞或句子在不
同語境中可能具有完全不同的資訊價值，因此模型需要具備動態調整關注焦點的能
力。
```
Yang 等人(2016)提出的 HAN 架構通過四個核心組件實現了階層式注意力機制的完
```
```
整功能。第一個組件是詞序列編碼器(Word Sequence Encoder)，採用雙向門控循環單
```
```
元(Bidirectional GRU)對句子中的詞序列進行編碼。對於句子中的每個詞，雙向 GRU
```
分別從前向與後向兩個方向處理詞序列，前向 GRU 捕捉詞彙的左側上下文資訊，後向
GRU 則捕捉右側上下文，兩個方向的隱藏狀態拼接後形成完整的詞表示，該表示同時包
```
含了詞彙的雙向語境資訊。第二個組件是詞級注意力層(Word-level Attention)，該
```
層的作用是識別句子中對於表達語義最為關鍵的詞彙。注意力機制首先將每個詞的隱
```
藏狀態通過單層多層感知機(MLP)進行非線性變換，然後與可學習的詞級上下文向量進
```
行相似度計算，經過 softmax 歸一化得到每個詞的重要性權重。這些權重被用於對詞
表示進行加權求和，生成句子向量。關鍵在於詞級上下文向量是通過訓練學習得到
的，它捕捉了什麼樣的詞在當前任務中更為重要的高階模式。第三個組件是句子編碼
```
器(Sentence Encoder)，同樣採用雙向 GRU 結構，將文檔中的句子序列作為輸入，每
```
個句子以前一階段生成的句子向量表示。句子編碼器的作用是建模句子之間的序列關
係與依賴結構，捕捉句子在文檔中的位置資訊與上下文語境。第四個組件是句子級注
```
意力層(Sentence-level Attention)，其結構與詞級注意力層平行，但作用於句子層
```
次。該層識別文檔中最具資訊量的句子，通過與句子級上下文向量計算相似度，為每
個句子分配權重，最終加權聚合生成文檔的整體表示向量，用於最終的分類任務。
HAN 模型在六個大規模文本分類資料集上的實驗結果充分驗證了階層式注意力機制
```
的優越性。相較於使用卷積神經網路(CNN)或長短期記憶網路(LSTM)的基準方法，HAN
```
在各項評估指標上均取得了顯著的效能提升，證明了階層式結構設計與雙層注意力機
```
制的有效性。Yang 等人(2016)特別強調，階層式注意力不僅提升了分類準確率，更提
```
供了模型決策過程的可解釋性。通過視覺化詞級與句子級的注意力權重分佈，研究者
能夠直觀地觀察模型在處理特定文檔時關注了哪些詞彙與句子，這種透明性對於理解
模型行為與診斷錯誤案例具有重要價值。例如在 Yelp 評論分類任務中，視覺化結果顯
示模型確實能夠識別出帶有強烈情感傾向的詞彙如「delicious」、「a-m-a-z-i-n-g」，
並且能夠判斷出哪些句子對於整體評分的預測貢獻更大，例如直接表達滿意度的句子
11
相較於中性描述性句子獲得了更高的注意力權重。這種可解釋性使得階層式注意力機
制不僅是一個黑箱式的效能提升工具，更成為理解文本語義結構的分析手段。
```
儘管 Yang 等人(2016)的 HAN 模型成功地證明了階層式注意力設計的價值，但該方
```
法在應用於面向級情感分析任務時仍存在適配性不足的問題。HAN 模型設計的初衷是文
檔級分類任務，其詞級與句子級注意力的計算過程獨立於任何特定的面向資訊，注意
力權重的分配僅基於詞彙或句子本身的語義特徵與任務相關性，而未考慮不同面向可
能需要關注文本不同部分的需求。在 ABSA 任務中，同一句子包含多個評價面向時，不
同面向應該動態地關注句子的不同片段，例如「食物很棒但服務很差」這一句子，對
於「食物」面向應該關注「很棒」，而對於「服務」面向則應關注「很差」。HAN 的非面
向感知設計無法實現這種動態的、面向導向的選擇性關注，限制了其在 ABSA 任務中的
應用效果。此外，HAN 僅包含詞級與句子級兩個層次的注意力，對於 ABSA 任務而言，
```
片語級的語義模式(如「not good」、「very bad」等否定或強化結構)同樣具有重要價
```
值，但 HAN 的架構未明確建模片語級特徵。這些局限性指出了發展面向感知的階層式
注意力機制的必要性，需要將階層式結構設計與面向資訊進行深度整合，使得模型能
夠在詞級、片語級與句子級三個層次上同時進行面向導向的選擇性關注，這正是本研
究提出 AAHA 模組的核心動機所在。
2.1.3 面向感知注意力機制
階層式注意力網路的提出雖然確立了在不同語義層次上分別建模的重要性，但如
何在注意力機制中有效融入面向資訊，使模型能夠針對不同面向動態調整關注焦點，
仍是 ABSA 研究中的關鍵技術挑戰。傳統的階層式注意力方法如 HAN 雖然在詞級與句子
級分別計算注意力權重，但其注意力計算過程獨立於任何特定的面向資訊，所有面向
共享同一套注意力權重，無法實現面向導向的差異化關注。在多面向情感分析場景
中，這種非面向感知的設計存在根本性局限:當同一句子包含多個評價面向時，不同面
向應該關注句子的不同部分，例如在「食物很棒但服務很差」中，對於「食物」面向
應該聚焦於「很棒」，而對於「服務」面向則應關注「很差」，然而非面向感知的注意
力機制無法區分這種差異，導致所有面向獲得相同的上下文表示，降低了情感分類的
準確性。
```
為系統性地解決面向感知問題，近年來的研究探索了將自注意力(Self-
```
```
Attention)與面向感知注意力(Aspect-Aware Attention)相結合的方法，通過多頭機
```
```
制實現更精細的語義建模。Cheng 等人(2021)在其多粒度圖卷積網路(MGCN)研究中提出
```
了融合自注意力與面向感知注意力的雙重機制，該方法的核心創新在於認識到文本的
語義關聯包含兩個互補的維度:上下文詞彙之間的普遍語義關聯，以及面向詞彙與上下
文詞彙之間的特定語義關聯。單一類型的注意力機制難以同時捕捉這兩個維度的資
訊，需要設計專門化的注意力計算策略分別建模，然後通過適當的融合機制整合兩類
資訊。該研究提出的注意力層包含三個關鍵組件，共同實現了面向感知的多頭注意力
機制。
第一個組件是多頭自注意力機制，用於捕捉上下文詞彙之間的普遍語義關聯。該
```
機制採用標準的縮放點積注意力(Scaled Dot-Product Attention)，即權重矩陣反映
```
12
的是上下文詞彙之間的相互關聯強度，不依賴於任何特定的面向資訊。這種設計使得
模型能夠捕捉通用的語義模式，例如句子中的修飾關係、並列關係與轉折關係等結構
化語義資訊，為後續的面向特定分析提供基礎的語境理解。多頭機制通過使用 P 個不
同的投影矩陣，使每個注意力頭能夠關注不同的語義模式，例如某些頭可能聚焦於鄰
近詞彙的局部關聯，而其他頭則捕捉長距離的全局依賴關係，這種多樣性增強了模型
對複雜語義結構的表示能力。
第二個組件是多頭面向感知注意力機制，專門用於建模面向詞彙與上下文詞彙之
間的特定語義關聯。與自注意力的對稱性設計不同，面向感知注意力採用非對稱計算
```
方式，將面向詞彙的表示作為查詢(Query)，上下文的隱藏狀態作為鍵(Key)，計算面
```
```
向對上下文各位置的關注程度。通過對面向詞彙的隱藏狀態進行平均池化(Average
```
```
Pooling)並複製操作獲得。Cheng 等人(2021)特別指出，當面向由多個詞彙組成時，平
```
均池化與複製操作能夠有效解決注意力區域分散的問題，確保面向的整體語義被一致
地用於計算與上下文的關聯。面向感知注意力的非對稱設計確保了計算出的權重矩陣
反映的是「哪些上下文詞彙與給定面向最相關」這一面向導向的關注模式，使得模型
能夠針對不同面向動態調整其對句子的關注焦點，實現差異化的語義提取。
第三個組件是注意力融合機制，將自注意力與面向感知注意力進行整合，生成同
```
時包含通用語義關聯與面向特定關聯的綜合語義關聯矩陣。Cheng 等人(2021)採用逐元
```
```
素加法(Element-wise Addition)策略進行融合。這種加法融合策略的理論基礎在於，
```
自注意力提供的通用語義關聯與面向感知注意力提供的特定語義關聯是互補而非競爭
的關係，兩者的疊加能夠實現資訊的協同增強。具體而言，自注意力確保模型理解句
子的整體語義結構，例如識別否定詞「不」與其修飾的形容詞「好」之間的關聯，而
面向感知注意力則進一步強調與目標面向直接相關的詞彙權重，例如當面向為「食
物」時增強「美味」的權重，當面向為「服務」時增強「緩慢」的權重。融合後的語
義關聯矩陣作為後續圖卷積層的輸入，用於增強句法結構矩陣，實現語義資訊與句法
資訊的深度整合。研究通過視覺化分析展示，融合機制確實能夠生成兼具通用性與特
異性的注意力模式，在保持對句子整體結構理解的同時，針對不同面向凸顯不同的關
鍵詞彙。
```
Cheng 等人(2021)在 SemEval-2014 Restaurant、Laptop 與 Twitter 三個資料集上
```
的實驗結果驗證了面向感知多頭注意力機制的有效性。相較於僅使用自注意力或僅使
用面向感知注意力的基準方法，融合雙重機制的完整模型在準確率與 F1 分數上均取得
了顯著提升，證明了兩種注意力的互補價值。消融實驗進一步顯示，移除面向感知注
意力導致模型在多面向場景下的表現急劇下降，特別是當句子包含情感極性相反的多
個面向時，缺乏面向感知能力的模型往往產生錯誤的情感判斷，證實了面向感知機制
對於 ABSA 任務的關鍵作用。此外，研究透過調整注意力頭數 P 的實驗發現，適當增加
```
頭數(P 從 0 增至 4)能夠提升模型捕捉多樣化語義模式的能力，但過多的頭數(P 大於
```
```
4)則可能引入噪聲，導致效能下降，這一發現為多頭機制的設計提供了實證指導。
```
儘管面向感知的多頭注意力機制已展現出良好的效能，但在面向級情感分析的完
整需求下仍存在進一步改進的空間。現有方法主要在單一語義層次上實現面向感知，
未能與階層式注意力設計進行深度整合，導致詞級、片語級與句子級的面向感知能力
未被充分開發。理論上，不同語義層次需要不同粒度的面向感知策略:詞級注意力應識
13
別與面向直接相關的情感詞，片語級注意力應捕捉包含面向的局部語義組合，句子級
注意力則需理解面向在全局語境中的角色。然而現有方法對所有層次採用統一的面向
感知機制，無法實現這種層次化的差異建模。此外，多頭機制雖然提供了多樣化的注
意力模式，但不同頭之間的協同與分工機制缺乏明確設計，頭間關係主要依賴模型的
隱式學習，可解釋性與可控性有限。這些局限性指出了發展階層化、面向感知的多頭
注意力架構的必要性，需要在詞級、片語級與句子級三個層次上分別設計專門化的面
向感知策略，並通過顯式的層次融合機制整合多層次資訊，這正是本研究提出的 AAHA
模組所要解決的核心問題，通過將階層式建模、面向感知機制與多頭注意力進行有機
整合，實現更精細化的面向級情感分析。
2.1.4 小結與研究缺口
本節系統性回顧了階層式注意力機制在面向級情感分析領域的發展歷程，從基礎
的面向感知注意力設計，到階層式結構的引入，再到多頭機制的應用，展現了該領域
從單層平面式設計向多層結構化建模演進的重要軌跡。這一演進過程揭示了三個關鍵
的技術洞察，共同構成了現代 ABSA 方法論的理論基礎。
首先，面向資訊必須深度參與注意力計算過程，而非僅作為附加特徵使用。Wang
```
等人(2016)的開創性研究證明，將面向嵌入與注意力機制相結合能夠使模型針對不同
```
面向動態調整對句子的關注焦點，這種面向導向的選擇性關注能力是準確進行面向級
情感分類的基礎。該研究提出的 ATAE-LSTM 模型雖然在單層注意力架構下取得了顯著
效能提升，但其平面式設計未能充分利用文本的多層次語義結構，僅在句子的整體表
示層次上計算注意力權重，限制了模型對複雜語義模式的捕捉能力。
其次，文本表示的建構應當遵循階層式原則，鏡像文本的天然組織結構。Yang 等
```
人(2016)提出的階層式注意力網路(HAN)系統性地證明了在詞級與句子級分別建模注意
```
力的價值，該方法通過兩階段的表示建構策略，先在詞級別生成句子表示，再在句子
級別聚合生成文檔表示，使得模型能夠在不同抽象層次上分別提取和整合語義資訊。
實驗結果顯示，階層式設計相較於單層方法在文本分類任務上取得了一致的效能提
升，證明了多層次建模的必要性。然而 HAN 的設計初衷是文檔級分類任務，其注意力
計算過程獨立於任何特定的面向資訊，未能解決 ABSA 任務中不同面向需要關注不同上
下文片段的核心需求。
第三，語義關聯的建模需要整合通用模式與面向特定模式兩個互補維度。Cheng 等
```
人(2021)的研究揭示了自注意力與面向感知注意力的協同價值，前者捕捉上下文詞彙
```
之間的普遍語義關聯，後者建模面向詞彙與上下文之間的特定語義關聯，兩者的融合
能夠生成兼具通用性與特異性的綜合語義表示。多頭機制的引入進一步增強了模型捕
捉多樣化語義模式的能力，不同的注意力頭可以關注不同的語義關係，例如局部修飾
關係與長距離依賴關係，這種多樣性提升了模型對複雜語義結構的表示能力。
儘管上述研究在各自的技術方向上取得了重要進展，但現有方法在面向級情感分析的
完整需求下仍存在三個根本性的研究缺口，這些不足限制了模型在多面向情感分析場
景中的表現，為本研究提出整合式解決方案提供了明確的改進方向。
14
第一個關鍵缺口在於階層式建模與面向感知機制的深度整合不足。現有研究要麼
```
實現了階層式結構但缺乏面向感知能力(如 HAN)，要麼實現了面向感知但停留在單層架
```
```
構(如 ATAE-LSTM)，或僅在特定層次實現面向感知(如 MGCN 的注意力層)。缺乏在詞
```
級、片語級與句子級三個層次上均實現面向感知的完整階層式架構。這種割裂的設計
導致模型無法在不同語義粒度上同時進行面向導向的差異化關注。在處理多面向評論
時，詞級注意力應識別與特定面向直接相關的情感詞彙，片語級注意力應捕捉包含面
向的局部語義組合模式，句子級注意力則需理解面向在全局語境中的角色與關係，這
三個層次的面向感知能力需要協同工作才能實現精確的情感分析。然而現有方法缺乏
這種層次化的面向感知設計，無法充分利用多粒度語義資訊的互補性。
第二個關鍵缺口在於多粒度注意力的層次融合機制設計不足。儘管多頭注意力機
制已被證明能夠捕捉多樣化的語義模式，但現有方法對不同粒度注意力的整合主要依
賴簡單的拼接或加法策略，缺乏考慮粒度間的層次關係與相互增強效應。從理論角度
分析，詞級、片語級與句子級的語義資訊應當具有由底層到高層的遞進關係，詞級特
徵構成片語級特徵的基礎，片語級特徵進一步聚合形成句子級特徵，這種層次化的構
建過程需要更精細的融合機制來體現。現有方法未能建模這種層次依賴關係，導致不
同粒度的資訊被平等對待，無法反映其在語義建構過程中的不同作用。此外，缺乏動
態調整不同粒度重要性的機制，在處理不同類型的句子時，最有價值的語義粒度可能
不同，例如簡單句可能詞級資訊最關鍵，而複雜句則更依賴句子級的全局理解，但現
有方法對所有句子採用固定的融合權重，限制了模型的適應性。
第三個關鍵缺口在於面向感知能力與多面向場景需求的匹配度不足。現有的面向
感知機制主要針對單面向分析設計，當擴展到多面向場景時，通常採用為每個面向獨
立計算注意力的策略，忽略了多個面向共存於同一句子時可能產生的交互影響。在真
實應用中，某些面向的情感判斷可能受到其他面向的影響，例如在餐廳評論中，價格
面向的評價往往與食物品質面向相關聯，高品質可能使消費者更能接受較高價格，這
種面向間的語義關聯應當反映在注意力計算過程中。然而現有方法將不同面向的注意
力計算視為相互獨立的過程，未能捕捉面向間的協同效應。此外，多面向場景下的計
算效率問題未得到充分關注，為每個面向獨立執行完整的階層式注意力計算會導致計
算成本隨面向數量線性增長，在實際應用中可能面臨效率瓶頸。
這三個研究缺口共同指向了發展整合式階層化面向感知注意力機制的必要性。理
想的解決方案應當在詞級、片語級與句子級三個層次上均實現面向感知能力，通過層
次化的融合機制整合多粒度語義資訊，並在保持計算效率的前提下適應多面向分析場
```
景的特殊需求。這正是本研究提出的 AAHA 模組(Aspect-Aware Hierarchical
```
```
Attention)所要解決的核心問題。AAHA 模組通過三層面向感知注意力的協同設計，實
```
現了階層式建模與面向感知機制的深度整合，為後續章節探討的多面向特徵組合與面
向間關係建模奠定了堅實的表示學習基礎。
15
2.2 多面向特徵組合與融合方法
2.2.1 特徵組合策略回顧
深度學習模型在面向級情感分析任務中的成功，很大程度上取決於如何有效地組
合與融合來自不同來源或層次的特徵表示。當句子包含多個評價面向時，模型需要同
時處理多個面向的語義資訊，這使得特徵組合策略的設計成為影響模型效能的關鍵因
素。本小節系統性回顧情感分析領域中主流的特徵組合方法，為後續探討多面向場景
下的選擇性組合機制奠定理論基礎。
從特徵組合的抽象層次來看，現有方法可分為三個主要範疇：單一來源特徵的多
層次組合、跨來源特徵的融合策略，以及基於注意力機制的動態組合方法。這三類方
法分別對應了特徵組合問題的不同維度，其設計理念與技術路徑各有特色，共同構成
了現代深度學習模型中特徵組合策略的完整圖景。
首先，單一來源特徵的多層次組合策略關注如何整合來自不同抽象層次的表示。
```
Wang 等人提出的階層式融合模型(Cross-modal Hierarchical Fusion Model)展示了多
```
層次特徵組合的價值，該研究雖聚焦於多模態情感分析，但其提出的單模態特徵學習
```
(Unimodal Feature Learning)框架揭示了一個重要原則：深度神經網路的不同層次捕
```
捉了不同粒度的語義資訊，淺層特徵傾向於表示局部的、細粒度的語義模式，而深層
特徵則編碼更抽象的、全局性的語義概念。在面向級情感分析場景下，這一原則同樣
適用。對於單一面向的特徵表示，模型可以從 BERT 等預訓練語言模型的不同層提取表
示，淺層輸出保留了豐富的詞彙級語義細節，有助於識別與面向直接相關的情感詞
彙，而深層輸出則整合了更廣泛的上下文資訊，能夠捕捉句子級的情感傾向。Wang 等
人的研究證明，通過適當的組合策略整合多層次特徵，相較於僅使用單一層次的表
示，能夠顯著提升情感分析的準確性。該研究在 CMU-MOSEI 資料集上的消融實驗顯
示，引入多層次特徵學習後，模型的準確率從 70.49%提升至 84.31%，F1 分數從
64.34%提升至 84.18%，這一顯著的效能提升驗證了多層次特徵組合的有效性。
```
在多層次特徵的具體組合方式上，最直接的方法是特徵拼接(Feature
```
```
Concatenation)。這種方法將不同層次的特徵向量在維度上進行串接，形成一個更高
```
維度的綜合表示，然後通過全連接層進行降維與非線性轉換。特徵拼接的優勢在於實
現簡單且不會丟失任何原始資訊，但其缺陷在於缺乏對不同層次特徵重要性的區分，
所有層次的資訊被平等對待，這可能導致某些關鍵的層次資訊被稀釋。另一種常見的
```
組合方式是加權求和(Weighted Summation)，透過為不同層次的特徵分配可學習的權
```
重參數，模型能夠自適應地調整各層次特徵的貢獻度。Wang 等人的研究採用了平均池
```
化(Average Pooling)策略來整合多層次資訊，這可視為加權求和的特例，其中權重被
```
固定為均等值。實驗結果表明，即使採用簡單的平均池化，多層次特徵組合仍能帶來
顯著的效能改善，這暗示了多層次資訊本身的互補性價值，而非僅依賴複雜的組合機
制。
其次，跨來源特徵的融合策略探討如何整合來自不同資訊來源的表示。在多面向
情感分析場景中，最典型的跨來源融合發生在面向資訊與上下文資訊之間。每個面向
本身攜帶特定的語義資訊，例如面向詞彙的嵌入表示編碼了該面向的語義內容，而上
16
下文則提供了圍繞面向的情感線索。如何有效融合這兩類資訊，是面向級情感分析的
```
核心挑戰之一。Wang 等人提出的跨模態互動(Inter-Modal Interaction)模組，雖然原
```
始設計用於融合文本、音訊與視覺等不同模態的資訊，但其融合理念對於面向與上下
```
文的融合同樣具有啟發性。該研究採用張量融合網路(Tensor Fusion Network， TFN)
```
來建模不同來源之間的交互關係，TFN 透過外積運算捕捉兩個特徵向量之間的所有可能
組合，能夠顯式地表示特徵間的高階交互模式。
在面向級情感分析的脈絡下，TFN 的思想可應用於融合面向嵌入與上下文表示。面
向嵌入提供了目標實體的語義資訊，上下文表示則編碼了句子中與該面向相關的情感
線索，兩者之間的交互能夠產生面向感知的情感表示。例如，對於句子「食物很棒但
服務很差」中的面向「食物」，其面向嵌入 𝐚𝐟𝐨𝐨𝐝 與上下文表示 𝐜𝐟𝐨𝐨𝐝 的融合應當突出
「很棒」這一正面情感詞彙的影響，而抑制「很差」這一負面詞彙的干擾。TFN 透過建
模𝐚𝐟𝐨𝐨𝐝 與 𝐜𝐟𝐨𝐨𝐝 之間的高階交互，能夠學習到這種面向特定的選擇性關注模式。Wang
等人的實驗顯示，引入跨模態互動模組後，模型在需要整合多源資訊的樣本上表現出
更強的魯棒性，這驗證了顯式建模特徵交互的價值。
第三，基於注意力機制的動態組合方法提供了更靈活的特徵整合範式。相較於固
定的組合策略如拼接或加權求和，注意力機制能夠根據輸入的具體內容動態調整組合
```
權重，實現自適應的特徵選擇與融合。Wang 等人提出的多頭注意力(Multi-Head
```
```
Attention， MHA)模組，展示了注意力機制在特徵組合中的優勢。多頭注意力的核心
```
思想是在多個子空間中分別計算注意力權重，每個注意力頭關注特徵的不同方面，最
終將所有頭的輸出拼接並進行線性轉換得到最終表示。這種機制允許模型同時捕捉多
種互補的注意力模式，相較於單頭注意力具有更強的表達能力。
將多頭注意力應用於多面向情感分析，可以設計面向感知的注意力組合策略。對
於包含多個面向的句子，每個面向的上下文表示可視為一個查詢，全句的語義表示或
其他面向的表示可視為鍵與值。透過注意力機制，模型能夠為每個面向動態選擇最相
關的上下文資訊，實現面向特定的特徵組合。例如，在句子「餐廳的食物美味但價格
昂貴，服務態度友善」中，面向「食物」的注意力應當主要聚焦於「美味」與「價格
昂貴」，而面向「服務」的注意力則應關注「態度友善」。多頭機制進一步允許模型從
多個角度理解面向與上下文的關係，某些頭可能關注情感詞彙，某些頭則關注修飾關
係或句法依賴，這種多樣性增強了模型的表達能力。
綜合上述分析，現有的特徵組合策略為多面向情感分析提供了豐富的技術選擇。
多層次組合策略揭示了不同抽象層次資訊的互補性，跨來源融合方法如 TFN 提供了建
模高階交互的能力，而注意力機制則實現了自適應的動態組合。然而，這些方法在設
計時多聚焦於單一面向或獨立處理各面向的場景，對於多面向並存時如何選擇性地組
合特徵，避免不相關面向間的資訊干擾，現有研究尚缺乏系統性探討。Wang 等人的多
```
任務輔助學習(Multi-Tasking Assisted Learning)框架雖然引入了單模態與雙模態預
```
測作為輔助任務，體現了任務間資訊共享的思想，但其組合策略仍基於固定的拼接與
注意力機制，缺乏對任務間關係的顯式建模與自適應調節。這一局限性指出了發展選
擇性組合機制的必要性，即模型需要學習判斷何時應融合多個面向的資訊、何時應保
持面向間的獨立性，這正是後續小節將探討的核心議題。
17
2.2.2 門控機制在特徵融合的應用
在特徵組合的眾多策略中，固定的組合方式如簡單拼接或加權求和雖然實現簡
單，但其靜態的組合權重無法根據輸入樣本的具體特性進行調整，導致在面對不同類
```
型的樣本時缺乏適應性。為解決這一問題，門控機制(Gating Mechanism)作為一種動
```
態的特徵選擇與融合策略，在深度學習模型中獲得了廣泛應用。門控機制的核心思想
是引入可學習的門控單元，透過非線性變換計算出介於 0 與 1 之間的門控值，用以控
制不同特徵通道的資訊流動強度，從而實現自適應的特徵融合。本小節系統性探討門
控機制在特徵融合中的應用，分析其設計原理、實現方式與效能優勢，為後續提出的
選擇性多面向組合機制提供方法論基礎。
```
門控機制最早在循環神經網路(Recurrent Neural Networks， RNN)的改進版本中
```
```
獲得成功應用。長短期記憶網路(Long Short-Term Memory， LSTM)與門控循環單元
```
```
(Gated Recurrent Unit， GRU)透過引入遺忘門(Forget Gate)、輸入門(Input Gate)
```
```
與輸出門(Output Gate)等門控結構，有效解決了傳統 RNN 在處理長序列時的梯度消失
```
與梯度爆炸問題。這些門控單元的設計揭示了一個重要原則：透過可學習的門控值來
控制資訊的保留與更新，能夠使模型自動學習何時應記憶歷史資訊、何時應遺忘無關
資訊，從而在序列建模任務中取得顯著的效能提升。這一成功經驗啟發了研究者將門
控機制推廣至更廣泛的特徵融合場景，特別是在需要整合來自不同來源或不同抽象層
次特徵的任務中。
在特徵融合的具體應用中，Deng 等人提出的注意力機制結合雙向 LSTM 與捲積神經
```
網路的門控融合模型(Attention-based BiLSTM fused CNN with Gating Mechanism，
```
```
ABLG-CNN)為門控機制的應用提供了典型範例。該研究聚焦於中文長文本分類任務，面
```
臨的核心挑戰是長文本中存在大量冗餘資訊，且部分句子可能涉及其他主題的資訊，
這使得單一類型的特徵提取器難以同時捕捉文本的全局語義結構與局部關鍵資訊。為
解決這一問題，ABLG-CNN 採用雙向 LSTM 捕捉文本的上下文特徵，同時使用 CNN 提取局
部的主題顯著特徵，然後透過門控機制自適應地融合這兩類特徵。這種設計的核心洞
察在於：不同的文本樣本對於上下文特徵與局部特徵的依賴程度各異，某些樣本的情
感判斷主要依賴於全局的語義脈絡，而某些樣本則更依賴於特定的關鍵詞彙或短語，
因此需要一個自適應的機制來動態調整兩類特徵的融合權重。
ABLG-CNN 中門控機制的具體實現展現了其設計的優雅性與有效性。給定雙向 LSTM 在最
後時刻的輸出𝐇 ∈ 𝑅𝑑ℎ （編碼了文本的上下文特徵）與 CNN 的輸出𝐌 ∈ 𝑅𝑑𝑚
（捕捉了文本的局部顯著特徵），門控機制首先透過一個由權重矩陣𝐖與偏置項𝑏參數
化的線性變換，後接 sigmoid 激活函數，計算出上下文特徵的門控權重α1：
```
𝛼1 = 𝜎(W ⋅ H + b)
```
```
其中𝜎(⋅)為 sigmoid 函數，將輸出值壓縮至(0，1)區間。為保證兩類特徵權重之和
```
為 1，局部特徵的門控權重α2被設定為α2 = 1 − α1。最終的融合特徵向量𝑧透過加權求
和得到：𝑧 = 𝛼1 ⋅ 𝐻 + 𝛼2 ⋅ 𝑀
這個簡潔的公式背後蘊含了豐富的自適應機制。門控權重α1的值完全由模型從數
據中學習得到，對於那些主要依賴全局上下文進行分類的樣本，模型會學習到較大的
α1值（接近 1），使得融合特徵主要由上下文資訊主導；反之，對於依賴局部關鍵詞彙
18
的樣本，α1會接近 0，融合特徵則更多地保留局部顯著資訊。這種端到端的學習方式避
免了人工設定權重的主觀性，使得融合策略能夠自動適應數據的分佈特性。
Deng 等人在 THUCNews 與 Sogou 兩個中文長文本資料集上的實驗結果充分驗證了門
```
控機制在特徵融合中的有效性。消融實驗(Ablation Study)是評估模型各組件貢獻度
```
的標準方法，該研究設計了多個對比實驗，包括僅使用 BiLSTM 與注意力機制的模型
```
(Attention + BiLSTM)、僅使用 CNN 與注意力機制的模型(Attention + CNN)，以及使
```
```
用簡單拼接操作替代門控機制的模型(Attention + BiLSTM + CNN + splicing)。實驗
```
結果顯示，完整的 ABLG-CNN 模型在 THUCNews 資料集上達到 98.01%的 F1 分數，在
Sogou 資料集上達到 98.32%的 F1 分數，顯著優於所有基線模型。特別值得注意的是，
與使用拼接操作的變體模型相比，引入門控機制後在 THUCNews 資料集上帶來了 0.29%
的 F1 提升，在 Sogou 資料集上帶來了 0.35%的提升。雖然提升幅度看似較小，但考慮
到基線模型本身已達到 97%以上的高準確率，在如此高的效能基準上仍能獲得一致的改
善，這充分證明了門控機制的價值。
進一步分析門控機制的效能優勢，可以從兩個層面理解其作用機理。首先，從特
```
徵選擇的角度，門控機制實現了軟性的特徵篩選(Soft Feature Selection)。相較於
```
硬性的特徵選擇方法如閾值化或 top-k 選擇，門控機制產生的是連續的權重值，這使
得梯度能夠順暢地反向傳播至特徵提取層，支援端到端的訓練。這種軟性選擇既保留
了特徵融合的靈活性，又避免了硬性選擇可能引入的資訊丟失。其次，從模型容量的
```
角度，門控機制為模型提供了條件計算(Conditional Computation)的能力。不同於靜
```
態的融合策略對所有樣本採用相同的組合方式，門控機制使得模型能夠根據輸入樣本
的特性動態調整計算路徑，某些樣本可能更多地依賴某一類特徵，而其他樣本則依賴
另一類特徵。這種條件性提升了模型的表達能力，使其能夠更好地擬合訓練數據的複
雜分佈。
將門控機制的設計理念遷移至多面向情感分析場景，可以發現其具有天然的適配
性。在包含多個評價面向的句子中，不同面向之間可能存在語義關聯，也可能相互獨
立。例如在餐廳評論「食物美味但服務緩慢，環境優雅」中，「食物」與「服務」兩個
面向在語義上相對獨立，其情感表達互不影響，而「環境」面向的評價則可能與整體
用餐體驗相關聯。如果採用固定的融合策略，強制所有面向的特徵進行組合，將導致
語義獨立的面向之間產生不必要的資訊干擾；反之，如果完全不進行面向間的資訊交
互，則無法捕捉某些面向之間確實存在的關聯模式。門控機制提供了一個優雅的解決
方案：透過可學習的門控值來決定面向間是否進行資訊交互以及交互的強度。對於語
義獨立的面向對，模型可以學習到接近 0 的門控值，保持各面向特徵的獨立性；對於
存在關聯的面向對，模型則學習到較大的門控值，允許資訊流動與特徵融合。
然而，ABLG-CNN 中門控機制的設計也存在一定的侷限性，這些侷限性指出了進一步改
進的方向。首先，該模型中的門控機制僅控制兩個特徵來源（BiLSTM 與 CNN）的融合
權重，且兩者的權重關係被硬性約束為互補（𝛼1 + 𝛼2 = 1）。這種設計雖然保證了融合
特徵的範數穩定性，但限制了模型的靈活性。在多面向情感分析場景中，可能存在多
個面向（𝑁 > 2），每對面向之間的關聯強度各異，簡單的互補約束難以充分表達複雜
的關聯模式。其次，ABLG-CNN 的門控值僅依賴於 BiLSTM 的輸出𝐻，未考慮 CNN 輸出𝑀
19
M 的資訊。理論上，兩類特徵之間的融合權重應該同時依賴於兩者的內容，這樣才能充
分捕捉特徵間的交互關係。改進的門控機制可以將門控值定義為兩類特徵的聯合函
```
數，例如𝛼 = 𝜎(𝐖𝟏𝐇 + 𝐖𝟐𝐌 + 𝑏)，這將提供更豐富的表達能力。
```
```
另一個值得探討的改進方向是門控機制的稀疏性(Sparsity)。在 ABLG-CNN 的實現
```
```
中，sigmoid 激活函數產生的門控值分佈在(0，1)區間內，很少會嚴格等於 0 或 1，這
```
意味著兩類特徵總是會在某種程度上進行融合，缺乏明確的選擇性。對於多面向情感
分析任務，理想的門控機制應當能夠學習到稀疏的門控模式，即大部分面向對之間的
門控值接近 0（表示保持獨立），僅少數確實存在關聯的面向對具有顯著的門控值。實
現這一目標的方法包括在損失函數中加入稀疏性正則化項，或者採用能夠產生稀疏輸
出的激活函數如 ReLU。稀疏的門控模式不僅能夠減少不必要的資訊干擾，提升模型效
能，更重要的是增強了模型的可解釋性，透過檢視哪些面向對具有高門控值，研究者
可以深入理解模型學習到的面向間關聯模式，這對於實務應用具有重要價值。
綜合上述分析，門控機制在特徵融合中展現出顯著的優勢，其核心價值在於提供
了自適應的、數據驅動的融合策略，能夠根據輸入樣本的特性動態調整不同特徵的組
合權重。Deng 等人的工作證明了即使在高效能的基線模型上，引入門控機制仍能帶來
一致的效能提升，這驗證了門控機制的實用性。然而，現有的門控機制設計多聚焦於
兩個特徵來源的融合場景，對於多個特徵來源（如多個評價面向）的組合，以及如何
實現稀疏的選擇性融合，仍缺乏系統性的探討。這些局限性為發展更先進的門控融合
機制指明了方向，特別是在多面向情感分析場景下，需要設計能夠處理𝑁個面向、學習
稀疏門控模式、並且門控值同時依賴於所有相關面向特徵的門控機制。這正是下一小
節將要探討的選擇性組合方法的核心議題，透過結合門控機制的自適應性與稀疏性約
束，構建更精細化的多面向特徵組合策略。
2.2.3 選擇性組合方法
前述小節探討的門控機制雖然實現了自適應的特徵融合，但其應用場景主要集中
在兩個特徵來源的組合，且門控值的計算往往僅依賴單一特徵來源的資訊。當面對多
個特徵來源需要同時組合的場景時，如何設計更精細化的選擇策略，使得模型能夠自
動判斷哪些特徵應該融合、哪些特徵應該保持獨立，成為特徵組合研究的前沿議題。
```
選擇性組合方法(Selective Composition Methods)正是為解決這一挑戰而提出的，其
```
核心思想是不再對所有特徵進行無差別的融合，而是透過學習一個選擇機制，為每對
特徵賦予融合權重，實現稀疏的、有針對性的特徵組合。本小節深入探討選擇性組合
方法的設計原理、實現技術與效能優勢，為本研究提出的多面向選擇性組合機制提供
直接的方法論支撐。
選擇性組合的核心動機源於一個重要的觀察:在包含多個資訊來源的複雜場景中，
並非所有來源之間都存在有意義的關聯，盲目地融合所有來源的資訊可能引入大量冗
```
餘甚至噪聲，反而降低模型效能。Sun 等人在顯著目標檢測(Salient Object
```
```
Detection)任務中提出的選擇性特徵融合網路(Selective Feature Fusion Network，
```
```
SFFNet)為這一理念提供了具體實現。該研究面對的挑戰是如何有效融合來自深度神經
```
網路不同層次的特徵，淺層特徵包含豐富的空間細節，有助於精確定位目標邊界，但
20
```
同時也包含較多的背景噪聲;深層特徵則提供高階的語義資訊，能夠有效定位顯著目標
```
並抑制冗餘資訊，但缺乏細粒度的空間細節。傳統的融合方法如簡單拼接
```
(Concatenation)或逐元素相加(Element-wise Addition)將所有層次的特徵平等對
```
待，導致噪聲與有用資訊混雜，增加了檢測的難度。Sun 等人的實驗透過視覺化融合前
```
後的特徵圖(如圖 1 所示)，清晰地展示了這一問題:使用拼接操作的方法容易產生噪聲
```
並保留非顯著特徵，而使用逐元素乘法或加法的方法雖然保留了主要的顯著目標特
徵，但忽略了某些局部細節。
```
為克服這些局限性，Sun 等人提出的選擇性特徵融合模組(Selective Feature
```
```
Fusion Module， SFM)採用了一種創新的融合策略，其核心在於顯式地學習特徵的融
```
合權重矩陣，而非採用固定的組合方式。給定來自高層次的特徵𝐅𝐡 ∈ 𝑅𝐶×𝐻ℎ×𝑊ℎ
與來自低層次的特徵Fl ∈ RC×Hl×Wl ，SFM 首先對高層特徵進行上採樣以匹配低層特徵的
```
空間分辨率，然後分別通過卷積-批次歸一化-激活(CBR)模組進行特徵增強。關鍵的創
```
新在於，模組不僅拼接原始的增強特徵，還計算兩者的逐元素乘積以提取共同部分，
強化不同尺度特徵間的一致性。具體而言，多尺度特徵圖Fmix透過以下方式獲得:
```
𝐹𝑚𝑖𝑥 = 𝐶𝑎𝑡([𝐹𝐶𝐵𝑅(𝑈𝑝(𝐹ℎ)); 𝐹𝐶𝐵𝑅(𝐹𝑙); 𝐹𝐶𝐵𝑅(𝑈𝑝(𝐹ℎ)) ⊙ 𝐹𝐶𝐵𝑅(𝐹𝑙)])
```
```
其中Cat([⋅;⋅])表示拼接操作，⊙表示逐元素乘法，Up(⋅)表示上採樣操作，FCBR(⋅)表示
```
CBR 模組。這個設計的巧妙之處在於同時保留了兩個層次的獨立資訊與其交互資訊，為
後續的選擇性融合提供了豐富的輸入。
選擇性融合的核心機制體現在權重矩陣的生成與應用上。模組將原始的高層與低層特
徵拼接後，通過 CBR 模組進行通道壓縮並應用 softmax 歸一化，生成權重矩陣Fweight:
```
𝐹𝑤𝑒𝑖𝑔ℎ𝑡 = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥 (FCBR(Cat([Up(Fh); Fl])))
```
softmax 函數的使用確保了權重值在每個空間位置上的和為 1，這意味著對於每個像素
位置，模型需要在高層與低層特徵之間進行選擇性的權重分配。權重值較大的特徵將
在最終融合中占據主導地位，而權重值較小的特徵則被相對抑制。這種像素級的選擇
性允許模型在特徵圖的不同區域採用不同的融合策略，某些區域可能更依賴高層語義
資訊，而其他區域則更需要低層的空間細節。最終的融合特徵𝐅𝐦𝐢𝐱′ 透過逐元素乘法與
```
CBR 模組得到:𝐹𝑚𝑖𝑥′ = 𝐹𝐶𝐵𝑅(𝐹𝑚𝑖𝑥 ⊙ 𝐹𝑤𝑒𝑖𝑔ℎ𝑡)。這個公式的實質是將多尺度特徵圖
```
𝑚𝑎𝑡ℎ𝑏𝑓𝐹𝑚𝑖𝑥的每個元素乘以對應的權重，實現特徵的加權選擇與重分配，從而有效抑
制冗餘資訊並保留重要細節。
進一步分析選擇性融合方法的技術特點，可以從三個層面理解其優勢。首先，從
```
特徵選擇的粒度來看，SFM 實現了像素級(Pixel-wise)的選擇性融合，相較於通道級
```
```
(Channel-wise)或特徵圖級(Feature Map-wise)的選擇，像素級的粒度提供了最精細
```
的控制，使得模型能夠在特徵圖的不同空間位置採用不同的融合策略。這種細粒度的
選擇性對於處理空間異質性強的任務尤為重要，例如在顯著目標檢測中，目標區域與
背景區域對於高層與低層特徵的依賴程度顯著不同，像素級的選擇機制能夠自適應地
處理這種異質性。其次，從權重學習的方式來看，SFM 採用數據驅動的端到端學習，權
重矩陣完全由模型從訓練數據中學習得到，無需人工設定任何先驗知識或啟發式規
則。這使得選擇性融合策略能夠自動適應特定任務與數據集的特性，具有良好的泛化
21
能力。第三，從計算效率的角度，雖然 SFM 引入了額外的權重計算與逐元素乘法操
作，但這些操作的計算複雜度相對較低，不會顯著增加模型的推理時間。Sun 等人的實
驗表明，引入 SFM 後模型的訓練時間僅增加約 8%，而效能提升則遠超過這一額外成
本，證明了選擇性融合在效率與效能之間取得了良好的平衡。
將選擇性融合的設計理念遷移至多面向情感分析場景，可以發現其具有天然的適
配性與重要的改進空間。在包含多個評價面向的句子中，不同面向之間的語義關聯強
```
度差異顯著，某些面向對(如餐廳評論中的「食物品質」與「價格」)在語義上可能存
```
```
在關聯，消費者對價格的接受度往往受到食物品質的影響;而其他面向對(如「食物品
```
```
質」與「服務態度」)則相對獨立，各自的情感判斷互不干擾。理想的多面向組合策略
```
應當能夠學習這種異質性的關聯模式，對於語義相關的面向對賦予較大的組合權重，
允許資訊交互與特徵融合，而對於獨立的面向對則賦予接近零的權重，保持各面向特
徵的獨立性。SFM 提供的選擇性融合框架為實現這一目標提供了直接的方法論啟發:透
過學習面向對之間的權重矩陣，類似於 SFM 中的Fweight，可以實現面向級的選擇性組
合。
然而，SFM 的設計也存在需要改進的局限性，這些局限性指出了發展面向級選擇性
組合機制的具體方向。首先，SFM 的權重矩陣是在空間維度上定義的，即為每個像素位
置學習融合權重，這適用於圖像等具有空間結構的數據，但在多面向情感分析中，關
注的是面向之間的關聯而非空間位置，因此需要將選擇機制從空間維度轉換至面向維
度，學習一個面向對面向的權重矩陣。其次，SFM 僅處理兩個層次的特徵融合，當存在
𝑁個面向時，需要擴展選擇機制以處理𝑁 × 𝑁的面向對組合，這要求設計高效的權重計
算策略以避免組合爆炸。第三，SFM 使用 softmax 歸一化保證權重和為 1，這種約束適
用於互斥的選擇場景，但在多面向組合中，某個面向可能同時受到多個其他面向的影
響，簡單的歸一化約束可能限制模型的表達能力，可以考慮移除這一約束或採用稀疏
性正則化來鼓勵大部分權重接近零，從而實現真正的選擇性。
此外，SFM 的權重計算僅依賴於要融合的兩個特徵本身，未考慮其他上下文資訊。
在多面向情感分析場景中，面向對之間的關聯性不僅取決於兩個面向本身的語義內
容，還可能受到句子全局語境、其他面向的存在等因素的影響。改進的選擇性組合機
```
制可以將權重計算設計為多個面向特徵的聯合函數，例如𝑤𝑖𝑗 = 𝜎 (Gate([𝐚𝐢; 𝐚𝐣; 𝐜]))，
```
```
其中𝐚𝐢與aj分別為面向𝑖與𝑗的特徵表示，𝑐為句子的全局語境表示，Gate(⋅)為門控網
```
```
路，𝜎(⋅)
```
```
σ(⋅)為 sigmoid 函數。這種設計能夠捕捉更豐富的上下文依賴關係，提升選擇機制的
```
準確性。
綜合上述分析，選擇性組合方法代表了特徵融合研究的前沿方向，其核心價值在
於透過學習顯式的選擇機制，實現稀疏的、有針對性的特徵組合，有效抑制冗餘資訊
並保留重要細節。Sun 等人在顯著目標檢測任務中的工作證明了選擇性融合策略相較於
傳統的拼接或加法操作能夠帶來一致的效能提升，且與其他先進的融合模組相比具有
優越性。然而，現有的選擇性融合方法主要針對圖像領域中空間維度的特徵組合，對
```
於自然語言處理中面向維度的選擇性組合，以及如何擴展至多個(2𝑁 > 2)特徵來源的
```
場景，仍缺乏系統性的探討。此外，如何引入稀疏性約束以實現真正的選擇性，而非
22
僅僅是權重的重分配，也是需要深入研究的問題。這些挑戰與機遇共同指向了發展面
```
向級選擇性組合機制的必要性，這正是本研究提出的漸進式多面向組合(Progressive
```
```
Multi-Aspect Composition， PMAC)模組所要解決的核心問題，透過結合門控機制的
```
自適應性、選擇性融合的稀疏性原則，以及針對多面向場景的專門化設計，構建一個
能夠學習面向間選擇性關聯模式的精細化組合策略。
2.2.4 小結與研究缺口
本節系統性回顧了多面向特徵組合與融合方法的發展歷程，從基礎的組合策略、
到門控機制的引入、再到選擇性融合方法的提出，展現了該領域從固定組合向自適應
選擇演進的重要軌跡。這一演進過程揭示了三個關鍵的技術洞察，共同構成了現代特
徵組合方法論的理論基礎，同時也指出了多面向情感分析領域中亟待突破的研究缺
口。
首先，特徵組合的有效性取決於對不同來源或層次資訊互補性的充分利用。Wang
等人的階層式融合模型證明，深度神經網路不同層次的特徵編碼了不同粒度的語義資
訊，淺層特徵保留豐富的詞彙級細節，深層特徵則提供全局的語義理解，兩者的有機
整合能夠顯著提升模型效能。在 CMU-MOSEI 資料集上，多層次特徵組合使準確率從
70.49%提升至 84.31%，F1 分數從 64.34%提升至 84.18%，這一顯著的效能躍升驗證了
多層次組合的必要性。這一洞察對於多面向情感分析同樣適用，不同面向的特徵表示
可能編碼了不同粒度的情感資訊，某些面向聚焦於具體的情感詞彙，某些面向則反映
整體的評價傾向，有效整合這些多粒度資訊是提升分析準確性的關鍵。然而，現有研
究主要關注單一面向內部不同層次資訊的組合，對於多個面向之間如何進行跨面向的
特徵組合，尚缺乏深入探討。
其次，自適應的組合策略優於固定的組合方式。Deng 等人提出的門控融合機制展
示了動態調整組合權重的價值，透過可學習的門控單元，模型能夠根據輸入樣本的具
體特性自動調整不同特徵來源的融合比例。在 THUCNews 與 Sogou 長文本分類資料集
上，引入門控機制後 F1 分數分別達到 98.01%與 98.32%，相較於使用簡單拼接的變體
模型帶來 0.29%與 0.35%的提升。雖然提升幅度看似較小，但在已達 97%以上高效能基
準的情況下，一致的改善充分證明了自適應機制的實際價值。門控機制的核心優勢在
於實現了軟性的特徵選擇，連續的權重值支援端到端訓練並避免資訊丟失，同時提供
了條件計算能力，使模型能夠根據樣本特性動態調整計算路徑。這種自適應性對於多
面向情感分析尤為重要，不同句子包含的面向數量與類型各異，某些句子可能僅涉及
單一面向，某些句子則同時評價多個面向，固定的組合策略難以處理這種異質性，而
自適應的門控機制則能夠靈活應對。然而，現有的門控設計多聚焦於兩個特徵來源的
```
融合，當面向數量為𝑁(N > 2)時，如何擴展門控機制以處理多個面向的選擇性組合，
```
仍是未解決的問題。
第三，選擇性比全面性更符合特徵組合的實際需求。Sun 等人提出的選擇性特徵融
合模組打破了傳統方法對所有特徵進行無差別融合的範式，透過學習像素級的融合權
重矩陣，模型能夠在特徵圖的不同區域採用不同的融合策略，某些區域依賴高層語
23
義，某些區域則需要低層細節。在 DUT-OMRON 與 DUTS-TE 資料集上，選擎性融合使 F1
分數分別從 0.810 提升至 0.826 與從 0.877 提升至 0.893，且優於其他先進的融合模組
如 AIM、FIA 與 AFF。更重要的是，視覺化分析清晰展示了選擇性融合相較於簡單拼接
或逐元素運算能夠更有效地抑制冗餘資訊並保留重要細節。這一發現揭示了一個關鍵
原則：並非所有特徵來源之間都存在有意義的關聯，盲目融合可能引入噪聲反而降低
效能，而選擇性機制允許模型學習稀疏的組合模式，僅在確實存在關聯的特徵對之間
進行資訊交互。這一原則對於多面向情感分析具有直接的指導意義，在包含多個評價
面向的句子中，某些面向對在語義上相互獨立，其情感判斷應保持獨立性以避免干
擾，而某些面向對則存在關聯，需要適當的資訊交互。理想的多面向組合策略應當學
習這種選擇性模式，但現有方法尚未實現這一目標。
儘管上述研究為特徵組合提供了豐富的技術工具與設計理念，但在多面向情感分
析的具體應用場景下，仍存在三個關鍵的研究缺口，這些缺口構成了本研究提出整合
式解決方案的核心動機。
第一個關鍵缺口在於現有組合方法缺乏針對多面向場景的專門化設計。無論是
Wang 等人的多層次融合、Deng 等人的門控機制，還是 Sun 等人的選擇性融合，這些方
法的設計初衷都不是為多面向情感分析任務服務，而是分別針對多模態情感分析、長
文本分類與顯著目標檢測等任務。雖然其核心技術原理具有通用性，可以遷移至多面
向場景，但這種直接遷移忽略了多面向情感分析的特殊性需求。在多面向場景中，關
注的核心是面向與面向之間的關聯模式，而非空間維度的特徵融合或模態間的資訊整
合。例如，Sun 等人的選擇性融合在空間維度上學習權重矩陣，為每個像素位置分配融
合權重，這適用於圖像等具有空間結構的數據，但在文本情感分析中，需要的是面向
維度的選擇機制，學習面向對面向的關聯權重。目前尚無研究系統性地探討如何設計
專門針對多面向的選擇性組合機制，這是第一個亟待填補的研究缺口。
第二個顯著缺口體現在多個面向同時組合時的策略設計上。現有的門控與選擇性融合
方法主要處理兩個特徵來源的組合場景，Deng 等人的門控機制控制 BiLSTM 與 CNN 兩個
網路的融合，Sun 等人的選擇性融合處理高層與低層兩個層次的特徵。
當面向數量𝑁 > 2時，組合的複雜度呈平方級增長，需要學習𝑁 × 𝑁的面向對關聯矩
陣。簡單地擴展現有方法可能導致參數爆炸與訓練困難，更重要的是，多個面向的組
合順序與方式可能影響最終的表示質量。是否應該採用並行組合，同時計算所有面向
對的交互？還是應該採用序列組合，按某種順序逐步融合面向？或者應該採用階層式
組合，先組合語義相關的面向子集再進行全局整合？這些根本性的設計選擇在現有文
獻中缺乏系統性討論，且沒有實證研究比較不同組合策略的效能差異。這構成了第二
個重要的研究缺口。
第三個關鍵缺口在於稀疏性約束的缺失。雖然選擇性融合的理念強調選擇而非全
面融合，但 Sun 等人的實現中並未引入顯式的稀疏性約束，其權重矩陣透過 softmax
歸一化僅保證權重和為 1，但並不鼓勵大部分權重接近零。這意味著模型仍可能學習到
相對均勻的權重分佈，所有特徵都在某種程度上參與融合，未能真正實現稀疏的選擇
性組合。在多面向情感分析中，稀疏性尤為重要，理論上大部分面向對之間應該是語
義獨立的，其特徵應保持獨立而不進行融合，僅有少數面向對存在顯著關聯需要資訊
交互。實現稀疏的組合模式不僅能減少不必要的資訊干擾提升模型效能，更重要的是
24
增強模型的可解釋性，透過檢視哪些面向對具有非零的組合權重，研究者可以深入理
解模型學習到的面向間關聯模式，這對於實務應用具有重要價值。然而，如何在選擇
性組合框架中引入稀疏性約束，是採用 L1L_1
L1 正則化、使用能產生稀疏輸出的激活函數如 ReLU，還是設計專門的稀疏門控機制，
目前尚無定論，這構成了第三個研究缺口。
綜合上述分析，雖然特徵組合與融合方法在多個領域取得了顯著進展，但針對多
面向情感分析任務的專門化設計仍處於空白狀態。現有方法提供了寶貴的技術元件與
設計理念，包括多層次資訊整合的必要性、自適應門控的靈活性、以及選擇性融合的
```
精準性，但如何將這些技術元件有機整合，設計一個既能處理多個(𝑁 > 2)面向同時組
```
合、又能學習稀疏的選擇性關聯模式、並且專門針對面向維度而非空間或模態維度的
組合機制，是亟待解決的核心問題。這正是本研究提出的漸進式多面向組合
```
(Progressive Multi-Aspect Composition， PMAC)模組的核心貢獻所在。PMAC 透過結
```
合門控機制的自適應性、選擇性融合的權重學習策略、以及針對多面向場景的專門化
設計，特別是引入稀疏性鼓勵機制與面向對面向的關聯建模，構建一個能夠學習面向
間選擇性組合模式的精細化策略，從而填補現有研究的缺口，為多面向情感分析提供
更有效的特徵組合解決方案。
2.3 面向間關係建模技術
2.3.1 圖神經網路方法
面向間關係建模的首要挑戰在於如何有效捕捉句子中面向詞彙與情感描述詞之間
的結構化關係。傳統的基於注意力機制的方法雖然能夠學習詞彙間的語義關聯，但往
往引入大量噪聲，特別是在包含多個面向的句子中，模型難以準確區分不同面向各自
對應的情感詞彙。例如在句子「Our waiter was friendly and it is a shame that
he didn't have a supportive staff」中，對於面向「waiter」而言，基於注意力的
模型可能同時為「friendly」與「supportive」分配較高的注意力權重，儘管
「supportive」實際上是修飾另一個面向「staff」，這種不精確的關聯判斷會嚴重影
響情感分類的準確性。為了克服這一局限，近年來研究者開始探索利用句法依存樹
```
(dependency tree)所提供的顯式結構資訊，透過圖神經網路(Graph Neural
```
```
Networks， GNNs)在依存樹構成的圖結構上進行資訊傳播與聚合，從而更精確地建模
```
面向與其相關情感詞彙之間的句法關係。
圖神經網路方法的核心優勢在於將句法分析工具提供的依存樹轉換為圖結構，利
用圖卷積操作沿著句法依存邊進行特徵聚合。Huang 等人於 2025 年提出的多視角注
```
意力句法增強圖卷積網路(Multi-View Attention Syntactic Enhanced Graph
```
```
Convolutional Network， MASGCN)系統性地探討了如何充分利用依存樹中的多類型句
```
法資訊來提升面向級情感分析的效能。該研究指出，依存樹包含三類關鍵的句法資訊:
```
樹的拓撲結構(topology)、依存關係類型(dependency types)以及詞彙間的最短路徑
```
```
距離(minimum distances)。早期的 GNN 方法如 ASGCN 僅利用依存樹的原始拓撲結
```
構，將依存樹的鄰接矩陣直接作為圖卷積的輸入，這種做法雖然能夠捕捉基本的句法
25
連接，但未能區分不同類型依存關係的語義差異，也未考慮詞彙間距離對資訊傳播的
影響。後續研究如 BiGCN 嘗試整合依存類型資訊，R-GAT 則引入關係感知的圖注意力
網路來處理不同的依存類型，而 SSEGCN 進一步結合最短路徑距離來構建多視角的圖
表示。然而，這些方法在融合多種句法資訊時，往往採用簡單的拼接或加權策略，未
能有效區分不同視角資訊的重要性，導致在引入更多視角的同時也引入了額外的噪
聲，限制了模型的表達能力。
```
為解決多視角資訊融合的挑戰，Huang 等人提出了多視角注意力機制(multi-view
```
```
attention mechanism)，該機制能夠為不同的句法視角自適應地分配權重，增強與目
```
標面向高度相關的視角，同時抑制弱相關視角引入的噪聲。具體而言，MASGCN 首先根
```
據依存樹中詞彙間的最短路徑距離構建多個距離遮罩矩陣(distance mask
```
```
matrices)，每個遮罩矩陣對應一個不同的距離閾值，允許模型在不同的語義範圍內進
```
行特徵聚合。例如，距離閾值為 1 的視角僅關注直接相鄰的詞彙，而距離閾值為 3
的視角則能夠捕捉更全域的語義關聯。對於每個視角，模型計算該視角下聚合特徵的
平均表示，並透過注意力機制學習各視角的權重，最終將所有視角的資訊按權重組
合。這種多視角注意力機制使得模型能夠動態調整對不同句法範圍的關注程度，在需
要局部精細特徵時聚焦於近距離視角，在需要全局語義理解時則更多依賴遠距離視
角，從而實現靈活且自適應的句法資訊利用。
除了距離資訊，依存關係類型同樣是重要的句法特徵。不同的依存類型如主謂關
```
係(nsubj)、賓語關係(dobj)、修飾關係(amod)等，在語義上具有顯著差異，直接影響
```
面向與情感詞彙間的關聯強度。為了更深入地利用依存類型資訊，Huang 等人引入結
```
構熵理論(structural entropy theory)來學習依存類型矩陣。結構熵理論最初用於圖
```
結構學習，其核心思想是透過最小化圖的結構熵來發現圖的最優分層表示。在 ABSA
任務中，依存類型本身提供了一種自然的詞彙分組方式，模型可以根據依存類型將詞
彙劃分為不同的子集，並計算這些子集對應的結構熵。透過在訓練過程中加入結構熵
損失項，模型能夠學習到一個優化的依存類型鄰接矩陣，該矩陣不僅反映了原始依存
樹的類型資訊，更經過結構熵最小化的約束，使得矩陣能夠更準確地捕捉詞彙間的語
義分組關係。實驗結果顯示，引入結構熵損失後，模型在四個基準資料集上均取得了
一致的效能提升，驗證了利用依存類型資訊進行顯式關係建模的有效性。
在效能評估方面，MASGCN 在 SemEval-2014 的 Restaurant 與 Laptop 資料集、
SemEval-2016 的 Restaurant 資料集，以及 Twitter 資料集上進行了全面實驗。與
多個基於 GNN 的基準模型相比，包括僅利用拓撲結構的 ASGCN、結合依存類型的 R-
GAT、以及採用多視角距離資訊的 SSEGCN，MASGCN 在準確率與宏平均 F1 分數上均達
到了最優表現。例如在 Restaurant14 資料集上，使用 GloVe 詞向量時，MASGCN 的
準確率相較於次優模型提升了 0.79%，宏平均 F1 提升了 0.99%。在 Laptop14 資料
集上，準確率提升 0.52%，F1 提升 0.62%。這些提升幅度雖然看似不大，但在 ABSA
這類精細任務中已屬顯著改進，且在多個資料集上的一致性表現證明了模型的穩健
性。此外，當使用預訓練語言模型 BERT 作為編碼器時，MASGCN+BERT 在大部分資料
集上仍優於其他結合 BERT 的 GNN 模型，顯示出 MASGCN 的句法資訊利用策略即使在
強大的預訓練表示基礎上仍能帶來額外的效能增益。
26
儘管圖神經網路方法在利用句法結構方面展現出顯著優勢，但其固有的局限性不
容忽視。首要問題在於對句法分析工具的依賴。GNN 方法的有效性建立在依存樹準確
性的前提之上，然而句法分析器在處理複雜句式、非正式語言或領域特定文本時容易
產生錯誤，這些錯誤會透過圖結構直接傳播至後續的特徵聚合過程，導致模型學習到
錯誤的句法關係。Huang 等人在其研究中亦承認此局限，指出未來工作需要探索如何
減少對外部句法工具的依賴，或發展更魯棒的機制來處理不完美的依存樹。其次，GNN
方法在計算效率上存在挑戰。依存樹的構建與多視角圖卷積的計算增加了模型的複雜
度，特別是當句子長度較長或視角數量較多時，計算成本會顯著上升。雖然多視角注
意力機制能夠透過權重分配來篩選重要視角，但仍需在模型設計階段預先定義視角數
量，缺乏自適應調整視角的能力。最後，GNN 方法主要聚焦於單一面向的情感分析，
對於包含多個面向的句子，現有研究多採用獨立處理各面向的策略，未能顯式建模面
向之間的交互關係。這種面向獨立的假設在多面向情感分析場景下可能導致次優結
果，因為某些面向的情感判斷可能受到其他面向的影響，例如在餐廳評論中，價格面
向的情感往往與食物品質面向相關聯。
綜合上述分析，圖神經網路方法透過利用依存樹的結構化資訊，為面向級情感分
析提供了一種強大的關係建模範式。Huang 等人的 MASGCN 工作系統性地探討了如何
充分利用依存樹中的拓撲、類型與距離等多維度句法資訊，透過多視角注意力機制與
結構熵理論實現了更精細化的句法資訊融合，在多個基準資料集上取得了最優效能。
然而，GNN 方法對句法分析工具的依賴、計算複雜度，以及面向獨立處理的限制，構
成了該方法的主要研究缺口。這些局限性為後續研究指明了方向，特別是如何發展不
依賴外部句法工具、能夠自適應選擇句法視角，並且能夠顯式建模多面向間交互關係
的方法，成為亟待解決的核心議題。此外，Huang 等人在研究限制中明確提出，未來
工作應探索整合外部知識圖譜或大型語言模型如 GPT-4 等資訊來源，以進一步提升模
型的語義理解能力，這一方向與本研究後續章節將探討的 Transformer 自注意力機制
具有密切關聯，為面向間關係建模的方法演進提供了承上啟下的過渡。
2.3.2 Transformer 自注意力機制
圖神經網路方法雖然透過利用句法依存樹為面向級情感分析提供了結構化的關係
建模能力，但其對外部句法分析工具的依賴以及面向獨立處理的範式，限制了模型在
多面向情感分析場景下的效能與靈活性。為了克服這些局限，近年來研究者開始探索
```
基於 Transformer 架構的自注意力機制(self-attention mechanism)，該機制無需依
```
賴外部句法工具，能夠直接從原始文本中學習詞彙間的語義關聯，並且天然支援並行
```
計算與可變長度序列處理。更重要的是，Transformer 的多頭自注意力(multi-head
```
```
self-attention)能夠在多個表示子空間中同時捕捉不同類型的語義關係，為多面向情
```
感分析中的面向間關係建模提供了一種更靈活且自適應的解決方案。Wu 等人於 2020
```
年提出的基於 Transformer 的多面向建模方法(Transformer-based Multi-aspect
```
```
Modeling， TMM)，系統性地探討了如何利用自注意力機制同時處理句子中的多個評價
```
面向，並透過學習面向間的潛在關聯來提升情感分類的準確性，為面向間關係建模研
究開闢了新的技術路徑。
27
TMM 方法的核心動機源於對現有 ABSA 方法處理範式的深刻反思。傳統的 ABSA
方法，無論是基於注意力機制還是基於圖神經網路，普遍採用「分離處理」的策略，
即將包含多個面向的句子拆分為多個獨立的實例，每次僅針對一個目標面向進行情感
分析。這種處理範式隱含了一個重要假設:每個面向的情感判斷是相互獨立的，僅需考
慮該面向與其上下文詞彙之間的局部關聯。然而，Wu 等人指出，這一假設在真實應用
```
場景中往往不成立，特別是在新發布的 Multi-Aspect Multi-Sentiment (MAMS) 資料
```
集中，每個句子都包含至少兩個具有不同情感極性的面向，這些面向之間可能存在複
雜的語義關聯與相互影響。例如在句子「The salmon is tasty while the waiter is
very rude」中，若僅關注面向「salmon」而忽略句子中同時存在的面向「waiter」，
模型可能錯誤地將「rude」這一負面詞彙的注意力權重分配給「salmon」，導致情感判
斷錯誤。相反，若模型能夠同時感知句子中的所有面向，並理解「salmon」與
「waiter」分別對應於不同的情感描述區域，則能更準確地為每個面向提取相關的情
感資訊。這一觀察促使 Wu 等人將 ABSA 任務重新形式化為「多面向情感分析」問
題，其目標不再是獨立預測單一面向的情感，而是同時檢測句子中所有面向的情感極
性，並在此過程中學習面向間的關聯模式。
為實現多面向同時建模，TMM 方法採用 Transformer 編碼器作為核心架構，利用
其多層自注意力機制來自動學習面向間的關係。具體而言，對於包含多個面向詞彙的
句子，TMM 引入兩個特殊標記 [AS] 與 [AE] 分別表示面向詞彙的起始與結束位置，
將原始句子轉換為包含面向邊界標記的序列。例如，句子「The salmon is tasty
while the waiter is very rude」中包含兩個面向詞彙「salmon」與「waiter」，經
轉換後形成序列「The [AS] salmon [AE] is tasty while the [AS] waiter [AE] is
very rude」。這一設計使得模型能夠明確識別句子中的多個面向位置，並在後續的自
注意力計算中同時考慮所有面向的表示。Wu 等人採用預訓練的 RoBERTa 模型作為骨
幹網路，RoBERTa 是 BERT 的優化版本，透過更大規模的語料庫、更長的訓練時間以
及動態遮罩策略，學習到更豐富的語言表示。在微調階段，模型將標記化的句子輸入
多層 Transformer 編碼器，每層編碼器包含多頭自注意力模組與前饋神經網路。多頭
自注意力機制的核心在於將輸入表示投影至多個子空間，每個注意力頭獨立計算查詢
```
(query)、鍵(key)與值(value)的相似度，並根據相似度權重聚合資訊。透過多頭設
```
計，模型能夠在不同的表示子空間中捕捉詞彙間的多樣化語義關係，某些頭可能關注
面向與情感詞彙的直接關聯，某些頭則可能學習面向間的相互影響模式。
Transformer 自注意力機制在多面向建模中的關鍵優勢在於其並行計算能力與全
局資訊聚合特性。相較於圖神經網路需要預先構建依存樹並沿著樹邊進行逐層傳播，
自注意力機制允許句子中的每個詞彙直接與所有其他詞彙進行交互，無需依賴外部句
法工具提供的結構資訊。這種全連接的注意力模式使得模型能夠自動發現詞彙間的長
距離依賴關係，即使面向詞彙與其情感描述詞在句子中相隔較遠，自注意力機制仍能
透過高權重連接來捕捉這種關聯。更重要的是，當句子包含多個面向時，自注意力機
制能夠同時計算所有面向表示之間的相互注意力，使得每個面向的表示不僅融合了來
自上下文詞彙的資訊，也隱含地編碼了其他面向的存在與影響。Wu 等人透過注意力權
重視覺化實驗證實了這一點，在案例研究中，對於句子「Food was OK – fish was
cooked well」包含兩個面向「Food」與「fish」，採用 TMM 方法的模型能夠為
28
「Food」正確分配較高的注意力權重至「OK」，為「fish」分配較高權重至「cooked
well」，兩個面向的注意力模式清晰區分且無交叉干擾。相反，採用傳統單面向處理方
式的 RoBERTa 基準模型，在僅給定面向「Food」時，錯誤地將高注意力權重分配給
「cooked well」，導致將中性的「Food」誤判為正面情感。這一視覺化結果直觀展示
了多面向同時建模的價值:透過讓模型同時感知所有面向，能夠避免單一面向分析時因
缺乏全局資訊而產生的注意力錯配問題。
在效能評估方面，Wu 等人在 MAMS 資料集上進行了全面實驗，該資料集專為多面
向多情感場景設計，確保每個句子至少包含兩個具有不同情感極性的面向，相較於傳
統 ABSA 資料集如 SemEval-2014，MAMS 資料集的複雜度顯著提升。實驗結果顯示，
```
TMM 方法在面向詞彙情感分析(ATSA)與面向類別情感分析(ACSA)兩個子任務上均取得
```
了顯著的效能提升。在 ATSA 任務中，RoBERTa-TMM 相較於單獨使用 RoBERTa 基準模
型，準確率提升了 1.93%，宏平均 F1 分數提升了 1.91%，達到 85.64% 的準確率與
85.08% 的 F1 分數。在 ACSA 任務中，提升幅度相對較小但仍具統計顯著性，準確率
提升 0.59%，F1 提升 0.50%。Wu 等人將 ACSA 任務的相對較小提升歸因於面向類別
的抽象性，預定義的面向類別如「food」、「service」在句子中可能未被明確提及，模
型需要推斷隱含的面向資訊，這增加了多面向建模的難度。儘管如此，在具有足夠資
料規模的 MAMS 資料集上，TMM 方法仍展現出一致的效能優勢，證明了 Transformer
自注意力機制在多面向情感分析中的有效性。此外，與多個非 Transformer 基準模型
的比較實驗顯示，即使是採用多跳注意力的 RAM 模型或多粒度注意力的 MGAN 模型，
其效能仍顯著低於 TMM 方法，驗證了預訓練語言模型結合多面向建模策略的優越性。
Transformer 自注意力機制相較於圖神經網路方法的另一重要優勢在於計算效率與靈
活性。圖神經網路方法需要為每個訓練樣本預先進行句法分析以構建依存樹，這不僅
增加了預處理的時間成本，更重要的是引入了額外的錯誤來源。句法分析器在處理口
語化、非正式或領域特定文本時容易產生錯誤，這些錯誤會直接影響後續圖結構的品
質，進而降低模型效能。相反，Transformer 方法直接在原始文本上進行訓練，無需
任何外部工具的介入，模型能夠端到端地學習從文本到情感標籤的映射。此外，
Transformer 的自注意力計算是高度並行化的，矩陣運算可以充分利用現代 GPU 的並
行計算能力，訓練與推理速度顯著快於需要逐層傳播的圖神經網路。TMM 方法的另一
個實務優勢在於其對面向數量的靈活性，由於自注意力機制天然支援可變長度序列，
模型能夠處理包含任意數量面向的句子，無需針對不同面向數量設計不同的架構或調
整超參數。Wu 等人的實驗設定涵蓋了包含 2 至 4 個面向的句子，模型在所有設定下
均展現穩定的效能，證明了方法的通用性。
儘管 Transformer 自注意力機制在多面向情感分析中展現出顯著優勢，但現有研
究仍存在若干局限性值得深入探討。首先，雖然 TMM 方法能夠同時處理多個面向並透
```
過自注意力隱含地學習面向間關聯，但其並未顯式建模面向對(aspect pair)之間的關
```
係。自注意力機制計算的是詞彙級別的相似度權重，面向間的關係需要從注意力權重
矩陣中間接推斷，缺乏針對面向維度的專門化設計。在某些應用場景中，面向間存在
明確的依賴關係或互補關係，例如在產品評論中，價格面向與品質面向往往存在權衡
關係，使用者對價格的情感判斷會受到品質評價的影響。現有的 Transformer 方法未
能顯式捕捉這種面向級別的高階交互模式，僅依賴詞級自注意力可能無法充分利用面
29
向間的結構化關係資訊。其次，TMM 方法雖然提出了多面向同時建模的範式，但其主
要目標仍是提升每個面向情感分類的準確性，對於如何利用面向間關聯來增強模型的
可解釋性與可控性，研究著墨較少。在實務應用中，使用者不僅關心情感分析的結
果，更希望理解模型為何做出特定判斷，哪些面向之間存在關聯，這些關聯如何影響
最終預測。缺乏顯式的面向關係建模使得模型的決策過程較為黑盒，限制了其在需要
高可解釋性場景中的應用。
第三，Transformer 方法對於訓練資料規模的依賴性較高。雖然預訓練語言模型
如 RoBERTa 已經在大規模語料上學習了通用語言表示，但在特定領域的多面向情感分
析任務上仍需要足夠的標註資料進行微調。MAMS 資料集包含約 11，000 個訓練樣
本，這一規模對於微調 Transformer 模型而言較為充足，但在許多實務場景中，標註
多面向情感資料的成本較高，資料稀缺性問題可能限制模型效能。相較之下，圖神經
網路方法雖然依賴句法工具，但其結構化的歸納偏置使得模型在小資料集上可能展現
更好的泛化能力。如何在資料有限的情況下充分發揮 Transformer 的優勢，或設計結
合兩者長處的混合架構，是未來研究的重要方向。最後，TMM 方法的多面向建模雖然
提升了情感分類準確性，但其對面向間關係的利用仍停留在隱式層面。模型透過自注
意力學習到的面向間關聯並未被進一步結構化或正則化，導致學習到的關係模式可能
缺乏一致性與穩健性。例如，在不同訓練批次或不同初始化條件下，模型學習到的面
向間注意力模式可能存在較大差異，這種不穩定性影響了模型的可靠性與可重現性。
綜合上述分析，Transformer 自注意力機制為多面向情感分析中的面向間關係建模提
供了一種強大且靈活的技術路徑。Wu 等人的 TMM 方法透過重新形式化 ABSA 任務，
提出同時處理多個面向的建模範式，利用 Transformer 的多頭自注意力機制自動學習
面向間的潛在關聯，在具有挑戰性的 MAMS 資料集上取得了顯著的效能提升。相較於
圖神經網路方法，Transformer 方法無需依賴外部句法工具，具備更高的計算效率與
靈活性，並且能夠透過預訓練語言模型獲得豐富的語義表示。然而，現有研究對於面
向間關係的建模仍停留在隱式層面，缺乏針對面向維度的顯式關係建模機制，同時在
模型可解釋性、資料效率以及關係學習的穩健性方面仍存在改進空間。這些局限性為
後續研究指明了方向，特別是如何在 Transformer 架構基礎上設計專門的面向關係建
模模組，顯式學習並利用面向對之間的高階交互模式，同時增強模型的可解釋性與資
料效率，成為多面向情感分析領域亟待解決的核心議題。此外，Wu 等人的研究為本論
文後續章節將探討的面向依賴關係建模提供了重要的理論基礎與實證證據，特別是其
多面向同時建模的範式與注意力視覺化分析，為理解面向間如何相互影響提供了直觀
的觀察視角，這些發現將在接下來的文獻探討中得到進一步深化與擴展。
2.3.3 面向依賴關係建模
前述章節探討的圖神經網路方法與 Transformer 自注意力機制，雖然在面向級情
感分析任務上取得了顯著進展，但它們主要關注如何提取與單一目標面向相關的上下
文資訊，或透過多面向同時處理來隱含地學習面向間的潛在關聯。然而，這些方法普
遍將句子中的多個面向視為相對獨立的分析單元，在進行情感分類時，每個面向的表
示主要依賴於其與上下文詞彙之間的語義關聯，缺乏對面向之間顯式依賴關係的系統
30
性建模。在真實應用場景中，特別是在包含多個評價面向的句子中，不同面向的情感
判斷往往相互影響，某一面向的情感極性可能受到其他面向的制約或誘導，這種面向
間的依賴關係若未被充分捕捉，將導致情感分析結果的次優化。Mazumder 等人於
```
2018 年首次系統性地提出面向間關係建模(Inter-Aspect Relation Modeling，
```
```
IARM)的概念，透過顯式建模目標面向與句子中其他面向之間的依賴關係，為多面向情
```
感分析提供了一種全新的技術視角，也為後續研究奠定了重要的理論基礎。
Mazumder 等人的研究動機源於對現有 ABSA 方法處理多面向場景時局限性的深刻
洞察。傳統方法在處理包含多個面向的句子時，採用將句子拆分為多個獨立實例的策
略，每次僅針對一個目標面向進行分析，完全忽略了同一句子中其他面向的存在與影
響。這種獨立處理的假設在許多情況下並不成立，特別是當句子包含連接詞
```
(conjunctions)如「and」、「but」、「however」、「though」等時，這些連接詞往往暗示
```
著面向之間存在特定的語義關係。例如在句子「Coffee is a better deal than
overpriced cosi sandwiches」中，包含兩個面向「coffee」與「cosi
sandwiches」，連接詞「than」明確建立了兩者之間的對比關係，「coffee」的正面情
```
感(「better deal」)與「cosi sandwiches」的負面情感(「overpriced」)相互映
```
襯。若模型在分析「coffee」時完全忽略「cosi sandwiches」的存在，可能無法充分
```
理解「better」一詞在對比語境下的強化語義;同樣地，在分析「cosi sandwiches」
```
時若不考慮與「coffee」的對比，可能低估「overpriced」的負面程度。更微妙的例
子是「The menu is very limited - I think we counted 4 or 5 entries」，這裡的
面向「entries」本身並未直接被情感詞彙修飾，但子句「I think we counted 4 or
```
5 entries」實際上是對前述面向「menu」的負面評價(「very limited」)的具體說明
```
與強化。若獨立分析「entries」，模型可能難以判斷其情感極性，但若考慮到
「menu」的負面情感以及兩者之間的從屬關係，則能準確推斷「entries」同樣應被標
註為負面。這些案例清晰展示了面向間依賴關係建模的必要性:某一面向的情感判斷需
要參考句子中其他面向的情感資訊與相互關係，才能達到準確的語義理解。
為實現面向間依賴關係的顯式建模，Mazumder 等人提出了一個包含多階段處理的
```
架構。該架構首先為每個面向生成面向感知的句子表示(aspect-aware sentence
```
```
representation)，這一步驟與傳統方法類似，透過將面向嵌入與句子詞彙嵌入拼接，
```
經過雙向 LSTM 編碼後，再利用注意力機制聚合與目標面向高度相關的上下文詞彙，
從而為每個面向獲得一個初步的表示向量。這些初步表示雖然已經融合了面向特定的
上下文資訊，但仍是基於獨立處理的範式，尚未考慮面向之間的相互影響。為引入面
向間的依賴建模，Mazumder 等人設計了一個關鍵的 Inter-Aspect Dependency
Modeling 模組，該模組接收所有面向的初步表示作為輸入，透過另一個 GRU 網路在
這些面向表示之間進行資訊傳播。具體而言，將 M 個面向的表示按順序排列形成序
列，GRU 逐步處理這一序列，每一步的隱藏狀態不僅編碼當前面向的資訊，也累積了
之前面向的資訊，從而使得後續面向的表示能夠隱含地受到先前面向的影響。這種序
列化的處理方式部分實現了面向間資訊的流動，但其侷限在於資訊傳播是單向的，僅
從前向後，且傳播的強度與方式由 GRU 的門控機制自動學習，缺乏針對特定面向對之
間關係的顯式建模。
31
```
為進一步增強面向間依賴的建模能力，Mazumder 等人引入記憶網路(Memory
```
```
Networks)作為架構的核心組件。記憶網路最初應用於問答任務，其核心思想是將相關
```
```
資訊儲存於記憶單元中，並透過多跳注意力機制(multi-hop attention)從記憶中反覆
```
提取與查詢最相關的資訊，逐步精煉答案表示。在面向間依賴建模的脈絡下，
Mazumder 等人將經過 GRU 處理的所有面向表示視為記憶單元，目標面向的初步表示
```
作為查詢(query)。記憶網路透過計算查詢與每個記憶單元之間的相似度，為記憶單元
```
```
分配注意力權重，並根據權重聚合記憶資訊生成輸出表示。這一過程可以重複多次(多
```
```
跳)，每一跳都基於上一跳的輸出作為新的查詢，進一步從記憶中提取資訊。透過這種
```
機制，目標面向的表示能夠自適應地關注與其高度相關的其他面向，例如在分析
「coffee」時，記憶網路可能為「cosi sandwiches」分配較高的注意力權重，從而將
對比關係隱含地編碼至「coffee」的最終表示中。多跳注意力的設計使得模型能夠進
行更深層次的推理，第一跳可能識別出直接相關的面向，第二跳則可能捕捉間接的關
聯或更複雜的依賴模式。Mazumder 等人在實驗中採用兩跳記憶網路，平衡了模型的表
達能力與計算效率。
在效能評估方面，Mazumder 等人在 SemEval-2014 任務的 Restaurant 與
Laptop 資料集上進行了全面實驗。這兩個資料集包含大量包含多個面向的句子，其中
Restaurant 資料集約有 40% 的訓練樣本包含兩個或更多面向，Laptop 資料集約有
30%，為驗證面向間依賴建模的有效性提供了合適的測試平台。實驗結果顯示，相較於
未考慮面向間關係的基準模型，引入 Inter-Aspect Dependency Modeling 模組後，
模型在包含多個面向的句子上取得了顯著的準確率提升。Mazumder 等人進行了詳細的
消融實驗，分別移除 GRU 序列處理與記憶網路組件，結果顯示兩者均對效能有正向貢
獻，且結合使用時效果最佳。特別值得注意的是，在包含連接詞的句子子集上，模型
的效能提升尤為明顯，驗證了研究假設:連接詞是面向間依賴關係的重要語言學標記，
顯式建模這些關係能夠更準確地捕捉面向間的語義互動。案例分析進一步展示了模型
的工作機制，透過視覺化注意力權重，研究者觀察到在分析某一目標面向時，模型確
實為語義相關的其他面向分配了較高的注意力，證明了面向間資訊交互的有效性。
儘管 Mazumder 等人的工作為面向間依賴關係建模開闢了重要的研究方向，但其方法
仍存在若干局限性。首先，GRU 序列處理的單向性限制了面向間資訊傳播的靈活性。
在實際應用中，面向在句子中的出現順序可能是隨機的，並不必然反映它們之間的依
賴方向，但 GRU 的序列化處理隱含地假設了從前到後的資訊流動，這可能不適用於所
有場景。例如，在句子「Although the service was poor， the food was
excellent」中，「service」出現在「food」之前，但從語義上講，「food」的正面情
感對於理解整個句子的轉折關係同樣重要，理想的模型應允許雙向的資訊流動。其
次，記憶網路雖然提供了靈活的注意力機制，但其對面向間關係的建模仍停留在隱式
層面。模型透過注意力權重自動學習哪些面向應該相互關注，但並未顯式定義或學習
```
面向對之間的關係類型(如對比、從屬、互補等)，這限制了模型的可解釋性與可控
```
性。在複雜場景中，不同類型的關係可能需要不同的處理策略，統一的注意力機制可
能無法充分捕捉這種異質性。
第三，Mazumder 等人的方法依賴於 GRU 與記憶網路的組合，這兩者在當前深度
學習研究中已逐漸被 Transformer 架構所取代。GRU 的序列化處理限制了並行計算能
32
力，且其記憶容量受隱藏狀態維度的制約，在處理包含大量面向的長句子時可能表現
不佳。記憶網路的多跳注意力雖然理論上能夠捕捉複雜的依賴關係，但實際上跳數的
選擇需要手動調優，且隨著跳數增加，計算成本與訓練難度也相應提升。相較之下，
Transformer 的自注意力機制天然支援全局的、並行的資訊交互，無需預設資訊傳播
的方向或層數，在理論上更適合建模面向間的複雜關係。第四，Mazumder 等人的實驗
主要聚焦於包含兩個面向的句子，對於包含三個或更多面向的複雜場景，模型的效能
```
與擴展性尚未得到充分驗證。隨著面向數量的增加，面向間的關係模式呈指數增長(N
```
```
個面向有 N(N-1)/2 個成對關係)，如何有效建模這些高階交互模式，同時保持計算效
```
率與訓練穩定性，是面向間依賴建模研究的重要挑戰。
最後，Mazumder 等人的工作雖然提出了面向間依賴建模的概念，但對於依賴關係
本身的性質與分類著墨較少。在真實評論中，面向間的關係可能呈現多樣化的模式:某
```
些面向存在對比關係(如價格與品質的權衡)、某些存在從屬關係(如總體評價與具體細
```
```
節)、某些則可能完全獨立。現有方法透過注意力機制隱含地學習這些關係，但缺乏對
```
關係類型的顯式識別與利用，這限制了模型在需要精細化關係理解的場景中的應用潛
力。未來研究可以探索如何結合語言學知識或資料驅動的方法，自動識別並分類面向
間的關係類型，並針對不同類型設計專門的建模策略，從而進一步提升面向間依賴建
模的精準性與可解釋性。
綜合上述分析，Mazumder 等人於 2018 年提出的面向間依賴關係建模工作，為多
面向情感分析領域開闢了一個重要的研究方向。該研究首次系統性地指出，在包含多
個面向的句子中，不同面向的情感判斷並非相互獨立，而是存在複雜的依賴與影響關
係，顯式建模這些關係能夠顯著提升情感分類的準確性。透過結合 GRU 序列處理與記
憶網路的多跳注意力機制，該方法實現了面向間資訊的有效傳播與聚合，在 SemEval-
2014 基準資料集上取得了顯著的效能提升，特別是在包含連接詞的多面向句子上表現
優異。然而，該方法在資訊傳播的雜向性、關係建模的顯式性、架構的現代性以及對
複雜場景的擴展性方面仍存在局限，這些局限性為後續研究提供了明確的改進方向。
特別是隨著 Transformer 架構在自然語言處理領域的廣泛應用，如何利用自注意力機
制的全局資訊交互能力，設計更靈活且高效的面向間關係建模方法，成為當前研究的
前沿議題。此外，如何顯式識別與利用面向間關係的類型資訊，如何在保持計算效率
的前提下處理包含大量面向的複雜場景，以及如何增強模型的可解釋性以支援實務應
用，均是亟待解決的核心問題。Mazumder 等人的開創性工作為這些後續研究奠定了堅
實的理論基礎，其提出的 Inter-Aspect Relation Modeling 概念已成為多面向情感
分析領域的重要研究範式，為本研究後續章節將提出的基於 Transformer 的面向間關
係建模方法提供了直接的啟發與對比基準。
2.3.4 小結與研究缺口
本節系統性地回顧了面向間關係建模技術的三大研究方向，分別探討了圖神經網
路方法、Transformer 自注意力機制以及面向依賴關係建模的理論基礎與技術演進。
這三個研究方向從不同角度嘗試解決多面向情感分析中的核心挑戰，各自展現出獨特
的優勢，但同時也暴露出現有研究在理論深度與技術實現上的系統性缺口。透過對這
33
些方法的批判性分析，本研究識別出三個相互關聯且亟待解決的研究缺口，這些缺口
共同構成了本研究提出 HMAC-Net 架構的理論基礎與創新動機。
首先，圖神經網路方法透過利用句法依存樹的結構化資訊，為面向級情感分析提
供了一種顯式的關係建模範式。Huang 等人提出的 MASGCN 模型系統性地探討了如何
充分利用依存樹中的拓撲、類型與距離等多維度句法資訊，透過多視角注意力機制與
結構熵理論實現了精細化的句法資訊融合，在多個基準資料集上達到了最優效能。圖
神經網路方法的核心優勢在於其能夠沿著句法依存邊進行精確的資訊傳播，明確建立
面向詞彙與情感描述詞之間的連接路徑，從而避免基於注意力機制方法中常見的噪聲
干擾問題。然而，圖神經網路方法的根本局限在於其對外部句法分析工具的強依賴，
句法分析器在處理複雜句式、非正式語言或領域特定文本時容易產生錯誤，這些錯誤
會透過圖結構直接傳播至後續處理階段，導致模型效能下降。更重要的是，圖神經網
路方法主要聚焦於面向與意見詞之間的關係建模，對於多個面向之間的交互關係缺乏
系統性設計，現有研究普遍採用獨立處理各面向的策略，未能充分利用面向間的語義
關聯資訊。
其次，Transformer 自注意力機制為多面向情感分析提供了一種更靈活且高效的
技術路徑。Wu 等人提出的 TMM 方法透過重新形式化 ABSA 任務，將其轉變為多面向
同時建模的問題，利用 Transformer 的多頭自注意力機制自動學習面向間的潛在關
聯，在具有挑戰性的 MAMS 資料集上取得了顯著的效能提升。Transformer 方法的關
鍵優勢在於其無需依賴外部句法工具，能夠端到端地從原始文本中學習詞彙間的語義
關係，並且透過並行計算實現高效的訓練與推理。多頭自注意力機制允許模型在多個
表示子空間中同時捕捉不同類型的語義關聯，為面向間關係的學習提供了豐富的表達
能力。然而，Transformer 方法對面向間關係的建模仍停留在隱式層面，自注意力機
制計算的是詞彙級別的相似度權重，面向間的關係需要從這些詞級權重中間接推斷，
缺乏針對面向維度的專門化設計。在實務應用中，面向間可能存在明確的依賴關係或
互補關係，例如價格面向與品質面向之間的權衡，僅依賴詞級自注意力可能無法充分
捕捉這種面向級別的高階交互模式。此外，Transformer 方法雖然能夠同時處理多個
面向，但對於如何選擇性地組合面向資訊，避免不相關面向間的干擾，現有研究缺乏
明確的機制設計。
第三，面向依賴關係建模研究為多面向情感分析領域開闢了一個重要的理論方
向。Mazumder 等人首次系統性地提出 Inter-Aspect Relation Modeling 的概念，明
確指出在包含多個面向的句子中，不同面向的情感判斷並非相互獨立，而是存在複雜
的依賴與影響關係。透過結合 GRU 序列處理與記憶網路的多跳注意力機制，該方法實
現了面向間資訊的有效傳播與聚合，特別是在包含連接詞的多面向句子上表現優異，
為面向間關係建模的必要性提供了實證支持。然而，該方法採用的 GRU 架構存在資訊
傳播單向性的問題，且記憶網路的多跳注意力雖然理論上能夠捕捉複雜依賴，但其對
面向間關係的建模仍是隱式的，未能顯式定義或學習面向對之間的關係類型。此外，
隨著深度學習技術的發展，GRU 與記憶網路已逐漸被 Transformer 架構所取代，如何
將面向依賴關係建模的理論洞察與現代 Transformer 架構相結合，成為當前研究的前
沿議題。
34
綜合上述三個研究方向的分析，本研究識別出多面向情感分析領域存在三個相互關聯
的核心研究缺口。第一個缺口在於缺乏針對多面向場景的選擇性組合機制。現有研究
在處理包含多個面向的句子時，主要採用兩種策略:一是獨立處理各面向，完全忽略面
```
向間的關係;二是透過注意力機制隱含地學習面向間關聯，但缺乏對組合過程的精細控
```
制。在多面向情感分析中，理論上大部分面向對之間應該是語義獨立的，其特徵應保
持獨立而不進行融合，僅有少數面向對存在顯著關聯需要資訊交互。現有方法無論是
完全獨立處理還是無差別融合，都未能實現這種選擇性的組合模式。缺乏選擇性組合
機制導致模型要么損失面向間的有效關聯資訊，要么引入不必要的噪聲干擾，限制了
模型在複雜多面向場景下的效能表現。此外，如何在選擇性組合框架中引入稀疏性約
束，使得模型能夠自動學習哪些面向對應該融合、哪些應該保持獨立，目前仍缺乏系
統性的方法論支撐。
第二個缺口在於面向間關係建模的顯式性不足。雖然 Mazumder 等人提出了面向
間依賴關係建模的概念，Wu 等人的 TMM 方法也透過多面向同時處理隱含地學習面向
間關聯，但這些方法對於面向間關係的建模仍停留在隱式層面，缺乏針對面向對之間
高階交互模式的顯式建模機制。在真實應用場景中，面向間的關係呈現多樣化的模
式，包括對比關係、從屬關係、互補關係等，這些不同類型的關係可能需要不同的處
理策略。現有方法透過統一的注意力機制處理所有面向對，未能區分關係的異質性，
也未能顯式學習並利用關係結構資訊。更重要的是，隱式的關係建模限制了模型的可
解釋性，實務應用者難以理解模型為何做出特定判斷，哪些面向之間的關係影響了最
終預測，這在需要高可解釋性的場景中構成顯著障礙。如何設計顯式的面向間關係建
模模組，既能夠學習面向對之間的高階交互模式，又能夠提供可解釋的關係結構，是
亟待解決的核心問題。
第三個缺口在於現有方法未能有機整合階層式語義建模、選擇性組合與關係建模
三個關鍵要素。圖神經網路方法雖然在句法層面提供了結構化建模，但其對預訓練語
言模型的利用不足，且面向間關係建模能力有限。Transformer 方法雖然利用了預訓
練模型的強大表示能力，但缺乏針對多面向場景的專門化設計，特別是在選擇性組合
與顯式關係建模方面存在空白。面向依賴關係建模研究雖然提出了重要的理論視角，
但其技術實現仍基於較舊的架構，未能充分發揮現代 Transformer 架構的優勢。這三
個問題相互關聯且共同影響多面向情感分析的整體效能，單獨優化其中一個方面難以
達到理想的整體效果。例如，即使擁有精確的面向感知注意力機制，若缺乏選擇性組
```
合策略，仍可能在多面向場景下產生資訊干擾;即使能夠選擇性地組合面向特徵，若未
```
能顯式建模面向間的依賴關係，仍可能遺漏重要的語義關聯。因此，亟需一個整合式
的架構，能夠協同處理階層式語義理解、選擇性特徵組合以及顯式關係建模三個關鍵
挑戰。
這三個研究缺口為本研究提出 HMAC-Net 模型提供了明確的理論動機與技術方
```
向。針對第一個缺口，本研究設計 PMAC(Progressive Multi-Aspect Composition)模
```
組，透過結合門控機制的自適應性與選擇性融合的權重學習策略，構建能夠學習面向
間選擇性組合模式的精細化機制，特別是引入稀疏性鼓勵機制，使模型能夠自動判斷
哪些面向對應該進行資訊交互，哪些應該保持獨立。針對第二個缺口，本研究提出
```
IARM(Inter-Aspect Relation Modeling)模組，採用 Transformer 編碼器架構，透過
```
35
多頭自注意力機制顯式建模面向間的高階交互關係，相較於 Mazumder 等人基於 GRU
的單向資訊傳播，Transformer 的全局自注意力允許任意面向對之間的雙向資訊流
動，更適合捕捉複雜的面向間依賴模式。針對第三個缺口，HMAC-Net 透過整合
```
AAHA(階層式面向感知注意力)、PMAC(漸進式多面向組合)與 IARM(面向間關係建模)三
```
個模組，構建一個統一的端到端架構，使得階層式語義理解、選擇性組合與關係建模
能夠協同優化，實現整體效能的提升。這三個模組並非孤立設計，而是相互支撐、層
層遞進:AAHA 為每個面向提取精確的上下文表示，PMAC 在此基礎上進行選擇性組合以
融合相關面向的資訊，IARM 則在組合後的表示上進一步建模面向間的高階關係，三者
共同構成一個完整的多面向情感分析解決方案。透過填補現有研究的這三個核心缺
口，本研究期望為多面向情感分析領域提供新的理論視角與技術貢獻，推動該領域向
更精細化、可解釋化與實用化的方向發展。
36
2.4 面向級情感分析基準模型
2.4.1 基於 LSTM 的序列建模方法
```
在面向級情感分析發展的早期階段，研究者開始探索利用長短期記憶網路(Long
```
```
Short-Term Memory， LSTM)的序列建模能力來處理此一任務。LSTM 作為循環神經網
```
路的一種特殊變體，因其能夠有效捕捉長距離依賴關係而在自然語言處理領域獲得廣
泛應用。然而，將 LSTM 直接應用於面向級情感分析時，研究者很快發現了一個根本
性問題:標準的 LSTM 模型以與面向無關的方式處理整個句子，對於同一句子中的不同
評價面向會產生相同的表示，這與 ABSA 任務需要針對特定面向進行情感判斷的本質
需求存在明顯衝突。
```
Tang 等人(2016)針對此一問題進行了系統性研究，提出了兩個融合面向資訊的
```
LSTM 變體模型。該研究首先驗證了標準 LSTM 模型在面向級情感分析任務中的局限
性。以句子「I bought a new camera. The picture quality is amazing but the
battery life is too short.」為例，當評價面向為「picture quality」時，情感極
```
性應為正面;而當面向為「battery life」時，情感極性應為負面。然而標準 LSTM 對
```
這兩個不同面向產生完全相同的句子表示，導致無法正確區分不同面向的情感極性。
實驗結果顯示，在 Twitter 基準資料集上，標準 LSTM 僅達到 66.5% 的準確率，這
一表現甚至低於某些基於特徵工程的傳統方法，證實了面向資訊對於此任務的關鍵
性。
```
為解決上述問題，Tang 等人(2016)提出了目標依賴長短期記憶網路(Target-
```
```
Dependent LSTM， TD-LSTM)。TD-LSTM 的核心設計思想是分別建模面向詞彙前後的上
```
下文資訊，從而生成面向特定的句子表示。具體而言，TD-LSTM 使用兩個獨立的 LSTM
```
網路:左側 LSTM(LSTM_L)處理面向詞彙之前的上下文加上面向詞彙本身，從左到右進
```
```
行序列建模;右側 LSTM(LSTM_R)處理面向詞彙之後的上下文加上面向詞彙本身，從右
```
到左進行序列建模。這種雙向處理策略使得模型能夠充分利用面向詞彙兩側的上下文
語義資訊。最終，TD-LSTM 將兩個 LSTM 的最後隱藏狀態向量進行拼接，形成綜合的
面向特定表示，並輸入 softmax 層進行情感極性分類。實驗結果顯示，TD-LSTM 在基
準資料集上達到 70.8% 的準確率和 69.0% 的 Macro-F1 分數，相較於標準 LSTM 有
顯著提升，驗證了整合面向資訊的有效性。
```
儘管 TD-LSTM 成功地將面向資訊納入模型，Tang 等人(2016)進一步指出該方法
```
仍存在不足:TD-LSTM 僅在序列分割層面考慮面向，但未能捕捉面向詞彙與各個上下文
詞彙之間的深層互動關係。基於此觀察，研究者提出了目標連接長短期記憶網路
```
(Target-Connection LSTM， TC-LSTM)，這是 TD-LSTM 的強化版本。TC-LSTM 的關鍵
```
創新在於引入了「面向連接」機制:在處理每個上下文詞彙時，模型不僅使用該詞彙的
詞嵌入，還將面向向量與詞嵌入進行拼接作為輸入。面向向量通過對面向詞彙序列的
詞嵌入進行平均獲得，這種做法已被證明在命名實體表示中簡單而有效。通過這種設
計，TC-LSTM 使得模型在建構句子表示的每一步都能顯式地考慮上下文詞彙與面向之
間的語義關聯性，從而生成更精確的面向特定表示。實驗結果顯示，TC-LSTM 達到
37
71.5% 的準確率和 69.5% 的 Macro-F1 分數，在所有基準方法中取得最佳效能，且無
需依賴句法解析器或外部情感詞典等額外資源。
```
Tang 等人(2016)的研究通過案例分析進一步揭示了面向資訊的重要性。研究發
```
現，標準 LSTM 傾向於為整個句子分配統一的情感極性而忽略評價面向的存在。例
如，在句子「Hey google， thanks for all these great Labs features on
Chromium， but how about 'Create Application Shortcut'?!」中，整體句子表達
中性情感，但針對面向「google」應判斷為正面情感。標準 LSTM 將其錯誤分類為中
性，而 TD-LSTM 和 TC-LSTM 均能正確識別面向特定的正面情感，展現了面向導向序
列建模的優勢。此外，研究者也發現這兩個模型的主要錯誤來源集中在中性類別，有
85.4% 的誤分類樣本涉及中性情感，這反映了中性情感判斷的內在困難性，為後續研
究指出了改進方向。
儘管 TD-LSTM 和 TC-LSTM 在當時取得了最先進的效能，這些基於 LSTM 的序列
建模方法仍存在三個本質性的局限。首先，這些方法採用簡單的向量拼接或平均策略
來整合雙向上下文資訊，缺乏更精細的資訊融合機制，無法動態調整不同上下文片段
的重要性權重。其次，面向詞彙的表示方式過於簡化，僅通過平均詞嵌入來表示多詞
面向，未能充分捕捉面向本身的內部語義結構與完整語義。第三，模型未能建模句子
中多個面向之間的潛在關聯與相互影響，在處理包含多個評價面向的複雜句子時，無
法捕捉面向間的語義關係與情感對比模式。這些局限性為後續研究引入注意力機制、
改進面向表示方法、以及開發多面向關係建模技術提供了明確的改進方向，推動了面
向級情感分析方法的持續演進。
2.4.2 基於注意力機制的方法
儘管基於 LSTM 的序列建模方法通過融合面向資訊取得了顯著進展，研究者逐漸
認識到這些方法仍存在一個根本性的不足:它們主要聚焦於如何利用面向資訊來指導上
下文表示的生成，卻忽略了面向本身也需要精細建模的事實。在實際的情感分析場景
中，面向詞彙與上下文詞彙之間存在雙向的語義關聯與相互影響，單向的注意力機制
難以充分捕捉這種互動關係。基於此觀察，研究者開始探索如何通過注意力機制實現
面向與上下文的互動式學習，推動 ABSA 方法向更精細化的方向發展。
```
Ma 等人(2017)針對現有方法的局限性提出了互動注意力網路(Interactive Attention
```
```
Networks， IAN)。該研究首先指出了先前方法的兩個關鍵問題，為 IAN 的設計提供
```
了理論依據。第一個問題是面向表示的簡化處理。以往研究通常將面向視為輔助資
訊，僅通過簡單的向量平均或嵌入查找來表示面向，未能充分建模面向詞彙之間的語
義組合關係。以「picture quality」為例，該面向由兩個詞彙組成，其中
「picture」在表示整個面向時扮演更重要的角色，但簡單的平均策略無法捕捉這種內
部結構差異。第二個問題是面向與上下文的單向互動。現有方法雖然利用面向資訊來
計算上下文的注意力權重，但這種互動是單向的，面向的表示學習過程並未受到上下
文資訊的監督與影響。然而在真實場景中，面向詞彙「short」在與「battery life」
搭配時傾向於表達負面情感，而與「spoon」搭配時則相對中性，這種語義差異源於上
下文對面向理解的影響，單向注意力機制難以捕捉此類現象。
38
```
基於上述兩點洞察，Ma 等人(2017)提出 IAN 模型，其核心設計理念是將面向與
```
上下文視為對等的建模對象，通過雙向互動的注意力機制分別學習它們的表示。IAN
的架構由兩個平行且互動的 LSTM 注意力網路組成，分別負責處理面向和上下文。具
體而言，模型首先使用兩個獨立的 LSTM 網路對面向詞彙序列和上下文詞彙序列進行
編碼，獲得各自的隱藏狀態表示。對於包含 m 個詞的面向和包含 n 個詞的上下文，
LSTM 分別生成隱藏狀態序列[ℎ1ᵗ，ℎ2ᵗ， … ，ℎₘᵗ]和[ℎ1ᶜ，ℎ2ᶜ， … ，ℎₙᶜ]。接著，模型
通過平均池化操作獲得面向與上下文的初始表示𝑡𝑎𝑣𝑔和𝑐𝑎𝑣𝑔，這兩個初始表示將作為
後續注意力計算的監督信號。IAN 的創新之處在於互動式注意力計算機制:當計算上下
文詞彙的注意力權重時，模型使用面向的初始表示 t_avg 作為查詢向量，通過注意力
```
函數𝛼𝑖 = 𝑒𝑥𝑝 (𝑠𝑐𝑜𝑟𝑒(ℎ𝑖ᶜ，𝑡𝑎𝑣𝑔)) /Σ𝑒𝑥𝑝 (𝑠𝑐𝑜𝑟𝑒(ℎ𝑗 ᶜ，𝑡𝑎𝑣𝑔)) 為每個上下文詞彙分配權
```
```
重;對稱地，當計算面向詞彙的注意力權重時，模型使用上下文的初始表示𝑐𝑎𝑣𝑔 作為
```
```
查詢向量，通過 𝛽𝑖 = 𝑒𝑥𝑝 (𝑠𝑐𝑜𝑟𝑒(ℎ𝑖ᵗ，𝑐𝑎𝑣𝑔)) /Σ𝑒𝑥𝑝 (𝑠𝑐𝑜𝑟𝑒(ℎ𝑗 ᵗ，𝑐𝑎𝑣𝑔))為每個面向詞
```
彙分配權重。這種設計使得面向與上下文能夠相互影響對方的表示生成過程，實現真
正的雙向互動學習。最終，模型將加權後的面向表示𝑡𝑟和上下文表示𝑐𝑟進行拼接，形成
綜合的句子表示輸入 softmax 層進行情感分類。
```
Ma 等人(2017)在 SemEval 2014 資料集上進行了系統性實驗評估。實驗結果顯
```
示，IAN 在 Restaurant 資料集上達到 78.6% 的準確率，在 Laptop 資料集上達到
72.1% 的準確率，在所有基準方法中取得最佳效能。與先前最佳的 ATAE-LSTM 模型相
比，IAN 分別帶來了 1.4% 和 3.2% 的準確率提升。值得注意的是，在學術界普遍認
為情感分類任務中提升 1% 準確率都極為困難的背景下，這樣的改進幅度展現了互動
注意力機制的顯著價值。研究者通過消融實驗進一步驗證了各個設計組件的貢獻。實
驗設計了三個變體模型:No-Target 模型完全忽略面向建模，僅使用上下文表示進行分
```
類;No-Interaction 模型雖然分別建模面向和上下文，但兩者之間無任何互
```
```
動;Target2Content 模型僅實現單向互動，即僅使用面向資訊來指導上下文注意力計
```
算。實驗結果顯示，No-Interaction 模型的效能甚至低於 No-Target 模型，這反映
出如果面向表示的學習缺乏上下文監督，其質量可能無法為分類任務提供有效資訊。
Target2Content 模型優於 No-Interaction 但遜於 IAN，證實了雙向互動機制相較於
單向互動的優越性。
研究者通過案例分析進一步展示了 IAN 的注意力分佈特性。以句子「the fish
is fresh but the variety of fish is nothing out of ordinary」為例，當面向為
```
「fish」時，模型在上下文中對「fresh」賦予較高權重;而當面向為「variety of
```
fish」時，模型則更關注「nothing」、「out」、「ordinary」等詞彙。更有趣的是，在
面向「variety of fish」的內部，模型對中心詞「variety」的注意力顯著高於修飾
詞「of」和「fish」，展現了 IAN 能夠自動識別面向內部的重要性差異。此外，研究
```
者發現 IAN 在 Laptop 資料集上的改進幅度(3.2%)明顯大於 Restaurant 資料集
```
```
(1.4%)，這與兩個資料集的面向長度分佈特徵密切相關。統計顯示，Laptop 資料集中
```
多詞面向的比例顯著高於 Restaurant 資料集，而 LSTM 與互動注意力機制在建模長
39
面向時相較於簡單的平均或池化策略更具優勢，因此在多詞面向占比更高的資料集上
獲得更大的效能提升。
儘管 IAN 在當時取得了最先進的效能，基於 LSTM 與注意力機制的方法仍面臨三
個本質性局限，這些不足限制了模型在更複雜場景中的表現，為後續研究指明了改進
方向。首先，LSTM 的序列建模特性雖然能夠捕捉上下文依賴關係，但其循環結構導致
訓練效率較低，且在處理長距離依賴時仍可能面臨梯度消失問題，限制了模型對全局
語義資訊的利用能力。其次，IAN 及其同時期方法所使用的詞嵌入主要基於 Word2Vec
或 GloVe 等靜態詞向量，這些預訓練表示缺乏上下文適應性，無法根據具體語境動態
調整詞彙語義，例如「bank」在金融語境與河岸語境中的語義差異無法通過靜態嵌入
有效區分。第三，注意力機制雖然實現了面向與上下文的互動，但這種互動仍局限於
特徵層面的加權組合，未能充分利用大規模語料中蘊含的深層語言知識與語義關聯模
式。這些局限性促使研究者開始探索基於預訓練語言模型的新方法，期望通過深度雙
向編碼器與大規模預訓練策略來突破現有方法的效能瓶頸，開啟 ABSA 研究的新階
段。
2.4.3 基於預訓練語言模型的方法
儘管基於注意力機制的方法在 ABSA 任務上取得了顯著進展，這些方法仍受制於
詞嵌入表示的固有局限。Word2Vec 與 GloVe 等靜態詞向量無法捕捉詞彙在不同語境
下的語義變化，例如「bank」在金融語境與河岸語境中具有完全不同的含義，但靜態
嵌入僅能提供單一的向量表示。此外，這些方法主要依賴任務特定資料集進行訓練，
```
訓練語料的規模限制了模型學習豐富語言知識的能力。2018 年 BERT(Bidirectional
```
```
Encoder Representations from Transformers)的提出為自然語言處理領域帶來了革
```
命性變革，其通過在大規模語料上進行預訓練獲得的深層雙向語言表示，在問答與自
然語言推理等任務上展現出卓越效能。然而，將預訓練 BERT 模型直接應用於 ABSA
任務時，研究者發現效能提升並不顯著，這促使學者探索如何有效地將 BERT 的優勢
遷移到面向級情感分析場景。
```
Sun 等人(2019)針對 BERT 在 ABSA 任務上的應用困境提出了創新性解決方案。
```
```
該研究的核心洞察在於:BERT 在問答(QA)與自然語言推理(NLI)等句對分類任務上表現
```
```
卓越，其優勢源於預訓練階段的遮罩語言模型(Masked Language Model)與下一句預測
```
```
(Next Sentence Prediction)任務，這兩個預訓練目標使 BERT 對句對關係的建模能
```
力特別突出。然而 ABSA 傳統上被視為單句分類問題，直接對預訓練 BERT 進行微調
無法充分發揮其在句對處理上的優勢。基於此觀察，研究者提出通過構造輔助句子
```
(Auxiliary Sentence)將 ABSA 轉換為句對分類任務，從而使任務形式與 BERT 的預
```
訓練目標相匹配，釋放預訓練模型的潛力。
```
Sun 等人(2019)設計了四種輔助句子構造策略，分別模擬問答與自然語言推理的
```
句對形式，並探討是否在輔助句子中編碼情感極性標籤資訊。第一種策略為 QA-
```
M(Question Answering without polarity Matching)，該方法將面向資訊轉換為問句
```
形式。以面向「safety」為例，構造的輔助句子為「what do you think of the
safety of location - 1?」，原始評論句子與此問句組成句對輸入 BERT，模型需要從
40
```
句對中推斷情感極性，輸出為三類標籤{positive， negative， none}。第二種策略
```
```
為 NLI-M(Natural Language Inference without polarity Matching)，該方法構造
```
更簡潔的偽句子形式。對於相同面向，輔助句子簡化為「location - 1 - safety」，
這種設計降低了輔助句子的結構約束，使模型更靈活地學習面向與上下文的語義關
```
聯。第三種策略為 QA-B(Question Answering with Binary classification)，該方
```
法在輔助句子中顯式編碼情感標籤資訊，將三分類問題轉換為二元判斷。對於面向
「safety」，模型需要分別判斷三個陳述句的真偽:「the polarity of the aspect
safety of location - 1 is positive」、「the polarity of the aspect safety of
location - 1 is negative」、「the polarity of the aspect safety of location -
1 is none」。模型對每個陳述句輸出 yes/no 的機率分佈，最終選擇 yes 機率最高的
```
陳述句所對應的情感類別作為預測結果。第四種策略為 NLI-B(Natural Language
```
```
Inference with Binary classification)，其邏輯與 QA-B 相同，但將問句形式替換
```
為偽句子，例如「location - 1 - safety - positive」。這四種策略系統性地探索了
問答與推理兩種句對範式以及隱式與顯式標籤建模兩個維度的組合空間。
```
Sun 等人(2019)在 SemEval-2014 Task 4 資料集上進行了系統性實驗評估，該資
```
```
料集包含子任務 3(面向類別檢測)與子任務 4(面向類別極性分類)。實驗使用預訓練的
```
BERT-base 模型進行微調，該模型包含 12 層 Transformer，隱藏層維度 768，自注意
力頭數 12，總參數量 1.1 億。微調時保持 dropout 機率 0.1，訓練輪數 4，學習率
2e-5，批次大小 24。實驗結果顯示，BERT 句對分類方法相較於之前的最佳方法取得了
顯著提升。在子任務 4 的面向極性分類中，BERT-pair-QA-B 在 4 分類設定下達到
85.9% 的準確率，在 3 分類設定下達到 89.9% 的準確率，在二分類設定下達到 95.6%
```
的準確率。相較於先前最佳的 ATAE-LSTM 模型(3 分類準確率 84.0%)，BERT-pair-QA-
```
B 帶來了 5.9 個百分點的絕對提升，這在情感分析任務中是極為顯著的改進幅度。值
得注意的是，直接對 BERT 進行單句分類微調的 BERT-single 模型在 3 分類設定下僅
達到 86.9% 的準確率，雖然優於 ATAE-LSTM，但遠不及 BERT-pair 方法，這驗證了
輔助句子構造策略的關鍵作用。在子任務 3 的面向檢測中，BERT-pair-NLI-B 達到
```
92.18% 的 F1 分數，相較於之前最佳的 NRC-Canada 系統(88.58% F1)提升了 3.6 個
```
百分點。
研究者通過深入分析揭示了 BERT-pair 方法優越性的兩個根本原因。第一個原因
是資料擴增效應。輔助句子的構造實質上將原始資料集進行了指數級擴充:原始資料集
```
中的單一句子 s_i 在句對分類框架下被擴展為(𝑠𝑖，𝑡1，𝑎1)， … ，(𝑠𝑖，𝑡1，𝑎𝑛)， … ，
```
```
(𝑠𝑖，𝑡𝑚，𝑎𝑛) 多個訓練樣本，其中 t 表示目標實體，a 表示面向。這種擴充不僅增
```
加了訓練樣本數量，更重要的是使模型能夠學習同一句子在不同面向下的情感差異，
強化了面向特定的情感判斷能力。第二個原因是預訓練任務的匹配效應。BERT 的遮罩
語言模型預訓練任務要求模型根據上下文預測被遮罩的詞彙，這培養了模型對詞彙間
```
深層語義關聯的理解能力;下一句預測任務則訓練模型判斷兩個句子是否在語義上連
```
貫，這與句對分類的任務形式高度一致。當 ABSA 被轉換為句對分類後，預訓練階段
習得的這些能力能夠被直接遷移並發揮作用，而單句分類形式則無法充分利用這些預
訓練知識。此外，研究者觀察到不同輔助句子構造策略在不同子任務上的效能差
異:BERT-pair-NLI 模型在面向檢測任務上表現相對更佳，而 BERT-pair-QA 模型在情
41
感分類任務上更具優勢，這反映了問答形式與推理形式對不同任務類型的適配程度差
異。
儘管基於 BERT 的方法在 ABSA 任務上取得了突破性進展，該方法仍面臨三個值
得後續研究探索的方向。首先，輔助句子的構造策略高度依賴人工設計，四種構造方
法的性能差異表明存在進一步優化的空間，如何自動學習最優的輔助句子形式是一個
有價值的研究問題。其次，BERT-pair 方法通過將單一句子擴展為多個句對樣本實現
了資料擴增，但這也顯著增加了推理階段的計算成本，對於包含 n 個面向的句子，模
型需要進行 n 次前向傳播，這在實際應用場景中可能造成效率瓶頸，探索更高效的批
次化推理策略具有實用價值。第三，該方法主要聚焦於如何有效利用預訓練 BERT 模
```
型，但未深入探討不同規模 BERT 模型(BERT-base 與 BERT-large)以及不同預訓練策
```
```
略(如 RoBERTa 的動態遮罩)對 ABSA 任務的影響，系統性地比較不同預訓練模型的遷
```
移效能將為模型選擇提供更明確的指導。這些局限性與開放問題為後續研究指明了方
向，推動基於預訓練語言模型的 ABSA 方法向更高效、更通用的方向發展，同時也為
本研究提出的 HMAC-Net 模型奠定了技術基礎與比較基準。
2.4.4 小結與本研究定位
本節系統性回顧了面向級情感分析領域中三個關鍵研究方向的代表性工作，從基
於注意力機制的方法、到基於預訓練語言模型的突破、再到基於 Transformer 自注意
力的多面向建模，展現了 ABSA 領域從單面向處理向多面向協同建模演進的重要軌跡。
這一發展歷程揭示了三個核心技術洞察，共同構成了現代面向級情感分析方法論的理
論基礎，同時也指出了面向級情感分析領域中亟待突破的研究缺口，為本研究提出
HMAC-Net 整合式架構提供了明確的理論動機與技術方向。
```
首先，面向資訊的精細化建模對於提升情感分類準確性至關重要。Ma 等人(2017)
```
提出的 IAN 模型開創性地將面向與上下文視為對等的建模對象，透過雙向互動的注意
力機制實現了面向特徵的動態學習。該研究證明，面向本身並非簡單的靜態標籤，而
是包含豐富語義資訊的結構化表示，需要通過 LSTM 與互動注意力機制進行精細建模。
實驗結果顯示，相較於僅使用上下文注意力的 ATAE-LSTM 模型，IAN 在 Restaurant 與
Laptop 資料集上分別帶來 1.4%與 3.2%的準確率提升，特別是在多詞面向占比更高的
Laptop 資料集上改進幅度更為顯著，這證明了互動式面向建模的必要性。然而，IAN
及其同時期的基於 LSTM 的方法仍存在三個本質性局限:循環結構導致的訓練效率低
下、靜態詞嵌入缺乏上下文適應性、以及注意力機制僅停留在特徵層面的加權組合，
未能充分利用大規模預訓練語言模型蘊含的深層語言知識。
其次，預訓練語言模型的引入為 ABSA 任務帶來了革命性的效能躍升，但如何有效
```
遷移預訓練知識仍是關鍵挑戰。Sun 等人(2019)針對直接微調 BERT 在 ABSA 任務上效能
```
提升有限的問題，提出了創新性的輔助句子構造策略，將 ABSA 轉換為句對分類任務，
```
使得任務形式與 BERT 的預訓練目標(遮罩語言模型與下一句預測)相匹配。該研究設計
```
```
了四種輔助句子構造方式(QA-M、NLI-M、QA-B、NLI-B)，系統性探索了問答與推理兩
```
種句對範式以及隱式與顯式標籤建模兩個維度的組合空間。實驗結果極具說服力:在
SemEval-2014 Task 4 的面向極性分類子任務中，BERT-pair-QA-B 達到 89.9%的準確
42
```
率，相較於先前最佳的 ATAE-LSTM 模型(84.0%)帶來 5.9 個百分點的絕對提升;在面向
```
檢測子任務中，BERT-pair-NLI-B 達到 92.18%的 F1 分數，相較於 NRC-Canada 系統
```
(88.58%)提升 3.6 個百分點。研究者深入分析揭示了兩個關鍵成功因素:資料擴增效應
```
```
使得單一句子被擴展為多個面向特定的訓練樣本，強化了模型的面向判斷能力;預訓練
```
任務匹配效應使得 BERT 在句對分類上的優勢得以充分發揮。然而，該方法仍面臨三個
值得後續研究探索的方向:輔助句子的構造策略高度依賴人工設計，如何自動學習最優
```
的輔助句子形式是一個有價值的研究問題;BERT-pair 方法通過將單一句子擴展為多個
```
句對樣本實現了資料擴增，但這也顯著增加了推理階段的計算成本，對於包含 n 個面
向的句子，模型需要進行 n 次前向傳播，這在實際應用場景中可能造成效率瓶頸，探
```
索更高效的批次化推理策略具有實用價值;該方法主要聚焦於如何有效利用預訓練 BERT
```
```
模型，但未深入探討不同規模 BERT 模型(BERT-base 與 BERT-large)以及不同預訓練策
```
```
略(如 RoBERTa 的動態遮罩)對 ABSA 任務的影響，系統性地比較不同預訓練模型的遷移
```
效能將為模型選擇提供更明確的指導。
第三，多面向同時建模相較於單面向獨立處理能夠更好地捕捉面向間的語義關
聯，但現有方法對於面向間關係的建模仍停留在隱式層面。Wu 等人透過重新形式化
ABSA 任務，提出同時處理多個面向的 TMM 方法，利用 Transformer 的多頭自注意力機
制自動學習面向間的潛在關聯。該研究的核心動機源於對傳統 ABSA 處理範式的深刻反
思:將包含多個面向的句子拆分為多個獨立實例進行分析，隱含地假設了各面向的情感
判斷相互獨立，但在真實應用場景中，特別是在 MAMS 資料集這類多面向多情感場景
中，面向間往往存在複雜的語義關聯與相互影響。TMM 方法採用預訓練的 RoBERTa 模型
作為骨幹網路，通過引入特殊標記[AS]與[AE]明確標識句子中的多個面向位置，使得
Transformer 的多頭自注意力機制能夠在詞彙級別自動學習面向間的語義關聯。實驗結
果顯示，在 MAMS 資料集的 ATSA 任務中，RoBERTa-TMM 相較於單獨使用 RoBERTa 基準模
型，準確率提升 1.93%，宏平均 F1 分數提升 1.91%，達到 85.64%的準確率與 85.08%的
F1 分數。注意力視覺化分析進一步證明，多面向同時建模能夠避免單面向分析時因缺
乏全局資訊而產生的注意力錯配問題，例如在包含"Food"與"fish"兩個面向的句子
中，模型能夠正確為不同面向分配與其語義相關的上下文詞彙權重，避免相互干擾。
然而，TMM 方法雖然能夠透過自注意力隱含地學習面向間關聯，但並未顯式建模面向對
之間的關係。自注意力機制計算的是詞彙級別的相似度權重，面向間的關係需要從注
意力權重矩陣中間接推斷，缺乏針對面向維度的專門化設計。在某些應用場景中，面
向間存在明確的依賴關係或互補關係，例如在產品評論中，價格面向與品質面向往往
存在權衡關係，使用者對價格的情感判斷會受到品質評價的影響，現有的 Transformer
方法未能顯式捕捉這種面向級別的高階交互模式。
綜合而言，本研究透過提出 HMAC-Net 整合式架構，實現三個協同運作的創新模
組，並進行系統性的實驗驗證，期望在理論、方法與實證三個層面為多面向情感分析
領域做出貢獻，推動該領域向更精確、更可解釋的方向發展。
參考文獻
英文文獻
➢ Rodríguez-Ibáñez, M., Casañez-Ventura, A., Castejón-Mateos, F., & Cuenca-Jiménez, P.
M. (2023). A review on sentiment analysis from social media platforms. Expert Systems
with Applications, 223, 119862. https://doi.org/10.1016/j.eswa.2023.119862
```
➢ Tiwari, D., Nagpal, B., Bhati, B. S., Mishra, A., & Kumar, M. (2023). A systematic review of
```
social network sentiment analysis with comparative study of ensemble-based
```
techniques. Artificial Intelligence Review, 56(11), 13407–13461.
```
```
https://doi.org/10.1007/s10462-023-10472-w
```
```
➢ Zhang, W., Li, X., Deng, Y., Bing, L., & Lam, W. (2022). A survey on aspect-based
```
sentiment analysis: Tasks, methods, and challenges. IEEE Transactions on Affective
Computing. https://arxiv.org/abs/2203.01054
```
➢ Cheng, Y., et al. (2025). Aspect-based sentiment analysis methods. Neural Networks,
```
192, 107864.
```
➢ Wang, Y., Huang, M., Zhao, L., & Zhu, X. (2016). Attention-based LSTM for aspect-level
```
sentiment classification. In Proceedings of the 2016 Conference on Empirical Methods in
```
Natural Language Processing (pp. 606-615).
```
```
➢ Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2016). Hierarchical attention
```
networks for document classification. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
```
Language Technologies (pp. 1480-1489).
```
```
➢ Cheng, Y., et al. (2021). Multi-granularity graph convolutional network for aspect-based
```
sentiment analysis. Neural Networks, 192, 107864.
```
➢ Wang, L., Peng, J., Zheng, C., Zhao, T., & Zhu, L. (2024). A cross modal hierarchical fusion
```
multimodal sentiment analysis method based on multi-task learning. Information
```
Processing and Management, 61(2), 103675.
```
```
https://doi.org/10.1016/j.ipm.2024.103675
```
```
➢ Deng, J., Cheng, L., & Wang, Z. (2021). Attention-based BiLSTM fused CNN with gating
```
mechanism model for Chinese long text classification. Computer Speech & Language,
68, 101182. https://doi.org/10.1016/j.csl.2020.101182
```
➢ Sun, F., Yuan, X., & Zhao, C. (2023). Selective feature fusion network for salient object
```
```
detection. IET Computer Vision, 17(4), 483–495. https://doi.org/10.1049/cvi2.12183
```
```
➢ Huang, X., Peng, H., Sun, S., Hao, Z., Lin, H., & Wang, S. (2025). Multi-view attention
```
syntactic enhanced graph convolutional network for aspect-based sentiment analysis.
arXiv preprint arXiv:2501.15968. https://arxiv.org/abs/2501.15968
```
➢ Wu, Z., Ying, C., Dai, X., Huang, S., & Chen, J. (2020). Transformer-based multi-aspect
```
modeling for multi-aspect multi-sentiment analysis. arXiv preprint arXiv:2011.00476.
```
https://arxiv.org/abs/2011.00476
```
```
➢ Majumder, N., Poria, S., Gelbukh, A., Akhtar, M. S., Cambria, E., & Ekbal, A. (2018).
```
```
IARM: Inter-aspect relation modeling with memory networks in aspect-based sentiment
```
analysis. In Proceedings of the 2018 Conference on Empirical Methods in Natural
```
Language Processing (pp. 3402-3411). Association for Computational Linguistics.
```
```
https://doi.org/10.18653/v1/D18-1377
```
```
➢ Ma, D., Li, S., Zhang, X., & Wang, H. (2017). Interactive attention networks for aspect-level
```
sentiment classification. Proceedings of the Twenty-Sixth International Joint Conference on
```
Artificial Intelligence (IJCAI-17), 4068-4074.
```
```
➢ Sun, C., Huang, L., & Qiu, X. (2019). Utilizing BERT for aspect-based sentiment analysis via
```
constructing auxiliary sentence. Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies
```
(NAACL-HLT 2019), 380-385. https://doi.org/10.18653/v1/N19-1035
```
```
➢ Wu, Z., Ong, D. C., & Liang, J. (2020). Multi-aspect sentiment analysis with aspect-specific
```
context positions. arXiv preprint arXiv:2010.07406.
網路資源
-
GAI 工具接露
使用工具
```
本研究在文獻探討與報告準備過程中使用 Anthropic Claude (Sonnet 4.5)
```
生成式 AI 工具進行輔助。
使用範圍與目的
✓ 文獻整理與摘要：協助整理上傳文獻的關鍵資訊，包括研究問題、方法、結果等
內容摘要
✓ 演講稿撰寫：協助組織口頭報告的邏輯架構與表達方式
✓ 格式優化：協助統一文獻引用格式、簡報視覺呈現與排版設計
人工審核與修正
所有 AI 生成內容均經過研究者逐一核對原始文獻，確保資訊正確性與學術嚴謹性。研
究者對所有引用內容、研究方法及結果詮釋負完全責任，並進行必要的修正與補充。
學術誠信聲明
本研究所有核心觀點、研究設計、文獻分析與結論詮釋均為研究者獨立完成。GAI 工具
僅作為輔助工具用於提升文獻整理效率與簡報製作品質，不涉及研究內容的實質性創
作或學術判斷。研究者完全理解並遵守學術倫理規範。