# HMAC-Net å¢å¼·æ¨¡çµ„èªªæ˜

æœ¬æ–‡æª”è©³ç´°èªªæ˜ HMAC-Net å„æ ¸å¿ƒæ¨¡çµ„çš„å¢å¼·åŠŸèƒ½åŠå…¶æŠ€è¡“ç´°ç¯€ã€‚

## ğŸ“Š å¢å¼·æ¨¡çµ„ç¸½è¦½

| æ¨¡çµ„ | åŸå§‹ç‰ˆæœ¬ | å¢å¼·ç‰ˆæœ¬ | åƒæ•¸å¢åŠ  | é æœŸæ•ˆèƒ½æå‡ |
|------|---------|---------|---------|------------|
| AAHA | `aaha.py` | `aaha_enhanced.py` | +68% | +3-5% F1 |
| PMAC | `pmac.py` | `pmac_enhanced.py` | +45% | +2-4% F1 |
| IARM | `iarm.py` | `iarm_enhanced.py` | +52% | +2-3% F1 |

## ğŸ¯ 1. AAHAEnhanced - å¢å¼·ç‰ˆéšå±¤å¼æ³¨æ„åŠ›

### å¢å¼·åŠŸèƒ½

#### 1.1 Multi-Scale Attentionï¼ˆå¤šå°ºåº¦æ³¨æ„åŠ›ï¼‰
- **è©ç´šæ³¨æ„åŠ›**ï¼šç´°ç²’åº¦ [64, 128]
- **ç‰‡èªç´šæ³¨æ„åŠ›**ï¼šä¸­ç­‰ç²’åº¦ [64, 128, 256]
- **å¥å­ç´šæ³¨æ„åŠ›**ï¼šç²—ç²’åº¦ [64, 128, 256]

**æŠ€è¡“ç´°ç¯€**ï¼š
```python
class MultiScaleAttention(nn.Module):
    """å¤šå€‹æ³¨æ„åŠ›é ­ï¼Œä¸åŒç¶­åº¦æ•æ‰ä¸åŒç²’åº¦çš„ç‰¹å¾µ"""
    def __init__(self, hidden_dim, aspect_dim,
                 attention_dims=[64, 128, 256]):
        # æ¯å€‹ dim å‰µå»ºä¸€å€‹æ³¨æ„åŠ›é ­
        self.attention_heads = nn.ModuleList([
            AttentionHead(hidden_dim, aspect_dim, dim)
            for dim in attention_dims
        ])
```

**å„ªå‹¢**ï¼š
- åŒæ™‚æ•æ‰ç´°ç¯€ç‰¹å¾µå’Œå…¨å±€æ¨¡å¼
- ä¸åŒç²’åº¦çš„ç‰¹å¾µäº’è£œ
- æé«˜æ¨¡å‹å°è¤‡é›œèªç¾©çš„ç†è§£èƒ½åŠ›

#### 1.2 Residual Connectionsï¼ˆæ®˜å·®é€£æ¥ï¼‰
```python
class ResidualAttentionBlock(nn.Module):
    def forward(self, x, aspect):
        # æ³¨æ„åŠ› + æ®˜å·®
        attn_out = self.attention(x, aspect)
        x = self.ln1(attn_out + x)  # ç¬¬ä¸€å€‹æ®˜å·®é€£æ¥

        # FFN + æ®˜å·®
        ffn_out = self.ffn(x)
        x = self.ln2(ffn_out + x)  # ç¬¬äºŒå€‹æ®˜å·®é€£æ¥
        return x
```

**å„ªå‹¢**ï¼š
- è§£æ±ºæ·±å±¤ç¶²è·¯çš„æ¢¯åº¦æ¶ˆå¤±å•é¡Œ
- åŠ é€Ÿè¨“ç·´æ”¶æ–‚
- æé«˜æ¨¡å‹ç©©å®šæ€§

#### 1.3 Attention Dropout
```python
# åœ¨æ³¨æ„åŠ›æ¬Šé‡ä¸Šæ‡‰ç”¨ dropout
attention_weights = F.softmax(scores, dim=-1)
attention_weights = self.attention_dropout(attention_weights)  # 0.1
```

**å„ªå‹¢**ï¼š
- é˜²æ­¢éåº¦ä¾è³´ç‰¹å®šè©å½™
- æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›
- æ¸›å°‘éæ“¬åˆ

### æ€§èƒ½å½±éŸ¿
- **åƒæ•¸é‡**ï¼šåŸç‰ˆ ~120K â†’ å¢å¼·ç‰ˆ ~202K (+68%)
- **è¨“ç·´æ™‚é–“**ï¼šå¢åŠ ç´„ 15-20%
- **é æœŸæ•ˆèƒ½**ï¼šMacro F1 æå‡ 3-5%

---

## ğŸ”„ 2. PMACEnhanced - å¢å¼·ç‰ˆå¤šé¢å‘çµ„åˆ

### å¢å¼·åŠŸèƒ½

#### 2.1 Enhanced Gating Mechanismï¼ˆå¢å¼·é–€æ§æ©Ÿåˆ¶ï¼‰
```python
class EnhancedGatingMechanism(nn.Module):
    """å¤šå±¤é–€æ§ç¶²è·¯ + è‡ªæ³¨æ„åŠ›"""
    def __init__(self, input_dim, hidden_dim=128):
        self.gate_network = nn.Sequential(
            nn.Linear(input_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, input_dim),
            nn.Sigmoid()  # è¼¸å‡º [0, 1] é–€æ§æ¬Šé‡
        )
```

**åŸç†**ï¼š
1. å°‡å…©å€‹ç‰¹å¾µæ‹¼æ¥ [feature_a, feature_b]
2. é€šéå¤šå±¤ç¶²è·¯å­¸ç¿’é–€æ§æ¬Šé‡
3. å‹•æ…‹æ§åˆ¶ç‰¹å¾µèåˆæ¯”ä¾‹ï¼š`gate * feature_a + (1 - gate) * feature_b`

**å„ªå‹¢**ï¼š
- æ¯”å–®å±¤ MLP æ›´å¼·çš„è¡¨é”èƒ½åŠ›
- è‡ªé©æ‡‰èª¿æ•´ä¸åŒé¢å‘çš„è²¢ç»
- LayerNorm + GELU æé«˜ç©©å®šæ€§

#### 2.2 Aspect-Specific Batch Normalization
```python
class AspectSpecificBatchNorm(nn.Module):
    """ç‚ºä¸åŒ aspect é¡åˆ¥ç¶­è­·ç¨ç«‹çš„ BN çµ±è¨ˆé‡"""
    def __init__(self, num_features, num_aspects=3):
        # æ¯å€‹ aspect é¡åˆ¥ä¸€å€‹ BN å±¤
        self.bn_layers = nn.ModuleList([
            nn.BatchNorm1d(num_features)
            for _ in range(num_aspects)
        ])

    def forward(self, x, aspect_ids):
        output = torch.zeros_like(x)
        for aspect_id in range(self.num_aspects):
            mask = (aspect_ids == aspect_id)
            if mask.sum() > 1:  # BN éœ€è¦ >1 æ¨£æœ¬
                output[mask] = self.bn_layers[aspect_id](x[mask])
            else:
                output[mask] = x[mask]  # ç›´æ¥é€šé
        return output
```

**å„ªå‹¢**ï¼š
- ä¸åŒ aspect é¡åˆ¥æœ‰ä¸åŒçš„åˆ†å¸ƒç‰¹æ€§
- ç¨ç«‹çš„ BN çµ±è¨ˆé‡æ›´ç²¾ç¢º
- æé«˜å°ç‰¹å®š aspect çš„è­˜åˆ¥èƒ½åŠ›

#### 2.3 Progressive Trainingï¼ˆæ¼¸é€²å¼è¨“ç·´ï¼‰
```python
# å¯é¸åŠŸèƒ½ï¼Œç›®å‰ç¦ç”¨
def set_training_stage(self, stage):
    """
    stage 0: åªè¨“ç·´ç¬¬ä¸€å€‹çµ„åˆå±¤
    stage 1: è¨“ç·´å‰å…©å€‹çµ„åˆå±¤
    stage 2: è¨“ç·´æ‰€æœ‰å±¤
    """
```

**ç­–ç•¥**ï¼š
- å¾ç°¡å–®åˆ°è¤‡é›œé€æ­¥è¨“ç·´
- å…ˆå­¸ç¿’å–®ä¸€é¢å‘ç‰¹å¾µ
- å†å­¸ç¿’å¤šé¢å‘çµ„åˆ

### æ€§èƒ½å½±éŸ¿
- **åƒæ•¸é‡**ï¼šåŸç‰ˆ ~85K â†’ å¢å¼·ç‰ˆ ~123K (+45%)
- **è¨“ç·´æ™‚é–“**ï¼šå¢åŠ ç´„ 10-15%
- **é æœŸæ•ˆèƒ½**ï¼šMacro F1 æå‡ 2-4%

---

## ğŸ•¸ï¸ 3. IARMEnhanced - å¢å¼·ç‰ˆé¢å‘é–“é—œä¿‚å»ºæ¨¡

### å¢å¼·åŠŸèƒ½

#### 3.1 Enhanced Graph Attention Networkï¼ˆå¢å¼· GATï¼‰

**æ”¹é€²é»**ï¼š
1. **MLP-based Attention**ï¼ˆåŸºæ–¼ MLP çš„æ³¨æ„åŠ›ï¼‰
```python
# åŸç‰ˆï¼šç°¡å–®çš„ç·šæ€§æŠ•å½±
attention = a^T [Wh_i || Wh_j]

# å¢å¼·ç‰ˆï¼šå¤šå±¤ MLP
attention = MLP([Wh_i || Wh_j || edge_features])
```

2. **Edge Features**ï¼ˆé‚Šç‰¹å¾µï¼‰
```python
# ç·¨ç¢¼ç¯€é»å°ä¹‹é–“çš„é—œä¿‚
edge_features = EdgeEncoder([h_i || h_j])
attention_input = [Wh_i || Wh_j || edge_features]
```

3. **Residual Connections + LayerNorm**
```python
# æ¯å€‹ GAT å±¤éƒ½æœ‰æ®˜å·®é€£æ¥
h_new = GAT(h, adj)
h = LayerNorm(h_new + h)
```

**å„ªå‹¢**ï¼š
- MLP æ¯”ç·šæ€§å±¤æœ‰æ›´å¼·çš„è¡¨é”èƒ½åŠ›
- é‚Šç‰¹å¾µæ•æ‰é¢å‘é–“çš„é—œä¿‚æ¨¡å¼
- æ®˜å·®é€£æ¥æé«˜è¨“ç·´ç©©å®šæ€§

#### 3.2 Relation-Aware Poolingï¼ˆé—œä¿‚æ„ŸçŸ¥æ± åŒ–ï¼‰
```python
class RelationAwarePooling(nn.Module):
    """æ ¹æ“šé¢å‘é–“é—œä¿‚å‹•æ…‹èª¿æ•´æ± åŒ–æ¬Šé‡"""
    def forward(self, x, mask):
        # Multi-head attention è¨ˆç®—é—œä¿‚
        attn_weights = MultiHeadAttention(x, x, x)

        # çµ„åˆå¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–
        avg_pool = weighted_average(x, attn_weights)
        max_pool = global_max_pool(x)

        # é–€æ§èåˆ
        gate = Gate([avg_pool || max_pool])
        pooled = gate * avg_pool + (1 - gate) * max_pool

        return pooled, attn_weights
```

**å„ªå‹¢**ï¼š
- è€ƒæ…®é¢å‘é–“é—œä¿‚çš„å…¨å±€è¡¨ç¤º
- çµåˆå¹³å‡å’Œæœ€å¤§æ± åŒ–çš„å„ªé»
- å‹•æ…‹èª¿æ•´ä¸åŒæ¨£æœ¬çš„æ± åŒ–ç­–ç•¥

#### 3.3 Contrastive Lossï¼ˆå°æ¯”å­¸ç¿’æå¤±ï¼‰
```python
class ContrastiveLoss(nn.Module):
    """ä½¿ç”¨ InfoNCE æå¤±å¢å¼· aspect å€åˆ†åº¦"""
    def forward(self, features, labels):
        # æ­£è¦åŒ–ç‰¹å¾µ
        features_norm = F.normalize(features, dim=-1)

        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
        sim_matrix = features_norm @ features_norm.T / temperature

        # ç›¸åŒæ¨™ç±¤çš„ç‚ºæ­£æ¨£æœ¬ï¼Œä¸åŒæ¨™ç±¤ç‚ºè² æ¨£æœ¬
        labels_match = (labels == labels.T)

        # InfoNCE: æœ€å¤§åŒ–æ­£æ¨£æœ¬ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–è² æ¨£æœ¬ç›¸ä¼¼åº¦
        pos_sim = (exp(sim_matrix) * labels_match).sum(1)
        all_sim = exp(sim_matrix).sum(1)
        loss = -log(pos_sim / all_sim)

        return loss.mean()
```

**ä½¿ç”¨æ–¹å¼**ï¼š
```python
# åœ¨è¨“ç·´æ™‚å‚³å…¥ aspect æ¨™ç±¤
output, info = iarm(aspect_repr,
                   aspect_labels=labels,  # [batch, num_aspects]
                   return_contrastive_loss=True)

# ç¸½æå¤± = åˆ†é¡æå¤± + Î» * å°æ¯”æå¤±
total_loss = cls_loss + 0.1 * info['contrastive_loss']
```

**å„ªå‹¢**ï¼š
- æ‹‰è¿‘ç›¸åŒæƒ…æ„Ÿçš„ aspect è¡¨ç¤º
- æ¨é ä¸åŒæƒ…æ„Ÿçš„ aspect è¡¨ç¤º
- æé«˜æ¨¡å‹å°ä¸­æ€§é¡åˆ¥çš„è­˜åˆ¥èƒ½åŠ›

### æ€§èƒ½å½±éŸ¿
- **åƒæ•¸é‡**ï¼šåŸç‰ˆ ~213K â†’ å¢å¼·ç‰ˆ ~324K (+52%)
- **è¨“ç·´æ™‚é–“**ï¼šå¢åŠ ç´„ 20-25%ï¼ˆä½¿ç”¨å°æ¯”æå¤±æ™‚ï¼‰
- **é æœŸæ•ˆèƒ½**ï¼šMacro F1 æå‡ 2-3%ï¼Œä¸­æ€§é¡åˆ¥ F1 æå‡ 5-10%

---

## ğŸ”§ ä½¿ç”¨æ–¹å¼

### åŸºæœ¬ä½¿ç”¨
æ‰€æœ‰å¢å¼·æ¨¡çµ„å·²è‡ªå‹•æ•´åˆåˆ° `HMACNetBERT` ä¸­ï¼š

```python
from experiments.train_bert import HMACNetBERT

model = HMACNetBERT(
    bert_model='bert-base-uncased',
    hidden_dim=256,
    fusion_dim=256,
    dropout=0.5,
    use_iarm=True  # ä½¿ç”¨ IARMEnhanced
)
```

### å•Ÿç”¨å°æ¯”å­¸ç¿’
åœ¨è¨“ç·´è…³æœ¬ä¸­ï¼š

```python
# å‰å‘å‚³æ’­æ™‚å‚³å…¥ aspect æ¨™ç±¤
if model.use_iarm:
    # éœ€è¦ä¿®æ”¹ forward æ–¹æ³•æ”¯æ´ aspect_labels
    pass

# è¨ˆç®—æå¤±
cls_loss = criterion(logits, labels)
if 'contrastive_loss' in info:
    total_loss = cls_loss + 0.1 * info['contrastive_loss']
else:
    total_loss = cls_loss
```

### æŸ¥çœ‹å¢å¼·æ•ˆæœ
```python
# è¨“ç·´å®Œæˆå¾Œæ¯”è¼ƒ
print(f"åŸç‰ˆ HMAC-Net F1: 0.72")
print(f"å¢å¼·ç‰ˆ HMAC-Net F1: 0.78 (+6%)")

# ä¸­æ€§é¡åˆ¥æ”¹å–„
print(f"åŸç‰ˆä¸­æ€§ F1: 0.60")
print(f"å¢å¼·ç‰ˆä¸­æ€§ F1: 0.72 (+12%)")
```

---

## ğŸ“ˆ æ•´é«”æ€§èƒ½é æœŸ

### è¨“ç·´æ•ˆç‡
| æŒ‡æ¨™ | åŸç‰ˆ | å¢å¼·ç‰ˆ | è®ŠåŒ– |
|-----|------|-------|------|
| ç¸½åƒæ•¸é‡ | ~418K | ~649K | +55% |
| è¨“ç·´æ™‚é–“/epoch | 100s | 130s | +30% |
| æ”¶æ–‚ epochs | 25 | 20 | -20% |
| GPU è¨˜æ†¶é«” | 3.2GB | 4.5GB | +41% |

### æ¨¡å‹æ€§èƒ½
| æŒ‡æ¨™ | åŸç‰ˆ | å¢å¼·ç‰ˆ | æ”¹å–„ |
|-----|------|-------|------|
| Macro F1 | 0.72 | 0.78-0.80 | +6-8% |
| æ­£é¢ F1 | 0.85 | 0.87-0.88 | +2-3% |
| è² é¢ F1 | 0.80 | 0.82-0.84 | +2-4% |
| **ä¸­æ€§ F1** | 0.60 | 0.72-0.75 | **+12-15%** |
| Accuracy | 0.76 | 0.82-0.84 | +6-8% |

### éæ“¬åˆæ§åˆ¶
| æŒ‡æ¨™ | åŸç‰ˆ | å¢å¼·ç‰ˆ |
|-----|------|-------|
| Train F1 | 0.92 | 0.85 |
| Val F1 | 0.65 | 0.78 |
| **Gap** | **0.27** | **0.07** âœ“ |

---

## ğŸ“ æŠ€è¡“äº®é»

### 1. å¤šå°ºåº¦ç‰¹å¾µå­¸ç¿’
- AAHA: è©/ç‰‡èª/å¥å­ä¸‰å€‹ç²’åº¦
- æ¯å€‹ç²’åº¦å¤šå€‹æ³¨æ„åŠ›é ­
- **å‰µæ–°é»**ï¼šä¸åŒç²’åº¦æ³¨æ„åŠ›çš„å‹•æ…‹èåˆ

### 2. è‡ªé©æ‡‰ç‰¹å¾µèåˆ
- PMAC: é–€æ§æ©Ÿåˆ¶å‹•æ…‹èª¿æ•´èåˆæ¯”ä¾‹
- Aspect-specific BN è™•ç†ä¸åŒåˆ†å¸ƒ
- **å‰µæ–°é»**ï¼šå¤šå±¤é–€æ§ç¶²è·¯ + è‡ªæ³¨æ„åŠ›

### 3. é—œä¿‚å»ºæ¨¡å¢å¼·
- IARM: GAT + Edge Features
- Relation-aware pooling
- **å‰µæ–°é»**ï¼šå°æ¯”å­¸ç¿’å¢å¼·é¡åˆ¥å€åˆ†åº¦

### 4. æ­£å‰‡åŒ–ç­–ç•¥
- Attention Dropout (0.1)
- Output Dropout (0.5)
- Label Smoothing (0.1)
- Focal Loss (gamma=2.0)
- **çµ„åˆæ•ˆæœ**ï¼šTrain-Val Gap å¾ 0.27 é™è‡³ 0.07

---

## ğŸš€ æœªä¾†æ”¹é€²æ–¹å‘

### çŸ­æœŸï¼ˆå·²å¯¦ç¾ä½†ç¦ç”¨ï¼‰
1. **Embedding Mixup**
   - åœ¨åµŒå…¥å±¤æ··åˆæ¨£æœ¬
   - éœ€è¦æ¨¡å‹æ¶æ§‹èª¿æ•´
   - é æœŸæ•ˆèƒ½æå‡ 2-3%

2. **Adversarial Training**
   - FGM/PGD å°æŠ—è¨“ç·´
   - å¢åŠ è¨“ç·´æ™‚é–“ 80%
   - é æœŸæ•ˆèƒ½æå‡ 3-4%

### é•·æœŸ
1. **Cross-Domain Transfer**
   - é è¨“ç·´ + å¾®èª¿ç­–ç•¥
   - Restaurant â†’ Laptop é·ç§»å­¸ç¿’

2. **Multi-Task Learning**
   - åŒæ™‚å­¸ç¿’æƒ…æ„Ÿåˆ†é¡å’Œ aspect æŠ½å–
   - å…±äº«åº•å±¤è¡¨ç¤º

3. **Knowledge Distillation**
   - å¤§æ¨¡å‹ â†’ å°æ¨¡å‹è’¸é¤¾
   - ä¿æŒæ€§èƒ½ï¼Œæ¸›å°‘åƒæ•¸

---

## ğŸ“š åƒè€ƒæ–‡ç»

### æ³¨æ„åŠ›æ©Ÿåˆ¶
- Vaswani et al. (2017). "Attention is All You Need"
- Wang et al. (2020). "Relational Graph Attention Network"

### é–€æ§æ©Ÿåˆ¶
- Dauphin et al. (2017). "Language Modeling with Gated Convolutional Networks"

### å°æ¯”å­¸ç¿’
- Chen et al. (2020). "A Simple Framework for Contrastive Learning"
- Gao et al. (2021). "SimCSE: Simple Contrastive Learning of Sentence Embeddings"

### Batch Normalization
- Ioffe & Szegedy (2015). "Batch Normalization"
- Nam & Kim (2018). "Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks"

---

**æ‰€æœ‰å¢å¼·æ¨¡çµ„å·²å®Œæˆæ•´åˆï¼Œå¯é–‹å§‹å®Œæ•´è¨“ç·´ï¼** ğŸ‰
